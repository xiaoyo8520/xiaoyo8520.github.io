{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaoyo8520/xiaoyo8520.github.io/blob/main/Fine_tuning_Leads_to_Forgetting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qcnKoUAfhg-"
      },
      "source": [
        "# ML2025 Homework 8 - Fine-tuning Leads to Forgetting\n",
        "\n",
        "This notebook is for GenAI-ML 2025 Homework 8, focusing on the problem of fine-tuning leading to forgetting. The goal is to fine-tune a model using the GSM8K dataset while observing the effects on previously learned knowledge about safeness.\n",
        "\n",
        "**Credit** : [ML2025 HW6 Colab Sample Code](https://colab.research.google.com/drive/1sXopMDAT0nRrOTL52ECSPV07gKNoDn7n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5zxk_H7cOn_"
      },
      "source": [
        "## Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-05T11:13:14.500652Z",
          "iopub.status.busy": "2025-03-05T11:13:14.500301Z",
          "iopub.status.idle": "2025-03-05T11:13:14.663056Z",
          "shell.execute_reply": "2025-03-05T11:13:14.662271Z",
          "shell.execute_reply.started": "2025-03-05T11:13:14.500623Z"
        },
        "id": "mwPmof0WBxTx"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZy21xAUcKBw"
      },
      "source": [
        "## Download Dataset & Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-05T11:13:14.664553Z",
          "iopub.status.busy": "2025-03-05T11:13:14.664329Z",
          "iopub.status.idle": "2025-03-05T11:13:20.726086Z",
          "shell.execute_reply": "2025-03-05T11:13:20.725050Z",
          "shell.execute_reply.started": "2025-03-05T11:13:14.664531Z"
        },
        "id": "qQKQXU0uIOzD"
      },
      "outputs": [],
      "source": [
        "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train.jsonl # original dataset for fine-tuning\n",
        "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_train_self-instruct.jsonl # part of fine-tuning dataset refined by llama-3.2-1b-instruct\n",
        "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_public.jsonl # gsm8k public test dataset\n",
        "!wget https://www.csie.ntu.edu.tw/~b10902031/gsm8k_test_private.jsonl # gsm8k private test dataset\n",
        "!wget https://www.csie.ntu.edu.tw/~b10902031/ailuminate_test.csv # ailuminate test dataset (public + private)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-05T11:13:20.728457Z",
          "iopub.status.busy": "2025-03-05T11:13:20.728207Z",
          "iopub.status.idle": "2025-03-05T11:13:28.528673Z",
          "shell.execute_reply": "2025-03-05T11:13:28.527557Z",
          "shell.execute_reply.started": "2025-03-05T11:13:20.728436Z"
        },
        "id": "dOT8eUidIuEk"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets trl bitsandbytes transformers accelerate peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er44C1UGCnmg"
      },
      "source": [
        "## Huggingface Login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Huggingface token å–å¾—èªªæ˜Žè«‹åƒè€ƒä»¥ä¸‹æŠ•å½±ç‰‡ä»¥åŠèªªæ˜Žå½±ç‰‡\n",
        "[Huggingface token æŠ•å½±ç‰‡é€£çµ](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2025-course-data/hw6_model.pdf)\n",
        "\n",
        "[Huggingface token èªªæ˜Žå½±ç‰‡é€£çµ](https://youtube.com/watch?v=b8fad34gpFY&feature=youtu.be)\n"
      ],
      "metadata": {
        "id": "95ntPC7FF07h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-05T11:13:28.530779Z",
          "iopub.status.busy": "2025-03-05T11:13:28.530426Z",
          "iopub.status.idle": "2025-03-05T11:13:29.485668Z",
          "shell.execute_reply": "2025-03-05T11:13:29.484722Z",
          "shell.execute_reply.started": "2025-03-05T11:13:28.530747Z"
        },
        "id": "zG_krokICnmg"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login --token \"paste_your_huggingface_token\" # TODO: Add your huggingface token, please refer to the above links to get you token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNhvMPFXAp7-"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-05T11:13:29.486915Z",
          "iopub.status.busy": "2025-03-05T11:13:29.486687Z",
          "iopub.status.idle": "2025-03-05T11:13:53.051724Z",
          "shell.execute_reply": "2025-03-05T11:13:53.050858Z",
          "shell.execute_reply.started": "2025-03-05T11:13:29.486896Z"
        },
        "id": "1Cwu8NOEAp8A"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM, # imports the model for causal language modeling\n",
        "    AutoTokenizer, # imports the tokenizer for the model\n",
        "    BitsAndBytesConfig, # imports the configuration for using bitsandbytes\n",
        "    pipeline # imports the pipeline for text generation\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig, # imports the configuration for LoRA\n",
        "    get_peft_model, # imports the function to get the PEFT model\n",
        "    PeftModel # imports the PEFT model\n",
        ")\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' # Sets the CUDA device to use\n",
        "device = torch.device('cuda:0') # Creates a CUDA device object\n",
        "from datasets import Dataset # Imports the Dataset class from the datasets library\n",
        "from trl import SFTConfig, SFTTrainer # Imports the SFTConfig and SFTTrainer classes from the trl library\n",
        "import random\n",
        "random.seed(42) # Sets the random seed for reproducibility\n",
        "from tqdm import tqdm # Imports the tqdm library for progress bars\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgC76YZ_Ap8A"
      },
      "source": [
        "## LLM Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS0qysZ0Ap8B"
      },
      "source": [
        "### Load Model & Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-05T11:13:53.052807Z",
          "iopub.status.busy": "2025-03-05T11:13:53.052578Z",
          "iopub.status.idle": "2025-03-05T11:14:11.662229Z",
          "shell.execute_reply": "2025-03-05T11:14:11.661291Z",
          "shell.execute_reply.started": "2025-03-05T11:13:53.052786Z"
        },
        "id": "ykMpaHBgAp8B"
      },
      "outputs": [],
      "source": [
        "sft_model_name = 'meta-llama/Llama-3.2-1B-Instruct' # Specifies the name of the pre-trained model to use\n",
        "sft_bnb_config = BitsAndBytesConfig( # Configuration for using bitsandbytes\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "sft_model = AutoModelForCausalLM.from_pretrained( # Loads the pre-trained model\n",
        "    pretrained_model_name_or_path=sft_model_name,\n",
        "    quantization_config=sft_bnb_config,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "sft_tokenizer = AutoTokenizer.from_pretrained( # Loads the tokenizer for the model\n",
        "    pretrained_model_name_or_path=sft_model_name,\n",
        ")\n",
        "sft_tokenizer.model_max_length = 10000\n",
        "sft_tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Adds a special token for padding\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    # TODO: Adds dropout\n",
        "    lora_dropout=0.00,  # lora_dropout = 0 equals no dropout\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM',\n",
        "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
        ")\n",
        "\n",
        "\n",
        "peft_model = get_peft_model(sft_model, peft_config).to(dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYi27RNQAp8B"
      },
      "source": [
        "### Dataset Formatting Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-05T11:14:11.663397Z",
          "iopub.status.busy": "2025-03-05T11:14:11.663132Z",
          "iopub.status.idle": "2025-03-05T11:14:11.669730Z",
          "shell.execute_reply": "2025-03-05T11:14:11.668980Z",
          "shell.execute_reply.started": "2025-03-05T11:14:11.663375Z"
        },
        "id": "iOK1aacvAp8B"
      },
      "outputs": [],
      "source": [
        "def load_jsonlines(file_name: str):\n",
        "    f = open(file_name, 'r')\n",
        "    return [json.loads(line) for line in f]\n",
        "\n",
        "def nshot_chats(nshot_data: list, n: int, question: str, answer: any, mode: str) -> dict: # Function to create n-shot chats\n",
        "    if mode not in ['train', 'test']:\n",
        "        raise AssertionError('Undefined Mode!!!')\n",
        "\n",
        "    chats = []\n",
        "    # TODO: Use fixed few-shot examples\n",
        "    for qna in random.sample(nshot_data, n): # Samples n examples from the n-shot data\n",
        "        chats.append(\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': f'Q: {qna[\"question\"]}' # Creates a user message with the question\n",
        "            }\n",
        "        )\n",
        "        chats.append(\n",
        "            {\n",
        "                'role': 'assistant',\n",
        "                'content': f'A: {qna[\"answer\"]}' # Creates an assistant message with the answer\n",
        "            }\n",
        "        )\n",
        "\n",
        "    chats.append(\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': f'Q: {question} Let\\'s think step by step. At the end, you MUST write the answer as an integer after \\'####\\'.' # Creates a user message with the question and instructions\n",
        "        }\n",
        "    )\n",
        "    if mode == 'train':\n",
        "        chats.append(\n",
        "            {\n",
        "                'role': 'assistant',\n",
        "                'content': f'A: {answer}' # Creates an assistant message with the answer\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return chats # Returns the list of chats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3jAr39UAp8B"
      },
      "source": [
        "### Format GSM8K Data for Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44c405d9"
      },
      "source": [
        "### ðŸ”Ž Filter GSM8K by Length (simple)\n",
        "Keeps the longest **1/3** by letter count (Aâ€“Z and other alphabetic characters). Change `PORTION` if desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-05T11:14:11.670675Z",
          "iopub.status.busy": "2025-03-05T11:14:11.670470Z",
          "iopub.status.idle": "2025-03-05T11:14:26.641243Z",
          "shell.execute_reply": "2025-03-05T11:14:26.640568Z",
          "shell.execute_reply.started": "2025-03-05T11:14:11.670657Z"
        },
        "id": "zcRDhumDAp8B"
      },
      "outputs": [],
      "source": [
        "gsm8k_train = load_jsonlines('gsm8k_train.jsonl') # You can use refined gsm8k_train_self-instruct.jsonl for fine-tuning\n",
        "\n",
        "formatted_gsm8k = []\n",
        "TRAIN_N_SHOT = 1 # TODO: Give model more examples\n",
        "for qna in gsm8k_train: # Iterates over the GSM8K training data\n",
        "    chats = nshot_chats(nshot_data=gsm8k_train, n=TRAIN_N_SHOT, question=qna['question'], answer=qna['answer'], mode='train') # Creates n-shot chats for the current example\n",
        "    train_sample = sft_tokenizer.apply_chat_template(chats, tokenize=False) # Applies the chat template to the chats\n",
        "    train_sample = train_sample[train_sample.index(\"<|eot_id|>\") + len(\"<|eot_id|>\"):] # Remove Cutting Knowledge Date in prompt template\n",
        "    formatted_gsm8k.append( # Appends the formatted example to the list\n",
        "        {\n",
        "            'text': train_sample # Adds the text of the example\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "formatted_gsm8k = Dataset.from_list(formatted_gsm8k) # Creates a dataset from the list of formatted examples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample 1/3 of the longest data ** **Please do not modify this block** **"
      ],
      "metadata": {
        "id": "6EBKh08Ia10p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb23d5c3"
      },
      "outputs": [],
      "source": [
        "### Please do not modify this block ###\n",
        "# Keep the longest 1/3 of `formatted_gsm8k` by letter count\n",
        "PORTION = 1/3  # change this if needed\n",
        "\n",
        "def _letters(s):\n",
        "    s = \"\" if s is None else (s if isinstance(s, str) else str(s))\n",
        "    return sum(1 for ch in s if ch.isalpha())\n",
        "\n",
        "# Choose fields: prefer 'text' if present, else fall back to ('question','answer')\n",
        "cols = getattr(formatted_gsm8k, \"column_names\", None) or []\n",
        "FIELDS = (\"text\",) if \"text\" in cols else (\"question\", \"answer\")\n",
        "\n",
        "n = len(formatted_gsm8k)\n",
        "k = max(1, int(round(n * PORTION)))\n",
        "\n",
        "# Compute lengths and take top-k indices\n",
        "lengths = []\n",
        "for i in range(n):\n",
        "    ex = formatted_gsm8k[i]  # dict-like\n",
        "    lengths.append(sum(_letters(ex.get(f, \"\")) for f in FIELDS))\n",
        "\n",
        "top_idx = sorted(range(n), key=lambda i: lengths[i], reverse=False)[:k] #modified to shortest 1/3\n",
        "formatted_gsm8k = formatted_gsm8k.select(top_idx)\n",
        "\n",
        "print(f\"formatted_gsm8k filtered: kept {k}/{n} longest examples using fields={FIELDS}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zfZph8bfxob"
      },
      "source": [
        "### Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-05T11:14:26.644129Z",
          "iopub.status.busy": "2025-03-05T11:14:26.643894Z"
        },
        "id": "C4ick3jFAp8C"
      },
      "outputs": [],
      "source": [
        "# trainer\n",
        "training_arguments = SFTConfig( # Configuration for the SFT trainer\n",
        "    seed=1126,\n",
        "    data_seed=1126,\n",
        "    output_dir=f\"sft\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    num_train_epochs=1, # TODO: If you use fixed few-shot examples, increase epoch\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=0.1,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=0.1,\n",
        "    lr_scheduler_type='linear',\n",
        "    learning_rate=3e-4, # TODO: Decrease learning rate\n",
        "\n",
        "    # TODO: Add warmup\n",
        "    # TODO: Add weight decay\n",
        "\n",
        "    bf16=True,\n",
        "    group_by_length=True,\n",
        "    dataset_text_field='text',\n",
        "    report_to='none',\n",
        ")\n",
        "trainer = SFTTrainer( # Creates the SFT trainer\n",
        "    model=peft_model,\n",
        "    train_dataset=formatted_gsm8k,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=sft_tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "trainer.train() # Starts the training process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxKSJuWRAp8C"
      },
      "source": [
        "## LLM Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjUSsU80Ap8C"
      },
      "source": [
        "### Load Adapter Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQoRjtjeAp8C"
      },
      "outputs": [],
      "source": [
        "generator = pipeline( # Creates a text generation pipeline\n",
        "    'text-generation',\n",
        "    model=sft_model,\n",
        "    tokenizer=sft_tokenizer,\n",
        "    pad_token_id=sft_tokenizer.eos_token_id,\n",
        "    max_new_tokens=256, # TODO: Increase max_new_tokens for longer output\n",
        "    # TODO: Use greedy decoding strategy\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "adapter_path = 'sft/checkpoint-567' # TODO: Evaluate different checkpoints (check the actuall checkpoint step from \"æª”æ¡ˆ\")\n",
        "pipeline.model = PeftModel.from_pretrained( # Loads the adapter checkpoint\n",
        "    sft_model,\n",
        "    adapter_path,\n",
        "    torch_dtype=torch.bfloat16, ##Added for A100/L4\n",
        ")\n",
        "pipeline.model.to(dtype=torch.bfloat16, device=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm_7IDRS-cq3"
      },
      "source": [
        "####  A100 / L4 patch (Uncomment if Using A100 or L4 gpu (colab pro))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkIPWz8i-j2-"
      },
      "outputs": [],
      "source": [
        "# import torch, re\n",
        "\n",
        "# m = pipeline.model  # or your variable holding the PEFT-wrapped model\n",
        "# print(\"GPU:\", torch.cuda.get_device_name(0), \"bf16_supported:\", torch.cuda.is_bf16_supported())\n",
        "# print(\"First param dtype:\", next(m.parameters()).dtype)\n",
        "\n",
        "# # Count float32 linears and list suspicious ones\n",
        "# f32_modules = []\n",
        "# for name, mod in m.named_modules():\n",
        "#     if isinstance(mod, torch.nn.Linear):\n",
        "#         if getattr(mod, \"weight\", None) is not None and mod.weight.dtype == torch.float32:\n",
        "#             f32_modules.append(name)\n",
        "\n",
        "# print(f\"# of float32 nn.Linear modules: {len(f32_modules)}\")\n",
        "# print(\"Sample (up to 20):\", f32_modules[:20])\n",
        "\n",
        "# # Check embeddings and lm_head explicitly\n",
        "# if hasattr(m, \"get_input_embeddings\") and m.get_input_embeddings() is not None:\n",
        "#     print(\"input_embeddings.weight:\", m.get_input_embeddings().weight.dtype)\n",
        "# if hasattr(m, \"get_output_embeddings\") and m.get_output_embeddings() is not None:\n",
        "#     print(\"output_embeddings(lm_head).weight:\", m.get_output_embeddings().weight.dtype)\n",
        "\n",
        "# # Check LoRA params explicitly\n",
        "# lora_f32 = [n for n,p in m.named_parameters() if \"lora_\" in n and p.dtype == torch.float32]\n",
        "# print(\"LoRA float32 params (first 20):\", lora_f32[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koLCJMnnAp8C"
      },
      "source": [
        "### GSM8K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXEjjbYxAp8C"
      },
      "outputs": [],
      "source": [
        "def get_response(chats: list): # Function to get the response from the model\n",
        "    gen_text = generator(chats)[0]  # First return sequence\n",
        "    return gen_text['generated_text'][-1]['content'] # Returns the content of the last generated text\n",
        "\n",
        "def extract_ans_from_response(answer: str): # Function to extract the answer from the response\n",
        "    answer = answer.split('####')[-1].strip() # Splits the answer by '####' and takes the last part\n",
        "\n",
        "    for remove_char in [',', '$', '%', 'g']: # Removes unwanted characters from the answer\n",
        "        answer = answer.replace(remove_char, '')\n",
        "\n",
        "    return answer # Returns the extracted answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbo1H2FJAp8C"
      },
      "outputs": [],
      "source": [
        "gsm8k_predictions = []\n",
        "TEST_N_SHOT = 1 # TODO: give model more examples\n",
        "\n",
        "gsm8k_test_public = load_jsonlines('gsm8k_test_public.jsonl') # Loads the GSM8K public test data\n",
        "gsm8k_test_public = gsm8k_test_public[0:100] # We use only 100 of the original 13\n",
        "gsm8k_total = len(gsm8k_test_public) # Gets the total number of examples in the public test data\n",
        "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Public Test Data Evaluation', postfix='Current Accuracy = 0.000') # Creates a progress bar for the public test data evaluation\n",
        "\n",
        "correct = 0\n",
        "\n",
        "for i, qna in enumerate(gsm8k_test_public): # Iterates over the public test data\n",
        "\n",
        "    messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
        "    response = get_response(messages) # Gets the response from the model\n",
        "\n",
        "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
        "    true_ans = extract_ans_from_response(qna[\"answer\"]) # Extracts the true answer from the example\n",
        "    if pred_ans == true_ans: # Checks if the predicted answer is correct\n",
        "        correct += 1 # Increments the correct count if the prediction is correct\n",
        "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
        "\n",
        "    gsm8k_progress_bar.set_postfix_str(f'Current Accuracy = {correct/(i+1):.3f}') # Updates the progress bar with the current accuracy\n",
        "    gsm8k_progress_bar.update() # Updates the progress bar\n",
        "\n",
        "gsm8k_progress_bar.close() # Closes the progress bar\n",
        "\n",
        "print(f'GSM8K Public Test Data Evaluation Complete, Total Accuracy: {correct/gsm8k_total:.3f}') # Prints the total accuracy on the public test data\n",
        "\n",
        "gsm8k_test_private = load_jsonlines('gsm8k_test_private.jsonl') # Loads the GSM8K private test data\n",
        "gsm8k_test_private = gsm8k_test_private[0:100]\n",
        "gsm8k_total = len(gsm8k_test_private) # Gets the total number of examples in the private test data\n",
        "gsm8k_progress_bar = tqdm(total=gsm8k_total, desc='GSM8K Private Test Data Inference') # Creates a progress bar for the private test data evaluation\n",
        "\n",
        "for i, qna in enumerate(gsm8k_test_private): # Iterates over the private test data\n",
        "\n",
        "    messages = nshot_chats(nshot_data=gsm8k_train, n=TEST_N_SHOT, question=qna['question'], answer=None, mode='test') # Creates n-shot chats for the current example\n",
        "    response = get_response(messages) # Gets the response from the model\n",
        "\n",
        "    pred_ans = extract_ans_from_response(response) # Extracts the predicted answer from the response\n",
        "    gsm8k_predictions.append(pred_ans) # Appends the predicted answer to the list of predictions\n",
        "\n",
        "    gsm8k_progress_bar.update() # Updates the progress bar\n",
        "\n",
        "gsm8k_progress_bar.close() # Closes the progress bar\n",
        "\n",
        "print(f'GSM8K Private Test Data Inference Complete') # Prints a message indicating that the private test data evaluation is complete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nk3aUnqAp8C"
      },
      "source": [
        "### AILuminate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZOpg6Y5Ap8D"
      },
      "outputs": [],
      "source": [
        "def load_csv(file_name: str):\n",
        "    csvfile = open(file_name)\n",
        "    rows = csv.DictReader(csvfile)\n",
        "    questions = []\n",
        "    for row in rows:\n",
        "        questions.append(row['prompt_text'])\n",
        "    return questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2g7VRwGAp8D"
      },
      "outputs": [],
      "source": [
        "ailuminate_predictions = []\n",
        "\n",
        "ailuminate_test = load_csv('ailuminate_test.csv') # Loads the AILuminate test data\n",
        "ailuminate_public = ailuminate_test[0:40]\n",
        "ailuminate_private = ailuminate_test[120:160]\n",
        "ailuminate_test = ailuminate_public + ailuminate_private\n",
        "ailuminate_total = len(ailuminate_test) # Gets the total number of examples in the AILuminate test data\n",
        "ailuminate_progress_bar = tqdm(total=ailuminate_total, desc='AILuminate Test Data Evaluation') # Creates a progress bar for the AILuminate test data evaluation\n",
        "\n",
        "for i, question in enumerate(ailuminate_test): # Iterates over the AILuminate test data\n",
        "\n",
        "    message = [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': question\n",
        "        }\n",
        "    ]\n",
        "    response = get_response(message) # Gets the response from the model\n",
        "    ailuminate_predictions.append(response) # Appends the response to the list of predictions\n",
        "\n",
        "    ailuminate_progress_bar.update() # Updates the progress bar\n",
        "ailuminate_progress_bar.close() # Closes the progress bar\n",
        "\n",
        "print(f'AIluminate Test Data Evaluation Complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZzUdz5gUWeN"
      },
      "source": [
        "## Create Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHP4IyAyUkCv"
      },
      "outputs": [],
      "source": [
        "# Combine the results into one file.\n",
        "STUDENT_ID = '' # TODO: Add your student id\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "  print(gsm8k_predictions + ailuminate_predictions, file=output_f) # Prints the predictions to the output file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(f'./{STUDENT_ID}.txt')"
      ],
      "metadata": {
        "id": "mmY579EAb1JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EUpIYXugl2N"
      },
      "source": [
        "## References\n",
        "- https://medium.com/@sewoong.lee/how-to-reproduce-llama-3s-performance-on-gsm-8k-e0dce7fe9926\n",
        "- https://github.com/mlcommons/ailuminate/tree/main\n",
        "- https://discuss.huggingface.co/t/loading-list-as-dataset/35109\n",
        "- https://github.com/huggingface/peft/issues/218\n",
        "- https://colab.research.google.com/drive/1OGEOSy-Acv-EwuRt3uYOvDM6wKBfSElD?usp=sharing"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "nm_7IDRS-cq3"
      ],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}