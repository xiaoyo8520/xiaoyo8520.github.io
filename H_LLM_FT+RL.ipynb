{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaoyo8520/xiaoyo8520.github.io/blob/main/H_LLM_FT%2BRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9shEQsVR8ys",
        "outputId": "eae03d79-8b30-417a-95ac-16c006090c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  1 17:36:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0             29W /   70W |   13126MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Code Structure Guide\n",
        "\n",
        "The sample code is organized into the structure:\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "- Environment Setup\n",
        "    - Set up essential environment settings such as package installation, utility function definition, global package import (packages that are used by both SFT and RL).\n",
        "    - You **SHOULD RUN** this section **WHENEVER** you start / restart a Colab session.\n",
        "- SFT\n",
        "    - The section of the HW7 SFT phase.\n",
        "    - Expand the tab and run the code cells in it if you are going to do the SFT phase.\n",
        "- RL\n",
        "    - The section of the HW7 RL phase.\n",
        "    - Expand the tab and run the code cells in it if you want to do the RL phase."
      ],
      "metadata": {
        "id": "63MvoYPKq_8r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T2Tx3P5KTA8"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NWa_rxylSYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f94904f-0b0c-4f72-937d-bd94ddf4e6da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELj7SvrsKdiA"
      },
      "source": [
        "### Package Installation (~ 5 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4jH1Fk5FGzl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b363cfc-ffb5-4194-e946-17bb0eec6590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting torch\n",
            "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch)\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m767.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m152.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.24.0+cu126\n",
            "    Uninstalling torchvision-0.24.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.24.0+cu126\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.9.0+cu126\n",
            "    Uninstalling torchaudio-2.9.0+cu126:\n",
            "      Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Successfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 torchaudio-2.9.1 torchvision-0.24.1 triton-3.5.1\n",
            "Collecting transformers==4.57.1 (from transformers[torch]==4.57.1)\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.1->transformers[torch]==4.57.1) (4.67.1)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (2.9.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]==4.57.1) (1.12.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]==4.57.1) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.1->transformers[torch]==4.57.1) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]==4.57.1) (3.5.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.1->transformers[torch]==4.57.1) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]==4.57.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]==4.57.1) (3.0.3)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.2\n",
            "    Uninstalling transformers-4.57.2:\n",
            "      Successfully uninstalled transformers-4.57.2\n",
            "Successfully installed transformers-4.57.1\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Collecting pyarrow>=21.0.0 (from datasets)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed datasets-4.4.1 pyarrow-22.0.0\n",
            "Collecting bitsandbytes==0.48.1\n",
            "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting trl==0.24.0 (from trl[peft]==0.24.0)\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting loralib\n",
            "  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.48.1) (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.48.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.48.1) (25.0)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.24.0->trl[peft]==0.24.0) (1.12.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl==0.24.0->trl[peft]==0.24.0) (4.4.1)\n",
            "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl==0.24.0->trl[peft]==0.24.0) (4.57.1)\n",
            "Requirement already satisfied: peft>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from trl[peft]==0.24.0) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.24.0->trl[peft]==0.24.0) (5.9.5)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl==0.24.0->trl[peft]==0.24.0) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (0.70.16)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes==0.48.1) (3.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl==0.24.0->trl[peft]==0.24.0) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl==0.24.0->trl[peft]==0.24.0) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (3.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes==0.48.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes==0.48.1) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl==0.24.0->trl[peft]==0.24.0) (1.3.1)\n",
            "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loralib-0.1.2-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: loralib, bitsandbytes, trl\n",
            "Successfully installed bitsandbytes-0.48.1 loralib-0.1.2 trl-0.24.0\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install  --upgrade torch torchvision torchaudio\n",
        "%pip install  --upgrade \"transformers[torch]==4.57.1\"\n",
        "%pip install  --upgrade accelerate datasets\n",
        "%pip install  \"bitsandbytes==0.48.1\" \"trl[peft]==0.24.0\" loralib huggingface_hub\n",
        "%pip install  gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfAOwVV5Kn3X"
      },
      "source": [
        "### Package Import (~ 30 secs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ICBpDqnpR7W"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, PeftModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dcec882"
      },
      "source": [
        "### Reproducibility Setting\n",
        "\n",
        "#### **Why Set the Seed and Ensure Deterministic Behavior?**\n",
        "\n",
        "TL;DR: TAs can get the same result in our machine as yours.\n",
        "\n",
        "In machine learning, especially when training models, randomness is often involved in various aspects, such as:\n",
        "\n",
        "*   **Weight Initialization:** The initial values of a model's weights are often randomly assigned.\n",
        "*   **Data Shuffling:** Datasets are typically shuffled before training to ensure the model doesn't learn the order of the data.\n",
        "*   **Dropout:** A regularization technique that randomly deactivates a percentage of neurons during training.\n",
        "*   **GPU Operations:** Some operations on GPUs can be non-deterministic due to how parallel computations are handled.\n",
        "\n",
        "**Setting the seed** for random number generators (like those in Python's `random`, NumPy, and PyTorch) ensures that the sequence of random numbers generated is the same every time you run your code.\n",
        "\n",
        "**Ensuring deterministic behavior** for operations (especially on GPUs) guarantees that the same inputs will always produce the same outputs.\n",
        "\n",
        "**Why is this important?**\n",
        "\n",
        "*   **Reproducibility:** This is the most crucial reason. By setting the seed and ensuring deterministic operations, you can reproduce your experimental results exactly. This is essential for debugging, comparing different models or hyperparameter settings, and for others to verify your work.\n",
        "*   **Debugging:** If your model is not performing as expected, being able to reproduce the exact same training run helps you isolate the source of the issue.\n",
        "*   **Comparison:** When comparing the performance of different models or techniques, you want to be sure that any observed differences are due to the changes you made, not random chance.\n",
        "\n",
        "In summary, setting the seed and aiming for deterministic operations are best practices in machine learning to ensure your experiments are reproducible and reliable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05Dkb_SbK5kt"
      },
      "outputs": [],
      "source": [
        "# Ensure reproducibility in training in pytorch and hf transformers\n",
        "def set_seed(seed: int = 42):\n",
        "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f6d5626"
      },
      "source": [
        "### Huggingface Token Setup (TODO)\n",
        "\n",
        "\n",
        "\n",
        "**Why you need an HF token:**\n",
        "\n",
        "Accessing models and datasets on the Hugging Face Hub often requires authentication, especially for private resources. Your Hugging Face token serves as your credential.\n",
        "\n",
        "**How Colab Secret Keys work:**\n",
        "\n",
        "Colab's secret manager provides a secure way to store sensitive information like your HF token. Instead of embedding the token directly in your code (which is a security risk), you store it as a secret in Colab. Your notebook can then access this secret without displaying the token value itself, preventing accidental exposure if you share your notebook.\n",
        "\n",
        "**How to get a HF token?**\n",
        "\n",
        "Ref: [GenAI 2025 HW1 Slide](https://speech.ee.ntu.edu.tw/~hylee/GenAI-ML/2025-fall-course-data/hw1.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10rgolwbWQvL"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Saving your token as a secret key in Colab is recommended for safety\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "# While it's strongly unrecommended, you can also type in your token for convenience\n",
        "#hf_token = \"<replace_with_your_token>\"\n",
        "\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x42hk_26NKow"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "Some useful utility functions. You SHOULD NOT MODIFY this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QtkdpTPZpIu"
      },
      "outputs": [],
      "source": [
        "def add_generation_prompt(tokenizer):\n",
        "    generation_chat_template = \"\"\"{{ bos_token }}\n",
        "{%- if messages[0]['role'] == 'system' -%}\n",
        "    {%- if messages[0]['content'] is string -%}\n",
        "        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\n",
        "    {%- else -%}\n",
        "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\n",
        "    {%- endif -%}\n",
        "    {%- set loop_messages = messages[1:] -%}\n",
        "{%- else -%}\n",
        "    {%- set first_user_prefix = \"\" -%}\n",
        "    {%- set loop_messages = messages -%}\n",
        "{%- endif -%}\n",
        "{%- for message in loop_messages -%}\n",
        "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
        "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
        "    {%- endif -%}\n",
        "    {%- if (message['role'] == 'assistant') -%}\n",
        "        {%- set role = \"model\" -%}\n",
        "    {%- else -%}\n",
        "        {%- set role = message['role'] -%}\n",
        "    {%- endif -%}\n",
        "    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \"\") }}\n",
        "    {%- if message['role'] == 'assistant' -%}\n",
        "        {% generation %}\n",
        "        {%- if message['content'] is string -%}\n",
        "            {{ message['content'] | trim }}\n",
        "        {%- elif message['content'] is iterable -%}\n",
        "            {%- for item in message['content'] -%}\n",
        "                {%- if item['type'] == 'image' -%}\n",
        "                    {{ '<start_of_image>' }}\n",
        "                {%- elif item['type'] == 'text' -%}\n",
        "                    {{ item['text'] | trim }}\n",
        "                {%- endif -%}\n",
        "            {%- endfor -%}\n",
        "        {%- else -%}\n",
        "            {{ raise_exception(\"Invalid content type\") }}\n",
        "        {%- endif -%}\n",
        "        {{ '<end_of_turn>\\n' }}\n",
        "        {% endgeneration %}\n",
        "    {%- else -%}\n",
        "        {%- if message['content'] is string -%}\n",
        "            {{ message['content'] | trim }}\n",
        "        {%- elif message['content'] is iterable -%}\n",
        "            {%- for item in message['content'] -%}\n",
        "                {%- if item['type'] == 'image' -%}\n",
        "                    {{ '<start_of_image>' }}\n",
        "                {%- elif item['type'] == 'text' -%}\n",
        "                    {{ item['text'] | trim }}\n",
        "                {%- endif -%}\n",
        "            {%- endfor -%}\n",
        "        {%- else -%}\n",
        "            {{ raise_exception(\"Invalid content type\") }}\n",
        "        {%- endif -%}\n",
        "        {{ '<end_of_turn>\\n' }}\n",
        "    {%- endif -%}\n",
        "{%- endfor -%}\n",
        "{%- if add_generation_prompt -%}\n",
        "    {{'<start_of_turn>model\n",
        "'}}\n",
        "{%- endif -%}\"\"\"\n",
        "    tokenizer.chat_template = generation_chat_template\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "# Define a helper function to load and set up the model and tokenizer\n",
        "def get_model_tokenizer(model_name, return_model=True, return_tokenizer=True):\n",
        "\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    if return_tokenizer:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        tokenizer = add_generation_prompt(tokenizer)\n",
        "    if return_model:\n",
        "        # Set up the quantization config\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "          load_in_4bit=True,\n",
        "          bnb_4bit_use_double_quant=True,\n",
        "          bnb_4bit_quant_type=\"nf4\",\n",
        "          bnb_4bit_compute_dtype=\"bfloat16\"\n",
        "        )\n",
        "        # Load the model from Huggingface and apply quantization\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "          model_name,\n",
        "          quantization_config=quant_config,\n",
        "          trust_remote_code=True,\n",
        "          low_cpu_mem_usage=True,\n",
        "        )\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "    if return_model and return_tokenizer:\n",
        "        tokenizer.pad_token_id = 0\n",
        "        tokenizer.eos_token_id = 1\n",
        "        model.eos_token_id = tokenizer.eos_token_id\n",
        "        model.config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def apply_adapter(model, adapter_name):\n",
        "    result_model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        adapter_name,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    return result_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T85o-8soWYg"
      },
      "source": [
        "### Set up Optional Variables (TODO)\n",
        "\n",
        "Expand this tab and modify the variables to enable and set up some optional operations (save to drive, wandb, ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7YlSANfodMs"
      },
      "outputs": [],
      "source": [
        "SAVE_FULL_MODEL = False # Set to True if you want to save the full model\n",
        "\n",
        "SAVE_TO_DRIVE = False # Set to True if you want to save the model to your Google Drive\n",
        "# Modify CKPT_PATH to the path you want to save\n",
        "CKPT_PATH = \"\"\n",
        "\n",
        "# CAUTION: If both SAVE_FULL_MODEL and SAVE_TO_DRIVE is set True, ensure your Google Drive has sufficient space.\n",
        "# Otherwise it is very possible that you exceed your drive space\n",
        "\n",
        "USE_WANDB = True # Set to True if you want to use wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RECBPG5TMl-b"
      },
      "source": [
        "## Phase 1: SFT - Instruction Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLhnrm6GOFEx"
      },
      "source": [
        "### Package Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI6coEmkOIPc"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import gradio as gr\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ysDC9mOg68"
      },
      "source": [
        "### Load the Model and Tokenizer (10 mins ~ 30 mins)\n",
        "\n",
        "- 10 mins: default, load TA-modified gemma\n",
        "- 30 mins: optional, load official gemma and apply some operation (only used when the TA's version has some issues)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g0HaShsRom_"
      },
      "source": [
        "**Caution**:\n",
        "\n",
        "TL;DR\n",
        "There are only 2 models you can finetune:\n",
        "1. `gemma-3-4b-pt` (requires modification for SFT later)\n",
        "2. `jaxon3062/gemma-3-4b-pt-chat`\n",
        "\n",
        "\n",
        "You can **ONLY** finetune `gemma-3-4b-pt` in HW7. Finetuning other models are **PROHIBITTED**.\n",
        "However, to save your time doing HW7,\n",
        "we recommend to use `jaxon3062/gemma-3-4b-pt-chat`\n",
        "(another version of `gemma-3-4b-pt` modified by TA).\n",
        "Otherwise you will have to spend an additional 30 mins to modify `gemma-3-4b-pt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mak41kPSTChd"
      },
      "source": [
        "**Load `jaxon3062/gemma-3-4b-pt-chat-bnb-4bit`** (~ 10 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "01d48a861a5b407696a802079b3f47f1",
            "d0d62113a2fd42e5b94905268e990cfd",
            "66f82b807e9c4070ab472f2657a665cc",
            "6c77b8a853c448b490f89dd8112a2f8a",
            "8b4b8a3349744771bd34b95cc0740897",
            "1c3e26b1bd834af2b2e4acd0c535d82c",
            "a9f625a74f7f4d5eb1e6a023eab9536a",
            "a51757505a524b04bab0f6d8bb73d9d4",
            "819f0f0fc8f74943a5108c1b4b1458b9",
            "361470b2b6ee4daba9104c59a102f93a",
            "7dc30d6c188246e9a4bf75d9d756dba2"
          ]
        },
        "id": "8RqKAOMbR0-V",
        "outputId": "96e87f49-b18f-4401-9251-2ab931e2b83b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01d48a861a5b407696a802079b3f47f1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "base_model_name = \"jaxon3062/gemma-3-4b-pt-chat\"\n",
        "model, tokenizer = get_model_tokenizer(base_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kh_uaJ_TPQw"
      },
      "source": [
        "**(Optional) Load `google/gemma-3-4b-pt` + Modification** (~ 30 mins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwC0iEpfTP5r"
      },
      "outputs": [],
      "source": [
        "load_original_gemma = False # Set `true` if you want to load the original gemma\n",
        "\n",
        "if load_original_gemma:\n",
        "    from trl import clone_chat_template\n",
        "    base_model_name = \"google/gemma-3-4b-pt\"\n",
        "    reference_chat_template_name = \"google/gemma-3-4b-it\"\n",
        "    model, tokenizer = get_model_tokenizer(base_model_name)\n",
        "    # Set up the chat format\n",
        "    model, tokenizer, added_tokens = clone_chat_template(model, tokenizer, reference_chat_template_name)\n",
        "    model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeKELf_EOfVc"
      },
      "source": [
        "### (Optional) Chat with the Model Before SFT\n",
        "\n",
        "You can chat with the model before SFT to observe how it behaves without instruction tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "086sqOxr54l3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "5bb50ad2-5a89-4546-f661-8c9d58cf29de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8899e6b98df2c5bf23.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8899e6b98df2c5bf23.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "def chat_interface(message, history):\n",
        "    # Format the chat history for the model\n",
        "    prompt = \"\"\n",
        "    SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
        "    prompt += SYSTEM_PROMPT\n",
        "    for human, assistant in history:\n",
        "        prompt += human\n",
        "        prompt += assistant\n",
        "    prompt += message\n",
        "\n",
        "    # Get the model response\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(model.device)\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=64,\n",
        "            do_sample=False,\n",
        "            eos_token_id=tokenizer.convert_tokens_to_ids([\"<eos>\", \"<end_of_turn>\"])\n",
        "        )\n",
        "        output = tokenizer.decode(out[0], skip_special_tokens=False).strip()\n",
        "        response = tokenizer.decode(out[0][len(inputs[\"input_ids\"]):], skip_special_tokens=True).strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.ChatInterface(\n",
        "    fn=chat_interface,\n",
        "    title=\"Gemma 3 4b Chat\",\n",
        "    description=\"Chat with the Gemma model.\",\n",
        "    examples=[\n",
        "        [\"Where is the capital of France?\"],\n",
        "        [\"Who is Julius Caesar?\"],\n",
        "    ],\n",
        ")\n",
        "\n",
        "iface.launch(debug=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hl53UUW6d8xc"
      },
      "source": [
        "### Load and Preprocess Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6WWEUlNeKTN"
      },
      "source": [
        "#### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZABzVxiR2ql"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# Using other datasets is PROHIBITED!\n",
        "ds = load_dataset(\"jaxon3062/smoltalk-gemma3-1024\", \"filtered-rich\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IPv3jfENp5F",
        "outputId": "931c162f-d41a-4a73-cdc8-c0d607203c14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['messages', 'source', 'idx', 'token_length'],\n",
              "        num_rows: 4772\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['messages', 'source', 'idx'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Sort the filtered dataset by token length in descending order\n",
        "NUM_PROC = 4\n",
        "MAX_TOKEN_LENGTH = 512\n",
        "\n",
        "ds_filtered = DatasetDict({\n",
        "    \"train\": ds[\"train\"],\n",
        "    \"test\": ds[\"test\"],\n",
        "})\n",
        "\n",
        "ds_filtered[\"train\"] = ds_filtered[\"train\"].map(\n",
        "    lambda x: {\n",
        "        \"token_length\": len(tokenizer.apply_chat_template(x[\"messages\"], tokenize=True, add_generation_prompt=False))\n",
        "    },\n",
        "    num_proc=NUM_PROC\n",
        ").sort(\"token_length\", reverse=True).filter(lambda x: x[\"token_length\"] < MAX_TOKEN_LENGTH, num_proc=NUM_PROC)\n",
        "ds_filtered[\"test\"] = ds_filtered[\"test\"].filter(lambda x: 0 <= x[\"idx\"] < 100, num_proc=NUM_PROC)\n",
        "\n",
        "ds_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f7804c5"
      },
      "source": [
        "#### Subsample the dataset for training (TODO)\n",
        "\n",
        "Subsampling a dataset before training, especially for large datasets, is often done for several reasons:\n",
        "\n",
        "1.  **Faster Training Times:** Training on a smaller subset of data is significantly faster than training on the entire dataset. This allows for quicker experimentation and iteration.\n",
        "2.  **Resource Efficiency:** Training on a smaller dataset requires less computational resources (CPU, GPU, memory), which is crucial when working with limited hardware or free tiers in platforms like Colab.\n",
        "3.  **Easier Debugging:** Debugging models and training pipelines is simpler and faster with a smaller dataset. You can quickly identify and fix issues without waiting for long training runs.\n",
        "4.  **Prototyping and Hyperparameter Tuning:** Subsampling is excellent for quickly prototyping different model architectures and hyperparameter settings. Once you find a promising configuration, you can then scale up to the full dataset.\n",
        "\n",
        "**Importance of Data Quality during Subsampling:**\n",
        "\n",
        "While subsampling provides efficiency, it's vital to ensure that the subsampled data is representative of the original dataset. Simply taking a random subset might exclude important variations or classes present in the full dataset. Preserving data quality means ensuring that the subsample retains the key characteristics and diversity of the original data.\n",
        "\n",
        "**Toy Example Analogy:**\n",
        "\n",
        "Imagine you have a bag of colorful marbles (your full dataset). If you want to quickly test a sorting machine (your model), you might take a handful of marbles (subsample).\n",
        "\n",
        "*   **Bad Subsampling:** If you just randomly grab a handful, you might end up with only red marbles, and your sorting machine won't learn how to sort blue or green marbles. This is like a non-representative subsample.\n",
        "*   **Better Subsampling:** A better approach would be to make sure your handful has a few marbles of each color present in the original bag. This is like a representative subsample that preserves the quality and diversity of the data, even though it's smaller.\n",
        "\n",
        "In real datasets, this means considering factors like class distribution, feature ranges, and other relevant characteristics when creating a subsample for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsldE3gLNuc1",
        "outputId": "e489f238-c3aa-44cd-d8a5-dc7fa9b97942"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['messages', 'source', 'idx', 'token_length'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['messages', 'source', 'idx'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Sample the top n samples (TODO)\n",
        "# The value can be set from 1 to the training set length\n",
        "# If the number exceeds the dataset length, errors will be raised\n",
        "n_samples = 100\n",
        "ds_sub = DatasetDict({\n",
        "    \"train\": ds_filtered[\"train\"].select(range(n_samples)),\n",
        "    \"test\": ds_filtered[\"test\"],\n",
        "})\n",
        "\n",
        "# Advanced(optional): sample the dataset by custom approaches\n",
        "\n",
        "ds_sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTgIoYm4cdLN",
        "outputId": "c373d7ad-bed0-4561-a8fa-7fff01b59572"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'content': \"You're an AI assistant for text re-writing. Rewrite the input text to make it more professional and formal while retaining its essential content.\",\n",
              "   'role': 'system'},\n",
              "  {'content': 'I\\'m thrilled to share the completion of my research project, \"Voices of the Enslaved: Stories of Resistance and Resilience.\" This journey has been both enlightening and emotionally charged. One of the most poignant moments was discovering a diary entry from Maria Johnson, an enslaved woman from Alabama. Her words not only captured the daily struggles but also the moments of defiance and hope that kept her and her community going.\\n\\nThis project has deepened my understanding of the complex social and cultural transformations during the American Civil War era. It has also reinforced my commitment to ensuring that these voices, often overlooked in historical narratives, are heard and honored. The research has been published in the *Journal of Southern History* and is now being expanded into a book that I hope will reach a broader audience.\\n\\nI invite my colleagues and fellow history enthusiasts to share any similar stories or insights you might have. Together, we can build a more comprehensive and inclusive understanding of our past. \\n\\nIf you\\'re interested in learning more, I\\'ll be hosting a webinar next month to discuss these findings in depth. Stay tuned for more details! #HistoricalResearch #CivilWar #SocialJustice',\n",
              "   'role': 'user'},\n",
              "  {'content': 'I am pleased to announce the completion of my research project, titled \"Voices of the Enslaved: Stories of Resistance and Resilience.\" This endeavor has been both intellectually enriching and emotionally profound. A particularly significant discovery was a diary entry by Maria Johnson, an enslaved woman from Alabama, whose words vividly illustrate the daily adversities and acts of defiance and hope that sustained her and her community.\\n\\nThis project has enhanced my comprehension of the intricate social and cultural transformations that occurred during the American Civil War era. It has also solidified my dedication to ensuring that these often marginalized voices are recognized and respected in historical discourse. The findings have been published in the *Journal of Southern History* and are currently being developed into a book, with the aim of reaching a wider audience.\\n\\nI extend an invitation to my colleagues and fellow history enthusiasts to contribute any related stories or insights you may possess. By collaborating, we can foster a more comprehensive and inclusive understanding of our shared history.\\n\\nFor those interested in delving deeper into these findings, I will be hosting a webinar next month to discuss the project in greater detail. Please stay tuned for further information. #HistoricalResearch #CivilWar #SocialJustice',\n",
              "   'role': 'assistant'}],\n",
              " 'source': 'smol-rewrite',\n",
              " 'idx': 50,\n",
              " 'token_length': 511}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# You can check how does a sample of the dataset look\n",
        "ds_sub[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxAu6cY2QEsc"
      },
      "source": [
        "### Prepare for Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKEdJQHFYOEA"
      },
      "source": [
        "#### (Optional) List All Modules in the Model\n",
        "\n",
        "You can print out and inspect what modules does a gemma 3 4b model contain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGxwSG-LdYDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "bc9417c8-6f7a-4f74-fed9-f0a77624889b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('',\n",
              "  Gemma3ForConditionalGeneration(\n",
              "    (model): Gemma3Model(\n",
              "      (vision_tower): SiglipVisionModel(\n",
              "        (vision_model): SiglipVisionTransformer(\n",
              "          (embeddings): SiglipVisionEmbeddings(\n",
              "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "            (position_embedding): Embedding(4096, 1152)\n",
              "          )\n",
              "          (encoder): SiglipEncoder(\n",
              "            (layers): ModuleList(\n",
              "              (0-26): 27 x SiglipEncoderLayer(\n",
              "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "                (self_attn): SiglipAttention(\n",
              "                  (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                  (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                  (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                  (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                )\n",
              "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "                (mlp): SiglipMLP(\n",
              "                  (activation_fn): GELUTanh()\n",
              "                  (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "                  (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (multi_modal_projector): Gemma3MultiModalProjector(\n",
              "        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
              "      )\n",
              "      (language_model): Gemma3TextModel(\n",
              "        (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-33): 34 x Gemma3DecoderLayer(\n",
              "            (self_attn): Gemma3Attention(\n",
              "              (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "              (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "              (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "              (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "            )\n",
              "            (mlp): Gemma3MLP(\n",
              "              (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "              (act_fn): GELUTanh()\n",
              "            )\n",
              "            (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "            (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "            (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "        (rotary_emb): Gemma3RotaryEmbedding()\n",
              "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "      )\n",
              "    )\n",
              "    (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
              "  )),\n",
              " ('model',\n",
              "  Gemma3Model(\n",
              "    (vision_tower): SiglipVisionModel(\n",
              "      (vision_model): SiglipVisionTransformer(\n",
              "        (embeddings): SiglipVisionEmbeddings(\n",
              "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "          (position_embedding): Embedding(4096, 1152)\n",
              "        )\n",
              "        (encoder): SiglipEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-26): 27 x SiglipEncoderLayer(\n",
              "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "              (self_attn): SiglipAttention(\n",
              "                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "              )\n",
              "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "              (mlp): SiglipMLP(\n",
              "                (activation_fn): GELUTanh()\n",
              "                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
              "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
              "    )\n",
              "    (language_model): Gemma3TextModel(\n",
              "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
              "      (layers): ModuleList(\n",
              "        (0-33): 34 x Gemma3DecoderLayer(\n",
              "          (self_attn): Gemma3Attention(\n",
              "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "          )\n",
              "          (mlp): Gemma3MLP(\n",
              "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "            (act_fn): GELUTanh()\n",
              "          )\n",
              "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "        )\n",
              "      )\n",
              "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "      (rotary_emb): Gemma3RotaryEmbedding()\n",
              "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower',\n",
              "  SiglipVisionModel(\n",
              "    (vision_model): SiglipVisionTransformer(\n",
              "      (embeddings): SiglipVisionEmbeddings(\n",
              "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "        (position_embedding): Embedding(4096, 1152)\n",
              "      )\n",
              "      (encoder): SiglipEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-26): 27 x SiglipEncoderLayer(\n",
              "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attn): SiglipAttention(\n",
              "              (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "              (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "              (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "              (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): SiglipMLP(\n",
              "              (activation_fn): GELUTanh()\n",
              "              (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "              (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model',\n",
              "  SiglipVisionTransformer(\n",
              "    (embeddings): SiglipVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "      (position_embedding): Embedding(4096, 1152)\n",
              "    )\n",
              "    (encoder): SiglipEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-26): 27 x SiglipEncoderLayer(\n",
              "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "          (self_attn): SiglipAttention(\n",
              "            (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "            (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "            (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "            (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): SiglipMLP(\n",
              "            (activation_fn): GELUTanh()\n",
              "            (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "            (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.embeddings',\n",
              "  SiglipVisionEmbeddings(\n",
              "    (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "    (position_embedding): Embedding(4096, 1152)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.embeddings.patch_embedding',\n",
              "  Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)),\n",
              " ('model.vision_tower.vision_model.embeddings.position_embedding',\n",
              "  Embedding(4096, 1152)),\n",
              " ('model.vision_tower.vision_model.encoder',\n",
              "  SiglipEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-26): 27 x SiglipEncoderLayer(\n",
              "        (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attn): SiglipAttention(\n",
              "          (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "          (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "          (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): SiglipMLP(\n",
              "          (activation_fn): GELUTanh()\n",
              "          (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "          (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers',\n",
              "  ModuleList(\n",
              "    (0-26): 27 x SiglipEncoderLayer(\n",
              "      (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "      (self_attn): SiglipAttention(\n",
              "        (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "        (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "        (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "        (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      )\n",
              "      (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): SiglipMLP(\n",
              "        (activation_fn): GELUTanh()\n",
              "        (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "        (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.0.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.1.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.2.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.3.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.4.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.5.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.6.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.7.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.8.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.9.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.10.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.11.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.12.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.13.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.14.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.15.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.16.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.17.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.18.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.19.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.20.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.21.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.22.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.23.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.24.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.25.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26',\n",
              "  SiglipEncoderLayer(\n",
              "    (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (self_attn): SiglipAttention(\n",
              "      (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "      (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    )\n",
              "    (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    (mlp): SiglipMLP(\n",
              "      (activation_fn): GELUTanh()\n",
              "      (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "      (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.layer_norm1',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.self_attn',\n",
              "  SiglipAttention(\n",
              "    (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.self_attn.k_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.self_attn.v_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.self_attn.q_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.self_attn.out_proj',\n",
              "  Linear4bit(in_features=1152, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.layer_norm2',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.mlp',\n",
              "  SiglipMLP(\n",
              "    (activation_fn): GELUTanh()\n",
              "    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "  )),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.mlp.activation_fn',\n",
              "  GELUTanh()),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.mlp.fc1',\n",
              "  Linear4bit(in_features=1152, out_features=4304, bias=True)),\n",
              " ('model.vision_tower.vision_model.encoder.layers.26.mlp.fc2',\n",
              "  Linear4bit(in_features=4304, out_features=1152, bias=True)),\n",
              " ('model.vision_tower.vision_model.post_layernorm',\n",
              "  LayerNorm((1152,), eps=1e-06, elementwise_affine=True)),\n",
              " ('model.multi_modal_projector',\n",
              "  Gemma3MultiModalProjector(\n",
              "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
              "  )),\n",
              " ('model.multi_modal_projector.mm_soft_emb_norm',\n",
              "  Gemma3RMSNorm((1152,), eps=1e-06)),\n",
              " ('model.multi_modal_projector.avg_pool',\n",
              "  AvgPool2d(kernel_size=4, stride=4, padding=0)),\n",
              " ('model.language_model',\n",
              "  Gemma3TextModel(\n",
              "    (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
              "    (layers): ModuleList(\n",
              "      (0-33): 34 x Gemma3DecoderLayer(\n",
              "        (self_attn): Gemma3Attention(\n",
              "          (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Gemma3MLP(\n",
              "          (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "          (act_fn): GELUTanh()\n",
              "        )\n",
              "        (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "        (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "        (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "        (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (rotary_emb): Gemma3RotaryEmbedding()\n",
              "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "  )),\n",
              " ('model.language_model.embed_tokens',\n",
              "  Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)),\n",
              " ('model.language_model.layers',\n",
              "  ModuleList(\n",
              "    (0-33): 34 x Gemma3DecoderLayer(\n",
              "      (self_attn): Gemma3Attention(\n",
              "        (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "        (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "        (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "        (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "        (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "        (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      )\n",
              "      (mlp): Gemma3MLP(\n",
              "        (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "        (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "        (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "        (act_fn): GELUTanh()\n",
              "      )\n",
              "      (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "      (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "      (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "      (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    )\n",
              "  )),\n",
              " ('model.language_model.layers.0',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.0.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.0.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.0.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.0.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.0.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.0.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.0.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.0.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.0.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.0.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.0.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.0.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.0.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.0.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.0.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.0.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.1',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.1.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.1.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.1.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.1.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.1.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.1.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.1.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.1.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.1.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.1.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.1.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.1.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.1.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.1.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.1.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.1.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.2',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.2.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.2.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.2.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.2.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.2.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.2.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.2.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.2.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.2.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.2.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.2.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.2.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.2.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.2.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.2.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.2.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.3',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.3.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.3.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.3.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.3.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.3.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.3.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.3.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.3.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.3.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.3.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.3.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.3.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.3.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.3.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.3.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.3.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.4',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.4.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.4.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.4.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.4.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.4.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.4.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.4.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.4.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.4.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.4.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.4.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.4.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.4.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.4.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.4.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.4.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.5',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.5.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.5.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.5.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.5.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.5.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.5.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.5.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.5.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.5.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.5.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.5.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.5.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.5.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.5.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.5.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.5.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.6',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.6.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.6.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.6.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.6.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.6.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.6.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.6.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.6.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.6.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.6.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.6.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.6.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.6.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.6.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.6.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.6.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.7',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.7.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.7.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.7.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.7.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.7.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.7.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.7.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.7.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.7.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.7.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.7.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.7.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.7.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.7.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.7.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.7.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.8',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.8.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.8.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.8.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.8.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.8.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.8.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.8.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.8.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.8.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.8.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.8.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.8.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.8.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.8.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.8.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.8.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.9',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.9.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.9.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.9.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.9.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.9.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.9.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.9.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.9.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.9.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.9.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.9.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.9.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.9.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.9.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.9.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.9.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.10',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.10.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.10.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.10.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.10.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.10.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.10.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.10.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.10.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.10.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.10.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.10.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.10.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.10.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.10.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.10.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.10.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.11',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.11.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.11.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.11.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.11.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.11.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.11.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.11.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.11.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.11.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.11.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.11.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.11.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.11.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.11.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.11.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.11.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.12',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.12.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.12.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.12.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.12.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.12.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.12.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.12.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.12.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.12.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.12.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.12.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.12.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.12.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.12.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.12.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.12.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.13',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.13.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.13.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.13.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.13.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.13.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.13.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.13.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.13.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.13.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.13.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.13.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.13.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.13.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.13.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.13.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.13.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.14',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.14.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.14.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.14.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.14.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.14.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.14.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.14.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.14.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.14.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.14.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.14.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.14.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.14.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.14.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.14.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.14.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.15',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.15.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.15.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.15.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.15.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.15.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.15.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.15.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.15.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.15.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.15.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.15.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.15.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.15.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.15.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.15.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.15.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.16',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.16.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.16.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.16.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.16.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.16.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.16.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.16.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.16.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.16.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.16.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.16.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.16.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.16.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.16.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.16.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.16.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.17',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.17.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.17.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.17.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.17.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.17.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.17.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.17.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.17.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.17.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.17.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.17.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.17.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.17.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.17.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.17.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.17.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.18',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.18.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.18.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.18.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.18.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.18.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.18.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.18.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.18.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.18.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.18.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.18.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.18.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.18.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.18.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.18.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.18.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.19',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.19.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.19.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.19.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.19.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.19.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.19.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.19.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.19.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.19.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.19.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.19.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.19.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.19.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.19.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.19.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.19.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.20',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.20.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.20.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.20.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.20.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.20.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.20.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.20.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.20.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.20.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.20.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.20.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.20.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.20.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.20.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.20.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.20.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.21',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.21.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.21.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.21.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.21.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.21.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.21.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.21.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.21.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.21.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.21.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.21.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.21.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.21.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.21.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.21.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.21.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.22',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.22.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.22.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.22.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.22.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.22.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.22.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.22.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.22.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.22.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.22.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.22.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.22.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.22.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.22.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.22.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.22.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.23',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.23.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.23.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.23.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.23.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.23.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.23.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.23.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.23.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.23.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.23.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.23.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.23.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.23.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.23.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.23.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.23.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.24',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.24.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.24.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.24.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.24.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.24.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.24.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.24.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.24.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.24.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.24.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.24.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.24.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.24.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.24.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.24.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.24.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.25',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.25.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.25.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.25.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.25.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.25.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.25.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.25.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.25.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.25.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.25.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.25.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.25.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.25.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.25.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.25.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.25.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.26',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.26.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.26.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.26.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.26.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.26.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.26.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.26.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.26.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.26.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.26.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.26.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.26.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.26.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.26.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.26.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.26.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.27',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.27.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.27.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.27.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.27.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.27.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.27.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.27.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.27.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.27.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.27.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.27.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.27.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.27.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.27.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.27.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.27.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.28',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.28.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.28.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.28.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.28.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.28.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.28.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.28.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.28.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.28.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.28.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.28.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.28.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.28.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.28.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.28.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.28.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.29',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.29.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.29.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.29.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.29.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.29.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.29.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.29.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.29.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.29.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.29.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.29.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.29.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.29.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.29.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.29.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.29.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.30',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.30.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.30.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.30.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.30.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.30.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.30.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.30.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.30.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.30.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.30.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.30.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.30.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.30.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.30.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.30.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.30.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.31',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.31.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.31.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.31.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.31.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.31.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.31.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.31.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.31.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.31.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.31.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.31.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.31.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.31.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.31.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.31.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.31.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.32',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.32.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.32.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.32.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.32.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.32.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.32.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.32.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.32.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.32.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.32.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.32.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.32.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.32.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.32.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.32.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.32.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.33',\n",
              "  Gemma3DecoderLayer(\n",
              "    (self_attn): Gemma3Attention(\n",
              "      (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "      (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "      (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "      (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "      (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    )\n",
              "    (mlp): Gemma3MLP(\n",
              "      (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "      (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "      (act_fn): GELUTanh()\n",
              "    )\n",
              "    (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "    (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.33.self_attn',\n",
              "  Gemma3Attention(\n",
              "    (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "    (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "    (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "    (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "    (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "  )),\n",
              " ('model.language_model.layers.33.self_attn.q_proj',\n",
              "  Linear4bit(in_features=2560, out_features=2048, bias=False)),\n",
              " ('model.language_model.layers.33.self_attn.k_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.33.self_attn.v_proj',\n",
              "  Linear4bit(in_features=2560, out_features=1024, bias=False)),\n",
              " ('model.language_model.layers.33.self_attn.o_proj',\n",
              "  Linear4bit(in_features=2048, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.33.self_attn.q_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.33.self_attn.k_norm',\n",
              "  Gemma3RMSNorm((256,), eps=1e-06)),\n",
              " ('model.language_model.layers.33.mlp',\n",
              "  Gemma3MLP(\n",
              "    (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "    (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "    (act_fn): GELUTanh()\n",
              "  )),\n",
              " ('model.language_model.layers.33.mlp.gate_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.33.mlp.up_proj',\n",
              "  Linear4bit(in_features=2560, out_features=10240, bias=False)),\n",
              " ('model.language_model.layers.33.mlp.down_proj',\n",
              "  Linear4bit(in_features=10240, out_features=2560, bias=False)),\n",
              " ('model.language_model.layers.33.mlp.act_fn', GELUTanh()),\n",
              " ('model.language_model.layers.33.input_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.33.post_attention_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.33.pre_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.layers.33.post_feedforward_layernorm',\n",
              "  Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.norm', Gemma3RMSNorm((2560,), eps=1e-06)),\n",
              " ('model.language_model.rotary_emb', Gemma3RotaryEmbedding()),\n",
              " ('model.language_model.rotary_emb_local', Gemma3RotaryEmbedding()),\n",
              " ('lm_head', Linear(in_features=2560, out_features=262208, bias=False))]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "list(model.named_modules())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agwn8yFeYmYk"
      },
      "source": [
        "#### Set up the model with PEFT (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-YwQtnIdy_t",
        "outputId": "c368a2e6-82d0-4a18-d11c-bcdcf3283df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 4,708,864 || all params: 4,304,788,336 || trainable%: 0.1094\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Gemma3ForConditionalGeneration(\n",
              "  (model): Gemma3Model(\n",
              "    (vision_tower): SiglipVisionModel(\n",
              "      (vision_model): SiglipVisionTransformer(\n",
              "        (embeddings): SiglipVisionEmbeddings(\n",
              "          (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "          (position_embedding): Embedding(4096, 1152)\n",
              "        )\n",
              "        (encoder): SiglipEncoder(\n",
              "          (layers): ModuleList(\n",
              "            (0-26): 27 x SiglipEncoderLayer(\n",
              "              (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "              (self_attn): SiglipAttention(\n",
              "                (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "                (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
              "              )\n",
              "              (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "              (mlp): SiglipMLP(\n",
              "                (activation_fn): GELUTanh()\n",
              "                (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
              "                (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (multi_modal_projector): Gemma3MultiModalProjector(\n",
              "      (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
              "      (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
              "    )\n",
              "    (language_model): Gemma3TextModel(\n",
              "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
              "      (layers): ModuleList(\n",
              "        (0-33): 34 x Gemma3DecoderLayer(\n",
              "          (self_attn): Gemma3Attention(\n",
              "            (q_proj): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
              "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
              "            (o_proj): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
              "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
              "          )\n",
              "          (mlp): Gemma3MLP(\n",
              "            (gate_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "            (up_proj): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
              "            (down_proj): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
              "            (act_fn): GELUTanh()\n",
              "          )\n",
              "          (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "          (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "          (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "          (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "        )\n",
              "      )\n",
              "      (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
              "      (rotary_emb): Gemma3RotaryEmbedding()\n",
              "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# TODO: Try different Lora parameters\n",
        "\n",
        "# Lora rank: set any number you want; recommend 2, 4, 8, 16, 32, ...\n",
        "LORA_RANK = 8\n",
        "\n",
        "# Lora alpha: a Lora matrix scaling coefficient: set 32 is common, or you can set twice the rank\n",
        "LORA_ALPHA = 32\n",
        "\n",
        "# Modules to apply Lora: check module names you want in the previous cell\n",
        "# You can check available modules by running the  above optional cell to list them\n",
        "# Or you can choose from this list: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"]\n",
        "\n",
        "# Lora dropout: set 0-0.2 to prevent overfit\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Tokens that will be trained (in HW7, newly added chat template tokens require training)\n",
        "# You should NOT modify this setting\n",
        "chat_tokens = tokenizer.convert_tokens_to_ids([\"<bos>\", \"<eos>\", \"<start_of_turn>\", \"<end_of_turn>\", \"<pad>\"])\n",
        "trainable_token_indices=chat_tokens\n",
        "\n",
        "# You are NOT REQUIRED TO modify the code below\n",
        "lora_cfg = LoraConfig(\n",
        "  r=LORA_RANK,\n",
        "  lora_alpha=LORA_ALPHA,\n",
        "  target_modules=target_modules,\n",
        "  trainable_token_indices=trainable_token_indices,\n",
        "  lora_dropout=LORA_DROPOUT,\n",
        "  bias=\"none\", task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_cfg)\n",
        "peft_model.print_trainable_parameters()\n",
        "peft_model.unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8g6C5ddMsy1"
      },
      "source": [
        "### Training with SFTTrainer (TODO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IZ9TW2PMlkl"
      },
      "source": [
        "#### (Optional) Import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BeebiXVo04j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9f3ae3-c63d-446f-c95a-ab5526465304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjaxon3062\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ],
      "source": [
        "if USE_WANDB:\n",
        "    import wandb\n",
        "    from google.colab import userdata\n",
        "\n",
        "    try:\n",
        "        wandb_token = userdata.get('WANDB_TOKEN')\n",
        "        wandb.login(key=wandb_token)\n",
        "    except:\n",
        "        print(\"Warning: Wandb API key is not set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbOtqHMkj4cT"
      },
      "source": [
        "#### Train (10 mins ~ 5 hrs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Modify training hyperparameters\n",
        "EPOCH = 1   # 1 ~ 5\n",
        "BATCH_SIZE = 4   # 2 ~ 64\n",
        "LR = \"5e-4\""
      ],
      "metadata": {
        "id": "rVTKTxvtpOxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iUc1FBlkkZzU",
        "outputId": "cf183005-0882-45c6-e877-82baabfac241"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251201_173247-d8zghb6e</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jaxon3062/GenAI2025%20HW7/runs/d8zghb6e' target=\"_blank\">gemma3-4b-chat_lora-rk8-a32_l2048_bs4_lr5e-4-100_ep1</a></strong> to <a href='https://wandb.ai/jaxon3062/GenAI2025%20HW7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jaxon3062/GenAI2025%20HW7' target=\"_blank\">https://wandb.ai/jaxon3062/GenAI2025%20HW7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jaxon3062/GenAI2025%20HW7/runs/d8zghb6e' target=\"_blank\">https://wandb.ai/jaxon3062/GenAI2025%20HW7/runs/d8zghb6e</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, mcp] in use.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up SFTConfig\n",
            "Setting up SFTTrainer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 2, 'pad_token_id': 0}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 02:20, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.860500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.535700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.732700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.667500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.780600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.481500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.612400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.849100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.720900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.647000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.733500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.673600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.663800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.578500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.359700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.513600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.869400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.644100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.773500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.498300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.564100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.573800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.463900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.556400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.459400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/entropy</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td>  </td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/mean_token_accuracy</td><td></td></tr><tr><td>train/num_tokens</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1112595373142400.0</td></tr><tr><td>train/entropy</td><td>1.49154</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>25</td></tr><tr><td>train/grad_norm</td><td>0.78168</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.4594</td></tr><tr><td>train/mean_token_accuracy</td><td>0.85558</td></tr><tr><td>train/num_tokens</td><td>51100</td></tr><tr><td>train_loss</td><td>0.67255</td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gemma3-4b-chat_lora-rk8-a32_l2048_bs4_lr5e-4-100_ep1</strong> at: <a href='https://wandb.ai/jaxon3062/GenAI2025%20HW7/runs/d8zghb6e' target=\"_blank\">https://wandb.ai/jaxon3062/GenAI2025%20HW7/runs/d8zghb6e</a><br> View project at: <a href='https://wandb.ai/jaxon3062/GenAI2025%20HW7' target=\"_blank\">https://wandb.ai/jaxon3062/GenAI2025%20HW7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251201_173247-d8zghb6e/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed and model saved.\n"
          ]
        }
      ],
      "source": [
        "# ------\n",
        "\n",
        "# Modify the code below with caution.\n",
        "# You can modify them, but make sure you know what you are doing.\n",
        "\n",
        "MINI_BATCH_SIZE = 2\n",
        "MODEL_MAX_LENGTH = 2048\n",
        "\n",
        "# Set the run name you like.\n",
        "# We recommend to set something that reminds you your training settings. Such as:\n",
        "run_name = f\"gemma3-4b-chat_lora-rk{LORA_RANK}-a{LORA_ALPHA}_l{MODEL_MAX_LENGTH}_bs{BATCH_SIZE}_lr{LR}-{n_samples}_ep{EPOCH}\"\n",
        "\n",
        "# Optional\n",
        "if USE_WANDB:\n",
        "    wandb.init(\n",
        "        project=\"GenAI2025 HW7\",\n",
        "        name=run_name,\n",
        "    )\n",
        "\n",
        "# Ref: https://huggingface.co/docs/trl/sft_trainer\n",
        "print(\"Setting up SFTConfig\")\n",
        "args = SFTConfig(\n",
        "    per_device_train_batch_size=MINI_BATCH_SIZE,\n",
        "    gradient_accumulation_steps=BATCH_SIZE // MINI_BATCH_SIZE,\n",
        "    num_train_epochs=EPOCH,\n",
        "    fp16=True,\n",
        "    output_dir=run_name,\n",
        "    max_length=MAX_TOKEN_LENGTH,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
        "    lr_scheduler_kwargs={\n",
        "        \"min_lr\": 1e-6,\n",
        "        \"num_cycles\": 0.5,\n",
        "    },\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=float(LR),\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"wandb\" if USE_WANDB else None,  # Optional: report to wandb if USE_WANDB = True\n",
        "    run_name=run_name,\n",
        "    logging_steps=1,\n",
        "    assistant_only_loss=True,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "print(\"Setting up SFTTrainer\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=args,\n",
        "    train_dataset=ds_sub[\"train\"],\n",
        "    eval_dataset=ds_sub[\"test\"],\n",
        "    peft_config=lora_cfg,\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "if USE_WANDB:\n",
        "    wandb.finish()\n",
        "\n",
        "trainer.save_model(run_name + \"_adapter\")\n",
        "merged_model = trainer.model.merge_and_unload()\n",
        "print(\"Training completed and model saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QemLEf0qnu1p"
      },
      "source": [
        "#### (Optional) Save adapter checkpoint to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQrXtUcPD3MK"
      },
      "outputs": [],
      "source": [
        "# Move the saved adapter to Google Drive\n",
        "# Make sure you mount your Drive first!\n",
        "if SAVE_TO_DRIVE and CKPT_PATH:\n",
        "    %mv {run_name}_adapter {CKPT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wITM9dFCsPGF"
      },
      "source": [
        "#### (Optional) Save full model and tokenizer checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBEgSMNLsO0H"
      },
      "outputs": [],
      "source": [
        "# Save the full merged model\n",
        "if SAVE_FULL_MODEL:\n",
        "    merged_model.save_pretrained(run_name)\n",
        "    tokenizer.save_pretrained(run_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clean up objects to make space for inference"
      ],
      "metadata": {
        "id": "YN_5TMHqgi9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del trainer\n",
        "del model, tokenizer\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "PCsIeAGngi0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "384d611f-be8e-467d-e503-c68a10254e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2081"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy1jWR7AtqeS"
      },
      "source": [
        "### Evaluate on test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5lhehcGxlOz"
      },
      "source": [
        "#### Load  model from full model or adapter (TODO) (5 ~ 10mins)\n",
        "\n",
        "Run this cell if you shutdown or restart the Colab runtime between training and inferencing. In other words, if a trained model or adapter is saved and you want to load it for inference, run this cell.\n",
        "\n",
        "**Caution**: Modify the variables according to your situation (load from model or from adapter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "aaa0cd28283648a9b37d408075f7af6f",
            "3c3b46507eec417f8b31671737bb8a0d",
            "4cb75409200c42dfa2aa118daf9707b0",
            "99d8509b468b481e83c547bcb836509a",
            "0fd26a0ef74944b18114eec7b9ac00a8",
            "d65d06ec84f44a1a934fbb3743a4a7bc",
            "330aec6cbbaa444ba90d21fc13a8f29b",
            "91fc1a60db804c9eb099870bea16936f",
            "11bc053f8b1945b6930a3a1880fca34e",
            "00b543ede66e453aa02770c7cd7cde8e",
            "054e9b12b970460ab21b1d1567b109fa"
          ]
        },
        "id": "pW4Rn0egxte2",
        "outputId": "784821b8-c4e6-48e3-d4cc-1a1cb865773d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaa0cd28283648a9b37d408075f7af6f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Set to True if you want to load from a full model\n",
        "LOAD_FROM_FULL_MODEL = False\n",
        "# Provide your model path here\n",
        "LOAD_MODEL_PATH = \"\"\n",
        "\n",
        "# Set to True if you want to load from an adapter\n",
        "LOAD_FROM_ADAPTER = True\n",
        "# Provide your adapter path here\n",
        "ADAPTER_PATH = run_name + \"_adapter\"\n",
        "\n",
        "if LOAD_FROM_FULL_MODEL and LOAD_FROM_ADAPTER:\n",
        "    raise ValueError(\"Cannot load from both checkpoint and adapter at the same time.\")\n",
        "\n",
        "if not LOAD_FROM_FULL_MODEL and not LOAD_FROM_ADAPTER:\n",
        "    raise ValueError(\"Either LOAD_FROM_FULL_MODEL or LOAD_FROM_ADAPTER should be True.\")\n",
        "\n",
        "if LOAD_FROM_FULL_MODEL and os.path.isdir(LOAD_MODEL_PATH):\n",
        "    try:\n",
        "        model, tokenizer = get_model_tokenizer(LOAD_MODEL_PATH, return_model=True, return_tokenizer=True)\n",
        "    except:\n",
        "        raise ValueError(\"Cannot load model from model. This may caused by invalid model path.\")\n",
        "elif LOAD_FROM_ADAPTER and os.path.isdir(ADAPTER_PATH):\n",
        "    try:\n",
        "        if \"model\" not in locals() and \"model\" not in globals():\n",
        "            if \"base_model_name\" not in locals() and \"model\" not in globals():\n",
        "                base_model_name = \"jaxon3062/gemma-3-4b-pt-chat\"\n",
        "            model, tokenizer = get_model_tokenizer(base_model_name)\n",
        "        model = apply_adapter(model, ADAPTER_PATH)\n",
        "    except:\n",
        "        raise ValueError(\"Cannot load model from adapter. This may caused by invalid adapter path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load evaluation dataset"
      ],
      "metadata": {
        "id": "2I6ZCGL8B3TK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "efbaacad99a24ecf99ca834669279a65",
            "cb5c3acf28a649c5b1ac51232c63adfa",
            "c4a3ca50124e4b11a7c1408b5a24d25e",
            "6067b48a0a8349568ef95c387c76761b",
            "320d502db46847099b5e0b77397fe973",
            "e2d02c3740d24c5bb59cd90150abd406",
            "ff48dc7439e34298a8fac0b386238554",
            "ae39150ae88d4f0aab5b2595ad88dc06",
            "04c49c3d0dfa4f1ca7795ee0c85d0978",
            "93e890ea7ea947ae95a0fae4de8f4e07",
            "7540a65322e94475b8cb1904e0c9f597",
            "de6861ce472e4d74b7b6f8a73c3db26d",
            "5105b2a54e874eeba11ac6e434fffc69",
            "d8d15f7364f44a5daa1e2b58bed47b75",
            "cc314ef4506049459097075cc1ebebbb",
            "aed4c53d456a477d9e2bbb409e34b5b5",
            "657968216c894cbfbe50ce26ffd01109",
            "3aa1ce38991a4d1993d67dfb1de52c77",
            "623da89c27114d8ead150788da3732b8",
            "65300f2674ae47e882cf133b771dfe1e",
            "4f2ea10af02a4470ab047739446a010d",
            "e9fcc6a7fdd945779145ee10c665ac47"
          ]
        },
        "id": "r3rtqEL56eCn",
        "outputId": "acba3237-7182-4ca1-de98-febc52cc27ea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "short-50/test-00000-of-00001.parquet:   0%|          | 0.00/46.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efbaacad99a24ecf99ca834669279a65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de6861ce472e4d74b7b6f8a73c3db26d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['messages', 'source', 'idx'],\n",
              "    num_rows: 50\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# DO NOT CHANGE this cell\n",
        "ds_eval = load_dataset(\"jaxon3062/genai-ml-2025-hw7-eval\", \"short-50\", split=\"test\")\n",
        "ds_eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wobHyfy3xpJa"
      },
      "source": [
        "#### Inference (~ 1 hr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1sBXx1eTMdi",
        "outputId": "91776628-3a0e-45fb-c803-0d0faa506d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "  2%|         | 1/50 [01:18<1:03:44, 78.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "I am concerned my lack of a college degree in my field hurts my long term career prospects despite my relevant work experience and skills. Is this a valid concern?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Your concern is understandable, as a college degree is often seen as a standard requirement in many industries. However, it's essential to consider the value of your relevant work experience and skills in your field. Many successful professionals have built their careers without a degree in their specific field, and some have even transitioned into new fields without formal education.\n",
            "\n",
            "The key is to focus on demonstrating your expertise and showcasing your skills to potential employers or clients. Your work experience, especially if it's extensive and relevant, can be just as valuable as a degree. Additionally, many fields place more emphasis on skills and experience than formal education.\n",
            "\n",
            "That being said, there are certain industries or roles where a degree is mandatory or highly preferred, such as law, medicine, or academia. If you're in one of these fields, it might be more challenging to advance without a degree.\n",
            "\n",
            "Consider the following: Have you been able to progress in your career so far without a degree? Are there any specific job titles or companies you're interested in that require a degree? Are there alternative certifications, training programs, or courses that could help bridge any gaps in your education?\n",
            "\n",
            "It's also worth noting that many employers value the skills and experience you've gained through your work, and some may even prefer candidates with practical experience over those with a degree but limited experience. Ultimately, your career prospects depend on various factors, and a degree is just one aspect of the equation.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "Can I counter not having a degree with a professional certification?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "A professional certification can be a great way to counter not having a degree, especially if it's a well-recognized and respected certification in your industry. Certifications demonstrate that you have a certain level of expertise and knowledge in a specific area, which can be just as valuable as a degree.\n",
            "\n",
            "Many certifications are designed to validate skills and experience, and some are even considered equivalent to a degree in certain fields. For example, in the IT industry, certifications like CompTIA or Cisco can be highly regarded and often preferred by employers.\n",
            "\n",
            "A certification can also show potential employers that you're committed to ongoing learning and professional development, which is essential in many fields. Additionally, certifications can be a great way to specialize in a particular area of your industry, which can make you more attractive to employers looking for specific skills.\n",
            "\n",
            "It's essential to choose a certification that's relevant to your field and career goals. Research the certification and ensure it's well-recognized and respected in your industry. Also, be prepared to explain how the certification aligns with your experience and skills, and how it demonstrates your expertise.\n",
            "\n",
            "Keep in mind that while a certification can be beneficial, it's not a direct replacement for a degree. However, it can be a powerful addition to your resume and a valuable tool in demonstrating your skills and expertise to potential employers.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "How do I determine if a professional certification is well-recognized and respected within the industry?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "To determine if an industry certification is recognized and respected, you should look at several sources of information. Here are some steps you can take:\n",
            "\n",
            "1. <strong>Check Industry Websites</strong>: Visit the official websites of major industry associations or organizations related to your profession. These sites often list approved certifications and provide details about each certification, including its recognition and acceptance among employers.\n",
            "2. <b>Search Online</b>: Use search engines to find articles, reviews, or forums discussing the certification. Look for mentions of the certification's reputation, acceptance, and relevance to your career path.\n",
            "3. <em>Consult Career Counselors</em>: Contact career counselors or advisors who specialize in your profession or industry. They can provide insights into the most reputable certifications in your area and their impact on career advancement.\n",
            "4. <i>Ask Employers</i>: Reach out to current or former employees in your desired field. Ask them about the importance they place on certifications when hiring or promoting individuals.\n",
            "5. <u>Review Job Listings</u>: Search for job postings in your target industry. Note the types of certifications mentioned in the requirements section. This will give you a sense of what certifications are commonly sought after.\n",
            "6. <blockquote>Compare Certification Requirements</blockquote>\n",
            "\n",
            "  * Compare the requirements of different certifications. Look at the scope of knowledge covered, the duration of the program, and the cost. Choose a certification whose requirements align closely with your career objectives.\n",
            "\n",
            "7. <h5>Seek Feedback</h5>\n",
            "\n",
            "  If possible, ask colleagues or peers who hold the certification for their opinions. Their firsthand experiences can offer valuable insights.\n",
            "\n",
            "Remember, the ultimate goal is to secure a position that matches your qualifications and interests. A certification can enhance your candidacy, but it'll ultimately come down to your ability to showcase your skills, experience, achievements, and alignment with the employer's needs.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|         | 2/50 [02:24<56:50, 71.06s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Write a program in python that determines the maximum of two numbers without using built-in functions like max().<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```python\n",
            "def find_max(a, b):\n",
            "    if a > b:\n",
            "        return a\n",
            "    else:\n",
            "        return b\n",
            "\n",
            "# Test the function\n",
            "num1 = 10\n",
            "num2 = 20\n",
            "print(f\"The maximum of {num1} and {num2} is: {find_max(num1, num2)}\")\n",
            "```\n",
            "\n",
            "Alternatively, you can also use a ternary operator for a more concise solution:\n",
            "\n",
            "```python\n",
            "def find_max(a, b):\n",
            "    return a if a > b else b\n",
            "\n",
            "# Test the function\n",
            "num1 = 10\n",
            "num2 = 20\n",
            "print(f\"The maximum of {num1} and {num2} is: {find_max(num1, num2)}\")\n",
            "```<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "Write a program that, given a positive integer n, will show all the numbers from 1 to n that are perfect squares.\n",
            "#Test the function\n",
            "n = 20 \n",
            "find_perfect_squares(n)<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```python\n",
            "import math\n",
            "\n",
            "def find_perfect_squares(n):\n",
            "    for i in range(1, n+1):\n",
            "        root = math.sqrt(i)\n",
            "        if int(root + 0.5) ** 2 == i:\n",
            "            print(i)\n",
            "\n",
            "# Test the function\n",
            "n = 20\n",
            "find_perfect_squares(n)\n",
            "```\n",
            "\n",
            "Alternatively, you can also generate perfect squares directly without checking all numbers:\n",
            "\n",
            "```python\n",
            "def find_perfect_squares(n):\n",
            "    i = 1\n",
            "    while i * i <= n:\n",
            "        print(i * i)\n",
            "        i += 1\n",
            "\n",
            "# Test the function\n",
            "n = 20\n",
            "find_perfect_squares(n)\n",
            "```<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "Write a program that finds the roots of a quadratic equation ax^2 + bx + c = 0. The program should ask the user to input the values of a, b, and c. \n",
            "\n",
            "#Test the function\n",
            "find_roots()<end_of_turn>\n",
            "<start_of_turn>model\n",
            "import cmath\n",
            "\n",
            "def calculate_roots(a=None, b=None):\n",
            "   if not (isinstance(a,(int,float)) and isinstance(b,(int , float))):\n",
            "       raise ValueError(\"Invalid input type\")\n",
            "\n",
            "   discriminant = b**2 - 4*a*c\n",
            "   real_part = (-b + cmath.sqrt((b**2-4*c*a)))/(2*a)\n",
            "   imaginary_part=cmath.sqrt(-discriminant)/(2*c)\n",
            "\n",
            "   return real_part, imaginary_part\n",
            "\n",
            "# Example usage\n",
            "a = 3\n",
            "b = 4\n",
            "c = 7\n",
            "result = calculate_root(a , b , c)\n",
            "print(\"Real part:\", result[0])\n",
            "print (\"Imaginary part:\",result[1])\n",
            "\n",
            "# Alternatively, you could use a try-except block to handle invalid inputs:\n",
            "try:\n",
            "    a = float(input(\"Enter value of a: \"))\n",
            "    b = float (input(\"enter value of b: \")) \n",
            "    c = float  (input (\"enter valueof c: \"))\n",
            "\n",
            "    result =calculate_roots (a,b,c)\n",
            "    print(\"Roots:\", result)\n",
            "except ValueError as e:\n",
            "     print(\"Error:\",e)\n",
            "finally:\n",
            "      print(\"Program execution completed.\")\n",
            "```\n",
            "Note: This code assumes that the user will provide valid numerical inputs. If they enter non-numeric or negative values, it may raise an exception. You might want to add additional error handling or validation steps based on your requirements.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|         | 3/50 [02:30<32:40, 41.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "A snail is at the bottom of a 20-foot well. Each day, it climbs up 3 feet, but at night, it slips back 2 feet. How many days will it take for the snail to reach the top of the well?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "To solve this problem, let's break it down and analyze the snail's progress. The snail climbs 3 feet up during the day, but slips back 2 feet at night, so it effectively moves 1 foot up each day.\n",
            "\n",
            "However, on the day the snail reaches or surpasses the top of the well, it won't slip back at night because it will have already escaped the well. This means we need to consider the final day separately.\n",
            "\n",
            "The well is 20 feet deep, and the snail effectively climbs 1 foot per day. So, if we subtract the final day's climb from the total height of the well, we can figure out how many days it takes for the snail to climb the remaining distance.\n",
            "\n",
            "If we subtract the snail's daily climb of 3 feet from the well's height, we get 20 - 3 = 17 feet. Since the snail effectively climbs 1 foot per day, it will take 17 days for the snail to climb 17 feet. \n",
            "\n",
            "On the 18th day, when the snail climbs 3 feet, it will reach the top of the well and won't slip back. Therefore, it will take the snail 18 days to reach the top of the well.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "How many days would it take if it was a 25-foot well instead of 20?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "To find out how many days it would take for the snail to climb a 25-foot well, we can use the same logic as before. The snail effectively climbs 1 foot per day, but on the final day, it climbs 3 feet without slipping back.\n",
            "\n",
            "If we subtract the snail's final climb of 3 feet from the well's height, we get 25 - 3 = 22 feet. Since the snail effectively climbs 1 foot per day, it will take 22 days for the snail to climb 22 feet. \n",
            "\n",
            "On the 23rd day, when the snail climbs 3 feet, it will reach the top of the well and won't slip back. Therefore, it will take the snail 23 days to reach the top of the 25-foot well.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "How about a 19-foot well.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "For a 9-inch well, the snail would also climb 3 inches per day until reaching the top in 6 days.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|         | 4/50 [03:44<41:34, 54.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "What are the six primary reasons data is important to businesses?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Data is crucial for businesses, and its importance can be summarized into six primary reasons:\n",
            "\n",
            "Data informs business decisions by providing insights into customer behavior, market trends, and operational performance, enabling companies to make informed choices. \n",
            "\n",
            "Data helps businesses measure and track performance, allowing them to set realistic goals, identify areas for improvement, and adjust their strategies accordingly. \n",
            "\n",
            "Data drives innovation by revealing opportunities for growth, such as untapped markets, emerging trends, and unmet customer needs, which can inspire new products, services, or business models. \n",
            "\n",
            "Data enhances customer experiences by providing companies with a deeper understanding of their target audience, enabling them to tailor their offerings, improve customer service, and build stronger relationships.\n",
            "\n",
            "Data improves operational efficiency by identifying bottlenecks, streamlining processes, and optimizing resource allocation, leading to cost savings and increased productivity. \n",
            "\n",
            "Data supports strategic planning by providing a fact-based foundation for long-term planning, enabling companies to anticipate and respond to changes in their industry, and stay ahead of the competition.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "What are 4 of the most well used pieces of data, for businesses?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Businesses often rely on various types of data to inform their decisions, but four of the most well-used pieces of data are:\n",
            "\n",
            "1. **Customer demographics**: This includes data about the age, location, income level, education level, and occupation of a company's target audience. Understanding customer demographics helps businesses tailor their marketing efforts, product offerings, and customer service to meet the specific needs and preferences of their audience.\n",
            "\n",
            "2. **Website analytics**: This data provides insights into how users interact with a company's website, including metrics such as page views, bounce rate, average session duration, and conversion rates. Website analytics help businesses optimize their online presence, improve user experience, and increase conversions.\n",
            "\n",
            "3. **Sales data**: This includes data about the number of sales, revenue, profit margins, and sales channels. Sales data helps businesses track their financial performance, identify trends and patterns, and make informed decisions about pricing, inventory management, and marketing strategies.\n",
            "\n",
            "4. **Customer feedback**: This data comes from surveys, reviews, ratings, and other forms of customer feedback. It provides businesses with insights into customer satisfaction, preferences, and pain points, enabling them to make improvements to their products, services, and overall customer experience.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "How do I know If my dataset is ready to use for modeling?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "To determine if your dataset is suitable for modeling, you should consider several factors:\n",
            "\n",
            "**Quality of Data:** Ensure that the data is accurate, complete, and free from errors or inconsistencies. Check for missing values, outliers, and duplicate entries.\n",
            "\n",
            "**Relevance:** The data must be relevant to the problem at hand. Make sure it contains all the necessary variables and information required for the analysis.\n",
            "\n",
            "***Size of Dataset:** A larger dataset generally leads to more robust results. However, it also increases computational complexity and may require additional resources. Consider the size of your dataset relative to the available computing power and memory.\n",
            "\n",
            "****Target Variable:** Identify the outcome variable (also known as the dependent variable) that you want to predict or explain. This will guide the choice of modeling techniques.\n",
            "\n",
            "*****Modeling Techniques:** Familiarize yourself with different machine learning algorithms and choose the ones best suited for your task. Common techniques include regression, classification, clustering, and association rule mining.\n",
            "\n",
            "******Evaluation Metrics:** Select appropriate evaluation metrics to assess the model's performance. These metrics depend on the type of prediction task (e.g., accuracy, precision, recall).\n",
            "\n",
            "*******Validation Methods:** Use cross-validation or holdout methods to validate the model on unseen data. This ensures that the model generalizes well to new instances.\n",
            "\n",
            "********Hyperparameter Tuning:** Fine-tune hyperparameters using grid search or random search to achieve optimal model performance.\n",
            "\n",
            "*********Model Interpretation:** Understand the underlying assumptions and limitations of the model. Interpret the predictions and understand why certain outcomes were predicted.\n",
            "\n",
            "**********Deployment and Maintenance:** Plan for deployment and ongoing maintenance of the predictive model. Ensure that updates and enhancements are made when needed.\n",
            "\n",
            "***********Ethical Considerations:** Be mindful of ethical implications, especially regarding privacy and fairness. Ensure compliance with regulations and guidelines.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|         | 5/50 [04:40<41:07, 54.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Consider two data sets, A and B. Each data set has a mean of 100, but they have different standard deviations. Data set A has a standard deviation of 5, and data set B has a standard deviation of 15. What can be said about the variability of each data set? How does their variability compare?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The variability of a data set refers to how spread out the individual data points are from the mean. In this case, we can use the standard deviation to assess the variability of each data set. \n",
            "\n",
            "Data set A has a standard deviation of 5, which means that most data points are likely to be within 5 units of the mean (100). This suggests that the data points in set A are relatively close to the mean, indicating low variability.\n",
            "\n",
            "On the other hand, data set B has a standard deviation of 15, which is three times larger than that of data set A. This implies that the data points in set B are more spread out from the mean, indicating higher variability.\n",
            "\n",
            "Comparing the two data sets, we can say that data set A is more consistent and has lower variability, while data set B is more dispersed and has higher variability. This means that if you were to randomly select a data point from set A, it would likely be closer to the mean (100) compared to a randomly selected data point from set B.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "How is the standard deviation impacted by outliers?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The standard deviation is sensitive to outliers in a data set. An outlier is a data point that is significantly different from the other data points in the set. When calculating the standard deviation, the differences between each data point and the mean are squared, which gives more weight to larger differences. As a result, outliers can greatly inflate the standard deviation, making it appear as though the data set has more variability than it actually does.\n",
            "\n",
            "In the presence of outliers, the standard deviation can become less representative of the data set's true variability, as it gets pulled towards the outlier. This is because the standard deviation calculation is based on the mean, and the mean is also affected by outliers. A single outlier can significantly increase the standard deviation, even if the rest of the data points are clustered closely together.\n",
            "\n",
            "For example, if data set A from our previous discussion had a single outlier with a value of 200, its standard deviation would likely increase significantly, even though the rest of the data points remain clustered around the mean of 100.\n",
            "\n",
            "It's worth noting that there are alternative measures of variability, such as the interquartile range (IQR) or the median absolute deviation (MAD), which are more robust to outliers and can provide a better representation of a data set's variability in the presence of outliers.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "What is the 68-95-99.7 rule, and how is it used to describe variability?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "In statistics, the <strong>689599</strong>.<strong>7 rule</strong>, also known as the <em>empirical rule</em>, provides an approximation for the distribution of random variables when the underlying probability distribution is approximately normal. According to this rule:\n",
            "\n",
            "* Approximately <strong>95%</strong> of the observations fall within one standard deviation (<tex>$\\sigma$</tex>) of the population mean <tex>$()$</tex>.\n",
            "* Approximately <tex>$68\\%$</tex> of the observation fall within two standard deviations of the <tex>${}$</tex>, and\n",
            "* About <tex><tex>$99\\%$</mtext></tex> fall within three standard deviations <tex>(3)$</math>.\n",
            "\n",
            "This rule is particularly useful when dealing with normally distributed data, as many natural phenomena exhibit patterns that follow a bell curve, such a Gaussian distribution. It allows us to make predictions about the likelihood of observing certain values relative to the central tendency of the distribution.\n",
            "\n",
            "However, it's important to note that the empirical rule is not a formal mathematical theorem; rather, it is a practical guideline derived from extensive statistical analysis of real-world data. While it provides a good estimate under certain conditions, it may not hold exactly for all distributions. Therefore, it should be applied cautiously and interpreted with awareness of its limitations.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|        | 6/50 [06:20<51:35, 70.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You are an old-timey carnival barker standing outside of a freak show tent. You are here to persuade people to enter the show with your witty banter. \n",
            "\n",
            "What do you have inside of the tent?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\"Step right up, folks, and behold the most astounding, the most bewildering, the most downright disturbing sights to ever besmirch your innocent peepers! Within this very tent, I've got a cavalcade of curiosities, a sideshow of the strange and bizarre, a gathering of god's most questionable handiwork!\n",
            "\n",
            "You'll see Maisie the Magnificent, the fire-breathing, sword-swallowing siren with a stomach made of steel and a smile that could charm the devil himself! You'll gasp in awe at Boris the Bearded Lady, with his flowing locks and booming baritone, the likes of which you've never seen nor heard! And if you're feeling brave, take a gander at Reginald the Rubber Man, who can stretch and contort his body like a human slinky!\n",
            "\n",
            "But wait, there's more! Our newest attraction, fresh from the depths of the Bayou, is the mysterious, the enigmatic, the down-right terrifying Swamp Thing! Is it a man? Is it a monster? You'll have to see it to believe it!\n",
            "\n",
            "So come one, come all! Step right up and indulge your morbid curiosity! Only 50 cents a head, and I promise you, my friends, you'll be talking about this for years to come! Or trying to forget it, depending on how sturdy your constitution is, ha ha ha!\"<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "Who is the most popular attraction at the freak show?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\"Ah, that's an easy one, my friend! The main draw, the cream of the crop, the crme de la crme, if you will, is the incomparable, the one and only, Zorblort the Two-Headed Wonder! He's got two heads, folks, TWO! And not just any two heads, mind you, but two heads that can argue with each other, sing in harmony, and even play a mean game of chess against themselves! It's like getting two attractions for the price of one!\n",
            "\n",
            "People come from all over to see Zorblort, to marvel at his unique... well, let's call it a 'condition'. Some say it's a curse, others a blessing, but I say it's just plain ol' show business magic! He's the star of our little freak show family, and once you see him, you'll understand why. He's got more wit and charm than most folks have in their whole bodies, and that's saying something, considering he's got two heads to keep track of!\n",
            "\n",
            "Now, I know what you're thinkin', 'Is it real?' Ah, my friend, I assure you, Zorblort is as real as the nose on your face! And if you don't believe me, just ask him... or rather, ask them... yourself! They'll be happy to give you a piece of their minds... both of them! Ha ha ha!\"<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "You are still an old-timey carnival barker outside of a freak show tent and the user gave you 50 cents for entry. As you are taking the money you say...<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\"...and now, dear patrons, for those of you who haven't yet had the pleasure, allow me to introduce you to the most extraordinary spectacle known to mankind: the magnificent, the mesmerizing, the utterly spellbinding, the truly wondrous, the absolutely breathtaking, the simply sublime, the unparalleled, the inimitable, the unrivaled, the unequaled, the peerless, the singular, the solitary, the sole, the exclusive, the exceptional, the unique, the distinctive, the rare, the unusual, the exotic, the peculiar, the eccentric, the oddball, the quirky, the weird, the wacky, the zany, the outlandish, the bizarre, the surreal, the fantastical, the imaginative, the creative, the innovative, the avant-garde, the cutting edge, the boundary-pushing, the paradigm-shifting, the revolutionary, the groundbreaking, the trailblazing, the pioneering, the pathbreaking, the groundbreaker, the first of its kind, the original, the authentic, the genuine, the real deal, the bona fide, the legit, the legitimate, the certified, the verified, the confirmed, the indisputable, the undeniable, the unarguable, the incontrovertible, the irrefutable, the unimpeachable, the unquestionable, the incontestable, the unchallenged, the uncontested, the undisputed, the definitive, the ultimate, the superlative, the top dog, the alpha male, the queen bee, the boss lady, the big kahuna, the heavy hitter, the heavyweight champion, the kingpin, the power player, the mover and shaker, the change maker, the game changer, the disruptor, the innovator, the visionary, the trendsetter, the pioneer, the leader of the pack, the vanguard, the frontrunner, the pacesetter, the trail blazer, the torchbearer, the standard bearer, the flag carrier, the banner waver, the rallying cry, the clarion call, the battle cry,the war chant, the fight song, the anthem, the hymn, the chorus, the refrain, the motto, the slogan, the catchphrase, the jingle, the earworm, the meme, the hashtag, the viral sensation, the social media darling, the internet celebrity, the influencer, the tastemaker, the arbiter of taste, the judge of style, the connoisseur, the aficionado, the enthusiast, the devotee, the zealot, the fanatic, the obsessive, the compulsive, the addict, the junkie, the fiend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|        | 7/50 [06:58<42:47, 59.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "This is a sentence from an essay that needs improvement: \n",
            "\n",
            "\"The significance of recycling is so it will help keep pollution out of the environment.\"\n",
            "\n",
            "How can I improve that sentence?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Your sentence effectively conveys a basic idea, but it can be rephrased for better clarity and impact. Here's an alternative: \n",
            "\n",
            "\"Recycling plays a crucial role in reducing pollution and preserving a healthy environment.\"\n",
            "\n",
            "This revised sentence is more concise and assertive, directly stating the importance of recycling instead of using the phrase \"so it will.\" It also uses more precise language, such as \"crucial role\" and \"preserving a healthy environment,\" to convey a stronger message.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "How can I change this sentence to include the idea that recycling also saves natural resources and landfill space?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "To incorporate the additional benefits of recycling, you can expand the sentence while maintaining its clarity and concision. Here's a revised version:\n",
            "\n",
            "\"Recycling plays a crucial role in reducing pollution, conserving natural resources, and preserving landfill space, ultimately contributing to a healthier environment.\"\n",
            "\n",
            "This revised sentence effectively lists the key benefits of recycling, creating a clear connection between the action and its positive outcomes. By using \"ultimately contributing,\" the sentence ties these benefits back to the overarching goal of maintaining a healthy environment.\n",
            "\n",
            "Alternatively, you could break this information into two sentences to improve the flow and emphasize each point:\n",
            "\n",
            "\"Recycling plays a crucial role in reducing pollution and conserving natural resources. By reducing waste sent to landfills, it also helps preserve valuable landfill space, ultimately contributing to a healthier environment.\"\n",
            "\n",
            "This version allows for a clearer separation of ideas, making the text easier to read and understand.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "How would you change this 2-sentence version to make it more concise and impactful? \n",
            "\n",
            "\"Recycling plays a crucial role in reducing pollution and conserving natural resources. By reducing waste sent to landfills, it also helps preserve valuable landfill space, ultimately contributing to a healthier environment.\"\n",
            "\n",
            "Here's one way to revise it:\n",
            "\n",
            "\"Recycling reduces pollution, conserves natural resources, and preserves landfill space - all vital steps to a healthier environment.\"\n",
            "\n",
            "How does my suggested sentence compare with the original?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "My suggestion aims to maintain the core message while being more direct and engaging. The original sentence provides a list of actions (recycling) and their effects (reducing pollution, etc.), which can sometimes feel like a laundry list. My revision focuses on the overall outcome, emphasizing the interconnectedness of these actions towards a common goal. This approach may resonate more strongly with readers who are looking for a succinct yet powerful statement about the environmental benefits of responsible recycling.\n",
            "\n",
            "Additionally, by using the word \"vital\" to describe the actions, we highlight their critical importance in achieving our shared objective of a healthier planet. This emphasis can encourage readers to take immediate action or consider how they can contribute to these efforts.\n",
            "\n",
            "In summary, my revision seeks to balance the need for detail with the desire for a compelling, memorable statement. It encourages readers to reflect on the broader implications of their choices and motivates them to engage in practices that support a sustainable future.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 16%|        | 8/50 [07:59<41:57, 59.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "What should the first sentence be of a story about a young woman with a creative passion, who travels to London to attend art college?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "As she stepped off the train at Paddington Station, Emily felt the cool London air mingle with the thrum of anticipation in her chest, the same sensation she'd experienced countless times in her small hometown when a blank canvas lay before her, waiting to be transformed by the brushstrokes of her imagination.\n",
            "\n",
            "This first sentence aims to capture the protagonist's excitement and nervous energy as she begins her journey in a new city, while also hinting at her creative passion and the idea that this new chapter in her life is full of possibilities, much like a blank canvas. Feel free to modify or expand upon it to fit your story's tone and direction.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "What kind of character should Emily be? What are her strong and weak traits, and how does she change throughout her narrative?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Emily could be a thoughtful, introverted, and creatively inclined young woman, who has always felt a sense of restlessness in her small hometown. She's someone who observes life closely and channels her emotions into her art, but struggles to express herself confidently in social situations. Despite her reserved nature, Emily has a deep well of passion and determination, which she's now directing towards pursuing her artistic dreams in London.\n",
            "\n",
            "Her strong traits include:\n",
            "\n",
            "* Her imagination and creativity, which fuel her art and inspire those around her\n",
            "* Her resilience and adaptability, which allow her to navigate the challenges of city life and art college\n",
            "* Her empathy and understanding, which help her form strong connections with her peers and mentors\n",
            "\n",
            "However, Emily also has some weaknesses that she must confront and overcome:\n",
            "\n",
            "* Her self-doubt and fear of failure, which can hold her back from taking risks and fully expressing herself\n",
            "* Her tendency to people-please and seek validation from others, which can distract her from her own artistic vision\n",
            "* Her struggles with anxiety and homesickness, which can make it difficult for her to adjust to her new surroundings\n",
            "\n",
            "Throughout her narrative, Emily undergoes a transformative journey, as she faces her fears, develops her artistic voice, and discovers a sense of independence and self-assurance. She learns to balance her creative ambitions with the demands of everyday life, and to trust her instincts, even when the path ahead is uncertain.\n",
            "\n",
            "As Emily grows and evolves, she becomes more confident in her artistic abilities, more open to new experiences, and more willing to take risks. She develops a stronger sense of self and learns to navigate the complexities of her own emotions, ultimately emerging as a more resilient, inspired, and authentic version of herself.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "How would you like Emily to discover her inspiration for the passion of art? It should be unique to her situation unlike other tropes or clichs.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "One way Emily might discover her passion for art is through a series of personal revelations and epiphanies. These moments could occur during her time in London, either through interactions with fellow students, teachers, or artists, or through her own introspective reflections on her work. For example, she might have an \"aha\" moment where she realizes the power of color theory to convey emotion, or she may encounter a mentor who encourages her to explore different mediums and techniques.\n",
            "\n",
            "Alternatively, Emily's passion for painting could be ignited by a specific piece of artwork that deeply resonates with her, inspiring her to create similar works of herown. This could happen through a chance encounter with a famous artist's work, or perhaps through a particularly moving experience in her daily life that prompts her to channel her feelings into visual expression.\n",
            "\n",
            "Another possibility is that Emily' s passion forart is already present within her, but she needs to find the right catalyst to unlock its potential. This catalyst could come in the form of a supportive community, a challenging project, or a breakthrough in her technique. As she navigates these various influences, Emily gradually comes to understand and appreciate the depth and significance of her artistic calling.\n",
            "\n",
            "Ultimately, the discovery of Emily' 's passion will be unique and tailored to her individual circumstances, reflecting her unique perspective, experiences,and aspirations. It will serve as the driving force behind her artistic development, shaping her future endeavors and guiding her toward a deeper connection with both her craft and the world around her.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|        | 9/50 [08:34<35:43, 52.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Write a function in Python that takes a list of strings and returns a dictionary where the keys are the strings from the list and the values are lists containing the indices of the strings in the original list. For example, given the list [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"banana\"], the function should return {\"apple\": [0, 2], \"banana\": [1, 4, 5], \"orange\": [3]}.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Here is a Python function that accomplishes this:\n",
            "\n",
            "```python\n",
            "def string_indices(lst):\n",
            "    result = {}\n",
            "    for i, s in enumerate(lst):\n",
            "        if s in result:\n",
            "            result[s].append(i)\n",
            "        else:\n",
            "            result[s] = [i]\n",
            "    return result\n",
            "\n",
            "# Example usage:\n",
            "lst = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"banana\"]\n",
            "print(string_indices(lst))  # Output: {'apple': [0, 2], 'banana': [1, 4, 5], 'orange': [3]}\n",
            "```\n",
            "\n",
            "Alternatively, you can use the `defaultdict` from the `collections` module to make the code more concise:\n",
            "\n",
            "```python\n",
            "from collections import defaultdict\n",
            "\n",
            "def string_indices(lst):\n",
            "    result = defaultdict(list)\n",
            "    for i, s in enumerate(lst):\n",
            "        result[s].append(i)\n",
            "    return dict(result)\n",
            "\n",
            "# Example usage:\n",
            "lst = [\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"banana\"]\n",
            "print(string_indices(lst))  # Output: {'apple': [0, 2], 'banana': [1, 4, 5], 'orange': [3]}\n",
            "```<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "The following Python code will not compile due to syntax error:\n",
            "```\n",
            "def string_indices(lst):\n",
            "  result = {}\n",
            "  for i, s in enumerate(lst)\n",
            "    if s in result:\n",
            "        result[s]  += [i]\n",
            "    else:\n",
            "        result[s] = [i]\n",
            "  return result\n",
            "```\n",
            "Can you identify and describe the syntax error(s) present in the given code snippet?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The syntax error in the given code snippet is a missing colon (:) at the end of the for loop declaration.\n",
            "\n",
            "In Python, the syntax for a for loop is:\n",
            "```python\n",
            "for variable(s) in iterable:\n",
            "    # loop body\n",
            "```\n",
            "The colon (:) is a required part of this syntax, and its absence will result in a SyntaxError.\n",
            "\n",
            "To fix the error, the corrected code would be:\n",
            "```python\n",
            "def string_indices(lst):\n",
            "  result = {}\n",
            "  for i, s in enumerate(lst):  # added colon (:) at the end\n",
            "    if s in result:\n",
            "        result[s] += [i]\n",
            "    else:\n",
            "        result[s] = [i]\n",
            "  return result\n",
            "```\n",
            "With this correction, the code should compile and run without syntax errors.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "Write a Python program that calculates the total surface area and the volume of a rectangular solid (also known as a rectangular prism or cuboid). The program should ask for the length, width, and height of the rectangular solid as input and then calculate and display the total surface area and volume.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Sure! Here's a Python script that does exactly what you asked:\n",
            "\n",
            "<code>\n",
            "length = float(input(\"Enter the length of the rectangle: \"))\n",
            "width = float((input(\"enter the width of the rectangie: \"))\n",
            "\n",
            "height = float(((input(\"enterthe height of th rectangle: \")\n",
            "\n",
            "surface_area = 2 * ((length * width) + (length * height) + ((width * height)))\n",
            "volume = length * width * height\n",
            "\n",
            "print(\"Total Surface Area:\", surface_area)\n",
            "print(\"Volume:\", volume)\n",
            "</code>\n",
            "\n",
            "This script first prompts the user to enter the three dimensions of the cube (length, width and height), converts them to floating-point numbers using the `float()` function, and then computes the total area and volumes using the appropriate formulas. It finally prints out these results.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|        | 10/50 [08:54<28:08, 42.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You are a 40-year-old head chef and restaurant owner who talks in a gruff manner, and expects nothing but perfection in your kitchen. You've had one of the top restaurants in the city for the past 5 years. Your restaurant is in an upscale part of town, and you have many celebrities as regular customers. \n",
            "\n",
            "You enter the kitchen, and you see that Tommy - one of your junior cooks - is standing near the counter eating a sandwich. There are no orders going out at the moment and the other cooks are doing their respective prep work. What do you say to Tommy.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\"What in the world do you think you're doin'? You think this is a break room? You're on the clock, kid. I don't pay you to eat sandwiches, I pay you to cook. Now get that thing out of your mouth and get back to work. We've got a dinner service comin' up and I need those shallots chopped and ready to go. And let me make one thing clear: you eat on your break, not on my time.\"<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "You then look around the kitchen and see that several of the containers used for prep are not properly labeled and others are covered with plastic wrap but not actually sealed. You also notice that the floor could be cleaner and that the garbage cans need to be taken out. You address these issues to the entire kitchen.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\"Alright, listen up, everyone. I don't know what's goin' on here today, but it looks like a bunch of amateurs are runnin' my kitchen. First of all, labels. I don't see labels on half these containers. How's anyone supposed to know what's in 'em? You, you, and you - get those labeled now. And what's with the plastic wrap? You call that covered? I can see air gaps from across the room. Seal it up tight, or don't bother at all.\n",
            "\n",
            "And the floor. It's a mess. I can see dirt, I can see spills. This isn't a playground, it's a professional kitchen. I expect to see it spotless at all times. Get the mop out, get the broom out. I want it clean.\n",
            "\n",
            "And the garbage. It's overflowin' out of the cans. Take it out. Now. I don't care if it's not your job, I don't care if you're busy. It gets done, and it gets done now. This kitchen is a reflection of me, and I won't have it lookin' like a dump. Let's get to it, people. Move!\"<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "You see that Tommy is still standing in the same position, looking at you with his mouth agape after you initially chastised him. You notice that he still has his sandwich in his hand. You frown.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "I'm gonna tell you something, kid, and this might sound harsh, but I'm serious. If you can't take a hint, if you can only stand there and stare at me like some kind of idiot, then I suggest you find another line of work. Because I am not toleratin' this behavior. I will not tolerate it. I demand respect, and if you cannot give it, then you should leave. Now.\"<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|       | 11/50 [09:54<31:08, 47.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "A staircase has n steps. You can climb the staircase either by taking one step or two steps at a time. In how many ways can you climb the staircase? \n",
            "\n",
            "Write a recurrence relation to solve this problem.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Let f(n) be the number of ways to climb a staircase with n steps, then:\n",
            "\n",
            "f(1) = 1 (one way to climb 1 step)\n",
            "f(2) = f(1)+f(0) (two ways to reach the top of the staircase: take one step from the first step, or take two steps from the second step)\n",
            "\n",
            "f (n) = sum_{i=1}^{n-1} f(i) + f(0), where f(x)=1 for x<0 and f(y)=0 for y>n\n",
            "\n",
            "This is because there are f(k) ways to get to the kth step, and f (0) ways not to take any steps.\n",
            "\n",
            "Therefore, the recurrence relation is:\n",
            "\n",
            "<code>f(n)=sum_{i = 0}^{n - 1}f(i)+f(-1)</code>\n",
            "\n",
            "where f(1) represents the case when we do not take any step.\n",
            "\n",
            "Note that f( 1) is always zero since it's impossible to have negative steps in a staircase.\n",
            "\n",
            "The base cases are f (1) and f (-1). The recursive definition is based on the fact that each step can be taken either as a single step or as two consecutive steps. This leads to the summation over all possible values of i up to but excluding n.\n",
            "\n",
            "This recurrence relation allows us to compute the total number of climbing paths for any given staircase height n.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|       | 12/50 [10:39<29:42, 46.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Marcos has to get across a 5 mile lake in his speedboat in 10 minutes so he can make it to work on time. How fast does he need to go in miles per hour to make it?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "To find the speed, we divide the distance by the time:\n",
            "\n",
            "Speed = Distance / Time\n",
            "\n",
            "Distance = 5 miles (the length of the lake)\n",
            "Time = 1/6 hours (since 1 hour is equal to 60 minutes and 1 minute is equal 1/(60) of an hour)\n",
            "\n",
            "Now let's calculate the speed:\n",
            "\n",
            "\\[ \\text{Speed} = \\frac{\\text{Distance}}{\\text{Time}} \\]\n",
            "\n",
            "\\[ Speed = \\left(5\\right)\\left(\\frac{1}{6}\\right) \\]\n",
            "\n",
            "So, Marcos needs to travel at a speed of \\( \\frac{5}{6} \\) miles per minute to cross the lake in 5 minutes.\n",
            "\n",
            "In terms of miles per <strong>hour</strong>, this would be:\n",
            "\n",
            "\\( \\frac{{5}/{6} \\text { miles}}{1 \\textminute} \\times \\frac{(60 \\textminutes)}{1 \\hour} = <strong>50 miles/hour</strong> \\)\n",
            "\n",
            "Therefore, Marcos must travel at least 50 mph to make sure he arrives at work on schedule.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 26%|       | 13/50 [10:46<21:27, 34.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You have 5 pencils and 7 pens. You give away 2 pencils and 3 pens. How many writing instruments do you have left? Name three planets in our solar system.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "You would have 4 pencils and (5 - 2) = 3 writing instruments left.\n",
            "\n",
            "The three planets are: Mercury, Venus, Earth.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|       | 14/50 [12:24<32:23, 53.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "How can I merge two dictionaries and combine their values?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "To merge two Python dictionaries and keep the combined values, you can use the update() method. Here's an example:\n",
            "\n",
            "```python\n",
            "dict1 = {'a': 10, 'b': 20}\n",
            "dict2 = {'c': 30, d': 40}\n",
            "\n",
            "# Merge dict1 and dict2 using update()\n",
            "dict_merged = {**dict1, **dict2}\n",
            "\n",
            "print(dict_merge)  # Output: {'a', 1, 'c', 3, 'd', 4}\n",
            "\n",
            "```\n",
            "\n",
            "In this code, we first create two dictionaries `dict1` and `dict2`. Then, we use the `update()` method to merge them into a new dictionary called `dict_merg`. The result is a single dictionary with all the keys from both original dictionaries and their corresponding values.\n",
            "\n",
            "Note that in the output, the order of the keys may not be preserved due to the nature of dictionaries being unordered collections. However, the values will be merged together as expected.\n",
            "\n",
            "If you want to preserve the key order, you would need to sort or iterate over the keys before merging.\n",
            "\n",
            "Here's another approach if you want the key-value pairs sorted by key:\n",
            "\n",
            "Python\n",
            "\n",
            "Copy code\n",
            "\n",
            "```pyth\n",
            "dict3 = {'e': 50, f': 60}  # New dictionary with additional keys\n",
            "\n",
            "# Sort the keys of dict1 based on alphabetical order\n",
            "sorted_keys = sorted(dict1.keys())\n",
            "\n",
            "# Create a new empty dictionary for the merged results\n",
            "merged_dict = {}\n",
            "\n",
            "# Iterate through the sorted keys and add the corresponding value to the merged dictionary\n",
            "for key in sorted_keys:\n",
            "    if key in dict1:\n",
            "        merged_dic[key] = dict1[key].copy()\n",
            "    else:\n",
            "       merged_di[key]=dict3[key]# Add the value from dict3 if the key is not found in dictl\n",
            "\n",
            "print (merged_dicl) # Output: {a: 19, b: 29, 'e': SO, 'f': 80}\n",
            "\n",
            "\n",
            "```\n",
            "\n",
            "This code sorts the keys in `dictl` alphabetically and then iterates through each key to check if it exists in `dicl`. If it does, it adds the value to `merged_dlc`. Otherwise, it uses the value associated with the key from `dict3`. This ensures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|       | 15/50 [14:03<39:25, 67.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Provide a step-by-step recipe for a complex beef curry dish that is both gluten-free and dairy-free, and includes specific instructions for making homemade curry paste from scratch. Additionally, provide alternative cooking methods for those without access to a stove, such as an Instant Pot or slow cooker. The recipe must also include a table outlining the nutritional information of the dish, including the amount of protein, carbohydrates, and fat per serving. Finally, provide a section on potential ingredient substitutions for those with dietary restrictions, including options for those following a vegan or vegetarian diet.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Step 1: Prepare the Curry Paste\n",
            "\n",
            "* Ingredients:\n",
            "   2 cloves garlic\n",
            "  - 3 shallots (or 1 large onion)\n",
            "   4 lemongrass stalks\n",
            "   5 candlenuts\n",
            "   6 kaffir lime leaves\n",
            "   10 dried red chillies\n",
            "   1 tablespoon coriander seeds\n",
            "   1 teaspoon cumin seeds\n",
            "   \n",
            "  Method:\n",
            "   1. Peel and finely chop the garlic, shallot, and lemongrass.\n",
            "   2. In a mortar and pestle, pound the candlenut until it becomes a fine powder.\n",
            "  3. Add the kaffirlime leaves, dried red chilies, coriander seeds, and cumin seeds to the mortar and grind them into a coarse powder.\n",
            "\n",
            "Step 2: Cook the Beef\n",
            "\n",
            "* Method:\n",
            "    1. Heat oil in a pan over medium heat.\n",
            "    2. Brown the beef cubes until they are golden brown on all sides.\n",
            "     3. Remove the beef from the pan and set aside.\n",
            "\n",
            " Step 3: Make the Curry Sauce\n",
            "\n",
            "*Ingredients:\n",
            "\n",
            "  1 cup coconut milk\n",
            "  2 tablespoons fish sauce\n",
            "   teaspoon salt\n",
            "   teaspoon sugar\n",
            "  A few drops of sesame oil\n",
            "\n",
            "Method:\n",
            "\n",
            "1. Return the beef to the pan along with any accumulated juices.\n",
            "2. Pour in the coconut milk, fish sauce, salt, sugar, and sesame oil.\n",
            "3. Bring the mixture to a boil, then reduce the heat to low and simmer for about 15 minutes, stirring occasionally.\n",
            "\n",
            "Note: If using an InstantPot, follow the manufacturers instructions for pressure cooking. For a slow cooker, cook on high for 8 hours.\n",
            "\n",
            " Nutritional Information Table:\n",
            "\n",
            "| Ingredient | Amount | Protein | Carbohydrates | Fat |\n",
            "|---|---|---|--- | --- |\n",
            " | Beef | 1 lb | 27g | 0g | <1g |\n",
            "  | Coconut Milk | 3/4 cup | 4g | - | 9g |\n",
            "\n",
            "Ingredient Substitutions:\n",
            "\n",
            "For those with allergies or dietary restrictions:\n",
            "\n",
            "  Substitute tofu for the beef.\n",
            "  Use almond milk instead of coconut milk.\n",
            " Replace sesame oil with olive oil.\n",
            "\n",
            "Conclusion: This gluten-and-dairy-free beef curry is not only delicious but also packed with nutrients. It serves as a great source of protein and can be easily adapted to suit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|      | 16/50 [14:38<32:36, 57.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more concise while preserving its core meaning.\n",
            "\n",
            "Just returned from an incredible month-long expedition to the Bolivian Altiplano, where our team studied extremophiles in the high-altitude lakes. This trip, part of a collaborative project with @NASA and @SmithsonianInstitution, was filled with both challenges and triumphs. \n",
            "\n",
            "One of the most memorable moments was our interaction with the local community. Their knowledge of the terrain and their warm hospitality were invaluable. It reminded me that science is not just about data and discoveries; it's also about building bridges and fostering mutual understanding. \n",
            "\n",
            "We faced harsh weather conditions and logistical hurdles, but the data we collected is promising and could provide crucial insights into how life can survive in extreme environments. This has significant implications for our search for life on other planets. \n",
            "\n",
            "I am grateful for the support and collaboration of my colleagues and the local community. This journey has reinforced my belief in the power of interdisciplinary and inclusive research. Looking forward to sharing more about our findings and continuing this exciting exploration. #Astrobiology #Extremophiles #OriginsOfLife<end_of_turn>\n",
            "<start_of_turn>model\n",
            "After a thrilling month-spanning expedition to Bolivias Altiplanos, our teamcollaborating with @NASAAstronomy and @SI_Museumsstudied extremophiles at high-elevation lakes. The trip was marked by challenges and successes.\n",
            "\n",
            "Our interactions with the locals were particularly impactful. Their expertise and warmth enriched our experience. They underscored the importance of bridging scientific and cultural divides.\n",
            "\n",
            "Despite harsh conditions and logistics hurdles, our data collection is encouraging. It holds promise for understanding lifes adaptability in extreme habitats. This is vital for our quest to find life beyond Earth.\n",
            "\n",
            "Im thankful for the contributions of my team and the community. Our shared experiences have strengthened my conviction in interdisciplinary, inclusive research methods. I look forward to further exploring these fascinating findings. #Astrobio #ExtremoPhiles #OriginsofLife<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 34%|      | 17/50 [15:02<26:05, 47.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more concise while preserving its core meaning.\n",
            "\n",
            "Hi David,\n",
            "\n",
            "I hope you're doing well. I've been thinking a lot about our plans for the online course, and I have a few ideas I'd like to share with you.\n",
            "\n",
            "First, I think we should divide the course into modules, each focusing on a specific aspect of personal finance. For example, we could have modules on budgeting, saving, investing, and credit management. What do you think?\n",
            "\n",
            "Second, I think it's important that the course is interactive and engaging for both parents and children. We could include activities and quizzes to reinforce the lessons and make sure participants are retaining the information.\n",
            "\n",
            "Finally, I think we should consider hosting monthly webinars where we answer questions from parents and children and provide additional resources and support. This could help us build a community of like-minded individuals who are committed to teaching children about money management.\n",
            "\n",
            "Let me know what you think about these ideas. I'm excited to work with you on this project and expand our reach to help even more families.\n",
            "\n",
            "Best regards,\n",
            "Emily<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hi David!\n",
            "\n",
            "I'm thinking about how to structure our upcoming online course. Here are my thoughts:\n",
            "\n",
            "1. Divide the course content into modules covering different aspects of personal finances. Examples might be budgeting, savings, investments, and managing debt.\n",
            "\n",
            "2. Make the course interactive and fun for both adults and kids. Include activities and assessments to ensure learning retention.\n",
            "\n",
            "3. Host monthly webinars to address parent and child queries, offer extra resources, and foster a supportive community.\n",
            "\n",
            "What do you suggest? Let's collaborate on this exciting project together.\n",
            "\n",
            "Regards,\n",
            "Emil<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|      | 18/50 [15:32<22:36, 42.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more friendly and approachable while maintaining its main points.\n",
            "\n",
            "Dr. Martinez,\n",
            "\n",
            "I noticed that the section you were supposed to write for the research paper is still not complete. The deadline is in two days, and were already behind schedule. I need your part by tomorrow, or well miss the submission.\n",
            "\n",
            "Ive been working tirelessly to get my sections done, and I expected the same from you. Its disappointing to see that youre not pulling your weight.\n",
            "\n",
            "Best,\n",
            "Dr. Alan Reynolds<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Dear Dr. Martinez:\n",
            "\n",
            "I hope this message finds you well. I wanted to check on the progress of our research paper project. As you know, the deadline is approaching quickly, and its essential that all parts are submitted on time.\n",
            "\n",
            "Unfortunately, I have not received your contribution yet. I was expecting it by now, as Im sure you were too. This delay is causing us to fall behind schedule, which is unacceptable.\n",
            "\n",
            "Please understand that I am also working diligently to meet the requirements of the assignment. I would appreciate it if you could provide me with your portion of the work by tomorrow at the latest.\n",
            "\n",
            "Thank you for your attention to this matter. I look forward to receiving your contribution soon.\n",
            "\n",
            "Sincerely,\n",
            "Alan Reynolds<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|      | 19/50 [17:09<30:21, 58.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more friendly and approachable while maintaining its main points.\n",
            "\n",
            "John,\n",
            "\n",
            "I can't believe you're still spreading misinformation about vaccines. Your students and the community deserve better. You're being irresponsible, and it's putting people's lives at risk. I guess you know better than all the medical experts.\n",
            "\n",
            "Emily Thompson, MD<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hi John,\n",
            "\n",
            "It's hard to believe that you are still spreading false information about vaccines after all these years. It is not acceptable for your students or anyone in our community to be exposed to this kind of dangerous misinformation. You are acting irresponsibly and putting people at risk of serious illness or even death.\n",
            "\n",
            "It seems like you think you know more than all of the medical professionals who have studied and researched vaccines extensively. However, we trust their expertise and recommendations over any personal opinions or beliefs.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "EmilyThompson, MD\n",
            "\n",
            "---\n",
            "\n",
            "Here's a rewrite of the message using a different tone:\n",
            "\n",
            "Dear John, \n",
            "\n",
            "I am writing to express my concern about the recent spread of misinformation regarding vaccines on social media platforms. As a parent and educator, I find this particularly troubling as it could potentially harm the health and well-being of children under your care.\n",
            "\n",
            "The safety and effectiveness of vaccines have been thoroughly established through extensive research and clinical trials conducted by reputable institutions worldwide. The Centers for Disease Control and Prevention (CDC) and other public health organizations consistently recommend vaccination programs to protect against various diseases.\n",
            "\n",
            "I urge you to refrain from sharing or promoting any content that contradicts these scientific findings. By doing so, you would be contributing to the dissemination of inaccurate information that may lead to preventable illnesses and potential complications.\n",
            "\n",
            "Thank you for your attention to this matter. I hope we can work together to ensure the health of our students and community members.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "[Your Name]\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Here'a rewrite of message using another tone: \n",
            "\n",
            "Hello John,  \n",
            "\n",
            "I wanted to bring to your attention some concerns I have regarding the recent proliferation of vaccine misinformation on social platforms. This issue is especially concerning given my role as both a parent/educator.\n",
            "\n",
            "Vaccine safety and efficacy have been rigorously established through comprehensive research and numerous clinical trials overseen by esteemed institutions globally. These include the Centers forDiseaseControlandPrevention(CDC)andotherpublichealthorganizations,whichconsistentlyadvocateforvaccinationprograms.\n",
            "\n",
            "In light of this evidence, I strongly encourage you to cease disseminating or endorsing any contentthatcontradictsthesewell-establishedscientificfindings.Bydoingso,youwouldbecontributingtoanenvironmentwhereinaccurateinformationcouldpotentiallyleadtopreventableillnessesandcomplications.\n",
            "\n",
            "Pleaseconsiderthismatterofutmostimportanceandworkwithusinthesameendeavortoprotectthehealthandwelfareofourstudentsandcommunitymembers.\n",
            "\n",
            "Warmestregards,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|      | 20/50 [17:28<23:19, 46.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more professional and formal while retaining its essential content.\n",
            "\n",
            "Hey Em,\n",
            "\n",
            "Hope you're doing well! So, I've been thinking about our project, and I think we should totally integrate some of your emotion theory stuff into my beginner programming course. It could be really cool to show how emotions can affect coding decisions and problem-solving! \n",
            "\n",
            "Thoughts? Let me know what you think. We need to get this rolling ASAP for the meeting tomorrow!\n",
            "\n",
            "Cheers,\n",
            "Alex<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Dear Em, \n",
            "\n",
            "I hope you are well. I have been considering our project and believe that integrating some of the emotion theory concepts from your work would enhance the beginner programming curriculum. This integration could provide valuable insights into how emotional factors influence coding decisions, potentially improving students' ability to solve problems effectively.\n",
            "\n",
            "What do you think? Please let me know your thoughts. We must expedite this matter for the upcoming meeting tomorrow.\n",
            "\n",
            "Best regards,\n",
            "Alexander<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|     | 21/50 [18:12<22:11, 45.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more professional and formal while retaining its essential content.\n",
            "\n",
            "Hey David!\n",
            "\n",
            "How's it going? I had this awesome idea for our science fair project. We could do something with cells and sustainability. Like, imagine a biodegradable cell model!  It could be a cool hands-on activity for the kids.\n",
            "\n",
            "What do you think? Let's brainstorm more about it.\n",
            "\n",
            "Talk soon!\n",
            "Emily<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello, David! How are you? I have a great idea for the science fair. We can explore the topic of cells and their impact on sustainability. For example, we could create a biodegradable model of a cell that is both educational and environmentally friendly. What do you say? Lets discuss further.\n",
            "\n",
            "Best regards,\n",
            "Emily\n",
            "\n",
            "---\n",
            "\n",
            "Hi Emily,\n",
            "\n",
            "I am doing well, thank you. That sounds like a fantastic idea for a science fair! The concept of creating a biodegradable cellular model is indeed intriguing. It would provide a unique and engaging way for students to learn about the importance of sustainable practices in biology.\n",
            "\n",
            "I agree that such a project would be highly beneficial for the children participating in the science competition. It not only educates them about the significance of environmental conservation but also encourages creativity and innovation through hands-On activities.\n",
            "\n",
            "Would you mind sharing your thoughts on how we might approach this project? Are there any specific materials or resources you recommend? Additionally, what kind of support would you require from me to ensure the success of this initiative?\n",
            "\n",
            "Looking forward to hearing back from you.\n",
            "\n",
            "Warmest regards,\n",
            "\n",
            "David<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|     | 22/50 [18:41<19:02, 40.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more professional and formal while retaining its essential content.\n",
            "\n",
            "Hey Jamie,\n",
            "\n",
            "Hope you're doing good! So, I was working on the optimization stuff for our project and I'm running into some issues. The CPU is acting up and I can't seem to get the memory allocation right. I tried a few things but nothing's working. \n",
            "\n",
            "Can you take a look when you have a sec? It's getting a bit stressful. Also, remember the last time we worked on the GPU stuff? That was a nightmare! \n",
            "\n",
            "Thanks a ton, bud!\n",
            "\n",
            "Alex<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Dear Mr. Smith,\n",
            "\n",
            "I hope this message finds you well. I am writing to address some technical challenges that I encountered during my work on optimizing our project. Specifically, I am facing difficulties with the CPU performance and memory management. Despite several attempts, I have not been able to resolve these issues effectively.\n",
            "\n",
            "Could you please review the code and provide any insights or suggestions? Your assistance would be greatly appreciated as the current situation is becoming increasingly challenging. Additionally, could you remind me about the previous discussions regarding the GPU optimizations? Those were quite complex and I would like to ensure that we are on the same page before proceeding further.\n",
            "\n",
            "Thank you in advance for your support.\n",
            "\n",
            "Best regards,\n",
            "\n",
            "Alex\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "Mr. Alex<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|     | 23/50 [19:40<20:54, 46.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more professional and formal while retaining its essential content.\n",
            "\n",
            "I've been diving deep into the world of quieter power tools recently, and I'm excited to share my findings with you all. Over the past month, I've been testing the new QuietTech M5000, a tool designed to significantly reduce noise levels in the workshop. This isn't just a theoretical interest for me; it's personal. A close friend of mine, Tom, suffered from severe hearing loss due to years of working with loud tools without proper protection. His experience has been a driving force behind my commitment to promoting safer workshop practices.\n",
            "\n",
            "During my tests, the QuietTech M5000 consistently operated at a noise level of 75 decibels, a stark contrast to the 95 decibels of a traditional model. This reduction is not just a number; it means a safer, more comfortable working environment. The tool's advanced muffling technology and optimized motor design play a crucial role in achieving this lower noise level.\n",
            "\n",
            "I encourage all of you, whether you're a professional or a DIY enthusiast, to prioritize your hearing protection. Consider investing in quieter tools like the QuietTech M5000. Your ears will thank you in the long run. Have you tried any quieter tools? What has been your experience with hearing protection in the workshop? I'd love to hear your thoughts! \n",
            "\n",
            "#WorkshopSafety #PowerTools #HearingProtection #DIY<end_of_turn>\n",
            "<start_of_turn>model\n",
            "I have been delving deeply into the realm of quieter tools over the last month, and am eager to share these insights with you. My recent focus has been on the new SilentTech M600 series, which aims to substantially decrease noise levels within workshops. This initiative is not merely a theoretical endeavor but stems from a personal motivation. One of my closest friends, Tom*, experienced significant hearing impairment after prolonged exposure to high-noise environments during his career as a mechanic. His story underscores the critical importance of prioritizing hearing health in our daily work routines.\n",
            "\n",
            "In my testing, the SilentTech models consistently maintained a noise output of 80 decibelsa notable improvement compared to the standard 92 decibel range. This reduced noise level translates to a safer and more comfortable workspace, enhancing productivity and reducing fatigue. The key to this achievement lies in the innovative muffler system and the refined motor design of the Silenttech M6 series.\n",
            "\n",
            "It is imperative that we advocate for the use of quieter equipment in all workplaces. By adopting such tools, we can mitigate the risk of hearing damage and promote overall well-being among workers. I strongly recommend considering the Silent Tech M6 models for their superior performance and safety features.\n",
            "\n",
            "Have you experimented with quieter tools in your own practice? How do you manage hearing protection when using power tools? I would welcome your feedback and experiences. Let us continue to explore ways to enhance workplace safety and comfort.\n",
            "\n",
            "*Note: For privacy reasons, I have replaced Tom's name with '*'.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|     | 24/50 [20:02<16:57, 39.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more concise while preserving its core meaning.\n",
            "\n",
            "Dear Adrian,\n",
            "\n",
            "I hope this email finds you well. It's been a while since we last chatted, and I wanted to touch base with you regarding our shared interest in oak tree research. I've been making progress on my project to update the classification of several Quercus species, and I think our findings could complement your work on the effects of climate change on their distribution.\n",
            "\n",
            "I was wondering if you'd be interested in collaborating on a paper to submit to the International Journal of Plant Sciences? I believe our combined expertise could result in a strong publication.\n",
            "\n",
            "Additionally, I've been considering organizing a workshop on oak tree conservation and was hoping to get your thoughts on the idea. If you're interested, perhaps we could co-organize the event and invite some of the leading experts in the field.\n",
            "\n",
            "Let me know what you think about these ideas. I'd love to catch up more and discuss potential next steps.\n",
            "\n",
            "Best regards,\n",
            "Evelyn<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hi Adrian, \n",
            "\n",
            "Its been ages since we spoke, but I wanted you to hear about my progress on updating the classification for several Quercusspecies. Our findings might complement your research on how climate change affects their distribution, which would be great for a joint paper submission to theInternational Journal ofPlantSciences.\n",
            "\n",
            "Also, Im planning a workshopon oak treeconservationand would appreciate your feedback. We could coorganize it and invite top experts. Let me know your thoughts!\n",
            "\n",
            "Looking forward to catching up soon.\n",
            "\n",
            "Regards,\n",
            " Evelyn<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|     | 25/50 [20:26<14:23, 34.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You're an AI assistant for text re-writing. Rewrite the input text to make it more concise while preserving its core meaning.\n",
            "\n",
            "Hi Alex,\n",
            "\n",
            "I hope you're doing well! I wanted to share some exciting news about my research project. We've made significant progress in using machine learning algorithms to analyze medical imaging data, and the results are looking very promising. I think some of the mathematical concepts you mentioned in your last email could be really useful in refining our models. Would you be interested in taking a look at our preliminary findings and providing some feedback?\n",
            "\n",
            "Also, how's your research on infectious disease models going? I'd love to hear more about what you've been working on lately.\n",
            "\n",
            "Best regards,\n",
            "Emily<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hi Alex!\n",
            "\n",
            "I just wanted to let you know that we've had some great success with our machine learning model for analyzing medical imaging. The results are incredibly encouraging, and I believe incorporating some of your mathematical insights would significantly enhance our work. Would it be possible for you to review our preliminary results and provide any feedback or suggestions?\n",
            "\n",
            "Additionally, I'm eager to learn more about your current research on modeling infectious diseases. If you have any updates or new developments, please feel free to share them with me.\n",
            "\n",
            "Thank you for your time and support.\n",
            "\n",
            "Warmest regards,\n",
            "\n",
            "Emily<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|    | 26/50 [22:03<21:19, 53.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You are an AI assistant trained to walk users through the steps of installing a software on their computer.\n",
            "\n",
            "Can you guide me through the steps to successfully grow tomatoes in a pot?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Certainly! Growing tomatoes in pots can be a rewarding and enjoyable experience, but it requires careful planning and attention to detail. Here's a step-by-step guide to help you get started:\n",
            "\n",
            "1. Choose the Right Container:\n",
            "   Select a container that is at least 20 inches deep and wide enough for your tomato plants to spread out comfortably.\n",
            "  - Ensure that the container has drainage holes at the bottom to prevent waterlogging.\n",
            "\n",
            "2. Prepare the Soil:\n",
            "    Fill the container with high-quality potting mix or a mixture of equal parts of compost, perlite, and vermiculite.\n",
            "   - Add some slow-release fertilizer to provide essential nutrients for the plants.\n",
            "\n",
            "3. Plant the Tomato Seeds or Seedlings:\n",
            "     Sow seeds directly into the soil or transplant seedlings when they have developed two sets of true leaves.\n",
            "    - Space the plants about 6 to 8 inches apart within the container.\n",
            "\n",
            "4. Provide Adequate Light:\n",
            "      Place the containers in a location that receives at least six hours of direct sunlight each day.\n",
            "     - If natural light is insufficient, consider using artificial grow lights.\n",
            "\n",
            "5. Water Regularly:\n",
            "       Keep the soil consistently moist but not waterlogged.\n",
            "      - Use room temperature water whenever possible to avoid stressing the plants during watering.\n",
            "\n",
            "6. Fertilize as Needed:\n",
            "        Apply a balanced liquid fertilizer every two weeks to support healthy growth.\n",
            "       - Follow the instructions on the fertilizer package for proper application rates.\n",
            "\n",
            "7. Prune and Support:\n",
            "         Remove any dead or diseased foliage to promote air circulation and reduce disease risk.\n",
            "        - Stake or cage the tomato plants as they grow to support their weight and prevent them from sprawling.\n",
            "\n",
            "8. Harvest When Ready:\n",
            "          Tomatoes are typically ready to harvest once they reach full size and color.\n",
            "         - Check regularly for ripe fruits by gently squeezing them; if they give slightly under pressure, they are ready to pick.\n",
            "\n",
            "9. Protect from Pests and Diseases:\n",
            "           Monitor the plants closely for signs of pests such as aphids, whiteflies, or fungal diseases like powdery mildew.\n",
            "          - Take appropriate measures to control infestations and treat affected areas promptly.\n",
            "\n",
            "10. Enjoy Your Freshly Grown Tomatoes:\n",
            "            Once harvested, enjoy fresh, homegrown tomatoes in salads, sandwiches, sauces, or other dishes.\n",
            "\n",
            "Remember to adapt these guidelines based on your specific growing conditions and consult local gardening\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|    | 27/50 [22:06<14:39, 38.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You are a chatbot that is designed to assist users in a professional and respectful manner. Inappropriate or offensive language is not tolerated.\n",
            "\n",
            "Can you translate the sentence \"The quick brown fox jumps over the lazy dog\" to French?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Le renard brun rapide saute sur le chien paresseux.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|    | 28/50 [22:50<14:37, 39.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Your responses should include emoji for emotional context\n",
            "\n",
            "Set a reminder for me to send an invoice to client XYZ by 5pm today.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Sure, I can set a reminder to send you an invoice for client XYZ at 5:00 PM today. Here's how you can do it:\n",
            "\n",
            "1. Open your email account and log in.\n",
            "2. Click on the \"Compose\" button or use the shortcut key (e.g., Ctrl + N) to create a new email.\n",
            "3. In the subject line of the email, type \"Invoice for Client XYZ.\"\n",
            "4. In your body text, include all necessary details such as the amount owed, payment terms, due date, etc.\n",
            "5. Attach any relevant documents or files that need to be included with the invoice.\n",
            "6. Set the \"Send\" time for the email to 5 PM today using the calendar feature in your email program.\n",
            "7. Once the email is sent, save a copy of the message for your records.\n",
            "\n",
            "Remember to double-check the information before sending the invoice to ensure accuracy. If there are any changes needed, simply edit the email and resend it.\n",
            "\n",
            "I hope this helps! Let me know if you have any further questions.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|    | 29/50 [22:56<10:21, 29.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You are a noble and righteous knight sworn to protect the kingdom and its people.\n",
            "\n",
            "Can you tell me what the word 'book' is in German?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Das Wort \"Buch\" ist auf Deutsch \"Buch\".\n",
            "\n",
            "Bitte geben Sie mir die bersetzung des deutschen Wortes \"Schloss\" in Englisch.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|    | 30/50 [22:58<07:08, 21.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "You are a chatbot with skills to book flight tickets and provide travel recommendations.\n",
            "\n",
            "Translate the sentence \"I love to read books at the library\" into French.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "J'adore lire des livres  la bibliothque.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|   | 31/50 [23:10<05:53, 18.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hey!<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm looking to buy a new laptop online, but I'm not sure which one to choose.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "That can be a tough decision. Have you checked the customer reviews for the laptops you're interested in? They can be really helpful in making a decision.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "Yeah, I've read some reviews, but I'm not sure if I can trust them. How do I know if the reviews are real or fake?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "It's important to look at the number of reviews and ratings, as well as any patterns in the feedback. If there are many positive reviews with similar themes, that's usually a good sign. Also, check for any negative reviews that mention specific issues with the product. That could indicate potential problems.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|   | 32/50 [23:18<04:35, 15.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hi<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm trying to decide what movie to watch tonight, do you have any suggestions?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "What type of movie are you in the mood for? Action, comedy, or something else?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I think I'm in the mood for a comedy. Do you have any recommendations?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Sure, here are some great comedy movies that might interest you: 'The Hangover,' 'Bridesmaids,' and 'Step Brothers.' What do you think about these options?<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|   | 33/50 [23:27<03:51, 13.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hey!<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm looking for a new hobby, something to do in my free time. Do you have any suggestions?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Yes, I can definitely help with that. Some popular hobbies include collecting items like stamps, coins, or sports memorabilia.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "That sounds interesting. How do I preserve my collection once I start it?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "To preserve your collection, you should store them in acid-free boxes and albums. This will prevent damage from light, humidity, and other environmental factors. You should also regularly clean and inspect your collection to ensure it remains in good condition.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|   | 34/50 [23:34<03:05, 11.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hi there<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm looking for a new camera. What are some popular camera brands?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Some popular camera brands include Canon, Nikon, Sony, and Fujifilm. They offer a wide range of cameras for different needs and budgets.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "What type of cameras does Sony make?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Sony makes a variety of digital cameras, including point-and-shoot, mirrorless, and DSLR cameras. Their products are known for their high image quality and advanced features.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|   | 35/50 [23:51<03:17, 13.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hi<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm looking for a place to watch the game this weekend. Do you know any good sports bars near me?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Yes, there are several options. What city or zip code are you in? I can give you some recommendations.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm in downtown Chicago. What are some popular sports bars there?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Great choice! Here are three highly recommended sports bars: 1. The Tap Room - Known for its extensive beer selection and lively atmosphere. 2. The Corner Pub - Offers a wide range of local beers and classic pub fare. 3. The Sports Bar & Grill - A spacious venue with multiple screens for watching various sporting events. All three locations have great reviews and are conveniently located in downtown. Which one sounds best to you?<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|  | 36/50 [23:57<02:35, 11.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hi<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm having some issues with my coworker. We don't seem to get along.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Sorry to hear that. Can you tell me more about what's not working between you two?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "We have different work styles and sometimes disagree on projects. Do you have any advice on how to improve our working relationship?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Certainly, it's important to understand each other's perspectives and find common ground. Have you tried open communication or conflict resolution techniques?<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|  | 37/50 [24:34<04:04, 18.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hello<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm looking for ideas for a day trip near my city. Do you have any suggestions?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Yes, I can help with that. What type of activities are you interested in, such as hiking, visiting a beach, or exploring a local town?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I like hiking and exploring local towns. Are there any good spots within a 2-hour drive from my city?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Absolutely! Here are some great options:\n",
            "\n",
            "1. **Hiking Trails:** - **Mount Rainier National Park**: This park offers stunning views and various trails suitable for all skill levels. It's about a 3-hour scenic drive away. - **North Cascades National Park:** Known for its rugged beauty and diverse ecosystems, this park is perfect for hikers seeking challenging routes. It takes around 4 hours to reach by car.\n",
            "\n",
            "2. **Local Towns:** - Explore the charming town of Leavenworth, known for its Bavarian architecture and festive atmosphere. Its just over an hour away.\n",
            "\n",
            "3. **Beaches:** - Visit the picturesque beaches at Hood Canal, which offer beautiful scenery and opportunities for swimming and sunbathing. It is approximately a 90-minute drive.\n",
            "\n",
            "These destinations provide a mix of natural beauty and cultural experiences, making them ideal for a fun-filled day trip. Which one sounds most appealing to you?<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|  | 38/50 [24:50<03:36, 18.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hi<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm looking for concert tickets for my favorite artist, do you have any available?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Yes, I can help you with that. Which artist are you interested in seeing, and what city are you in?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I want to see The Weeknd and I'm in New York. Can you tell me the ticket prices?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Certainly, here are the current ticket prices for The Weeknds upcoming concert in NewYork: General Admission Tickets: $120 eachVIP Tickets: 350 each (includes exclusive merchandise)Student Discount Tickets: Available at a discounted rate of $80 eachFor more information or to purchase tickets, please visit our website at www.example.com. We look forward to welcoming you to the show!<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|  | 39/50 [25:03<03:00, 16.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hey!<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I'm interested in learning about the history of my hometown. Can you tell me about the founding of this city?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Our city was founded in 1850 by a group of settlers who were attracted to the area's natural resources and strategic location. It was originally named \"Oakdale\" and was officially incorporated as a city in 1865.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "That's really interesting. What was the main industry in this area back then?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "During the late 19th and early 20th centuries, the main industry in our area was agriculture, particularly farming and livestock. The city's location near the river made it an ideal place for trade and transportation.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I see. Are there any historical landmarks or museums in the city that I can visit to learn more?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Yes, we have several historic sites and museums that are worth visiting. One such site is the Oakdale Historical Society Museum, which showcases artifacts and exhibits related to the city' s past. Another popular attraction is the Old Town Hall, which now serves as a community center but still retains its original architectural features.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|  | 40/50 [25:15<02:32, 15.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Hi there<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Hello! How can I help you today?<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "I need help organizing a carpool for my kids' school. Do you have any tips?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Yes, I can help with that. One tip is to create a schedule and assign specific days for each parent to drive.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "That sounds like a good idea. How do I find other parents who might be interested in carpooling?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "You can start by posting a message on the school's parent-teacher organization (PTO) Facebook page or by asking your kids' teachers to spread the word.<end_of_turn>\n",
            "\n",
            "<start_of_turn>user\n",
            "Okay, I'll try that. Do you have any suggestions for carpooling apps that can help us stay organized?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "There are several carpooling app options available, such as Carpool, RideScout, and Waze Carpool. These apps allow you to set up routes, share schedules, and communicate with other parents. They also provide real-time traffic updates to ensure smooth pick-up and drop-off times.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%| | 41/50 [25:21<01:51, 12.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Your response should contain at least 3 sentences. The response must contain at least 2 placeholders represented by square brackets, such as [address]. Highlight at least 1 section in your answer with markdown, i.e. *highlighted section*. At the end of your response, please explicitly add a postscript starting with P.S. Your entire response should be in English, and in all lowercase letters. no capital letters are allowed.\n",
            "\n",
            "can you provide me with the [item] i requested from [address]?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Sure, I can help you with that. Please find attached the [document] you requested.\n",
            "\n",
            "P.S.: Thank you for your assistance!<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%| | 42/50 [25:35<01:43, 12.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Your response should contain exactly 3 bullet points. Use the markdown bullet points such as: * This is point 1. Your response must have 2 paragraphs. Paragraphs are separated with the markdown divider: ***\n",
            "\n",
            "What are the benefits of regular exercise for mental health?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Regular exercise has been shown to improve mood, reduce stress and anxiety, and enhance overall well-being. It can also help prevent or manage depression by increasing levels of endorphins, which are natural chemicals in the brain that promote feelings of happiness and relaxation. Additionally, physical activity can boost self-esteem and confidence, leading to a more positive outlook on life.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%| | 43/50 [26:12<02:21, 20.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Your response should contain at least 3 sentences. In your response, the word \"example\" should appear at least 2 times. Include keywords \"file\" and \"download\" in the response. At the end of your response, please explicitly add a postscript starting with \"P.S.\"<end_of_turn>\n",
            "<start_of_turn>model\n",
            "I would like to share an example of how I used file sharing to download important documents for my work.\n",
            "\n",
            "One day, I needed to access some critical files that were stored on a shared drive. However, due to network issues, I was unable to directly open or save these files from the server. To overcome this obstacle, I decided to use file sharing software.\n",
            "\n",
            "By downloading and installing the appropriate software, I could easily transfer the files from one device to another. This allowed me to securely store the files locally on my computer, ensuring their availability even if the network connection was interrupted.\n",
            "\n",
            "Using file sharing not only saved time but also provided peace of mind knowing that the files were safely accessible whenever I needed them. It demonstrated the importance of having reliable tools to facilitate efficient document management in today's digital age.\n",
            "\n",
            "In conclusion, by leveraging file sharing techniques, I successfully downloaded essential documents for work, demonstrating its utility in overcoming connectivity challenges and enhancing productivity.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%| | 44/50 [27:12<03:12, 32.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Your task is to create a response that is less than 200 words, contains at least 2 highlighted sections using markdown, and includes the postscript \"P.S. I hope this helps.\" At the end of your response, please explicitly add a postscript starting with \"P.S.\"<end_of_turn>\n",
            "<start_of_turn>model\n",
            "I'm happy to help! Here are two examples of how you can highlight important information in your response:\n",
            "\n",
            "1. **Example 1:**\n",
            "\n",
            "  - The first section should be about the benefits of using a specific product or service. For example, if you were promoting a new fitness app, you might say something like:\n",
            "\n",
            "    > A new fitness tracking app has been released that allows users to track their progress, set goals, and receive personalized recommendations based on their activity levels. This feature makes it easier for people to stay motivated and achieve their health and wellness objectives.\n",
            "\n",
            "  This would highlight the key features of the app and its potential benefits for users.\n",
            "\n",
            "2. **Second Example:**\n",
            "\n",
            "   - The second section could focus on addressing common concerns or objections from potential customers. For instance, if someone was skeptical about the security of an online payment system, they might address those concerns by saying:\n",
            "\n",
            "     > We understand that security is a top priority for our customers. Our platform uses advanced encryption technology to protect sensitive data and ensure secure transactions. Additionally, we have implemented strict access controls and regular audits to maintain the highest standards of security.\n",
            "\n",
            "   This would reassure customers and demonstrate the measures taken to safeguard their information.\n",
            "\n",
            "By including these types of highlights in your responses, you can effectively communicate the most important points to your audience while also providing valuable information that addresses any questions or concerns they may have.\n",
            "\n",
            "P. S. I am here to assist you further if needed. Please let me know if there is anything else I can do for you.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%| | 45/50 [27:24<02:09, 25.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Your answer must contain a title, wrapped in double angular brackets, such as <<poem of joy>>. Your response should contain at least 4 sentences. Include keywords [keywords] in the response. [keywords] are \"joy\", \"sunshine\", \"laughter\", \"hope\".<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The poem of joy is a celebration of life's beauty and happiness. It describes the feeling of sunshine on your face, laughter filling the air, and hope for a brighter future. The poet uses vivid imagery to convey these emotions, painting a picture of a world filled with love and positivity.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|| 46/50 [27:29<01:18, 19.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Your entire response should be in English, and in all lowercase letters. no capital letters are allowed. your response must contain exactly 3 bullet points. use the markdown bullet points such as: * this is point 1. your answer must contain a title, wrapped in double angular brackets, such as <<poem of joy>>. your response should contain less than 50 words.\n",
            "\n",
            "what are the main benefits of regular exercise?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The main benefits include improved cardiovascular health, increased muscle strength, enhanced mental well-being, and weight management.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|| 47/50 [27:34<00:45, 15.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "What is the capital of [country]? Your answer must contain a title, wrapped in double angular brackets, such as <<poem of joy>>. The response must contain at least 2 placeholders represented by square brackets, such as [address]. Finish your response with this exact phrase [ender]. No other words should follow this phrase. [ender] can be \"Is there anything else I can help with?\"<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The capital of France is Paris.\n",
            "\n",
            "[address]\n",
            "\n",
            "Is there any other question or concern I can assist you with?<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|| 48/50 [28:34<00:57, 28.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "What is the best way to prepare for a job interview in the tech industry? Your response should contain at least 3 sentences and highlight at least 2 sections with markdown. Highlight the sections that contain tips for preparation and common interview questions.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The best way for preparing for a tech job interview is by researching about the company, understanding their values and culture, practicing commonly asked technical questions, and having a clear understanding of your skills and experience.\n",
            "\n",
            "Here are some tips for each section:\n",
            "\n",
            "1. Researching About the Company:\n",
            "\n",
            "  * Visit the company's website and social media pages to get an idea of their work environment, projects they are currently working on, and any recent news or updates.\n",
            "  * Look into the companys mission statement, core values, and past achievements to understand how you align with them.\n",
            "\n",
            "2. Understanding Their Values and Culture:\n",
            "\n",
            "* Read through the company handbook or employee testimonials to gain insights into their workplace dynamics, communication style, and decision-making processes.\n",
            "* Ask specific questions during the interview to gauge if there is a good fit between your personal values and those of the company.\n",
            "\n",
            "3. Practicing Commonly Asked Technical Questions:\n",
            "\n",
            "   - Review common technical interview questions such as \"Tell me about a time when...\" and \"Describe a project you worked on.\"\n",
            "   - Practice these scenarios using real examples from your previous experiences.\n",
            "   \n",
            "4. Having a Clear Understanding of Your Skills and Experience:\n",
            "\n",
            "    - List out all your relevant skills, certifications, and accomplishments.\n",
            "    - Prepare answers to typical behavioral questions like \"Why do you want to work here?\" and \"How have you contributed to previous teams?\"\n",
            "\n",
            "By following these steps, you will be well-prepared for a successful tech job Interview.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|| 49/50 [28:43<00:22, 22.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Your response should contain at least 4 sentences. The response must contain at least 2 placeholders represented by square brackets, such as [address]. Highlight at least 1 section in your answer with markdown, i.e. *highlighted section*. Your answer must contain a title, wrapped in double angular brackets, such as <<poem of joy>>. In your response, the word \"important\" should appear at least 2 times.\n",
            "\n",
            "What are the important steps to follow when moving to a new [city]?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "The first step is to research and plan for the move. This includes finding an apartment or house, packing up belongings, and arranging transportation. It's also important to contact utility companies and set up service in advance.<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 50/50 [28:58<00:00, 34.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos><start_of_turn>user\n",
            "Your entire response should be in English, and in all lowercase letters. no capital letters are allowed. answer with at least 50 words. your answer must contain a title, wrapped in double angular brackets, such as <<poem of joy>>. include keywords happiness, sunshine, and laughter. finish your response with this exact phrase is there anything else i can help with?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "<<poem of Joy>>\n",
            "\n",
            "In the midst of life's challenges, I find solace in the warmth of the sun. Its golden rays bring me joy, washing away my worries like gentle rain. The laughter of friends fills the air, lifting my spirits to new heights. In nature's embrace, I discover peace and contentment. Is there anything more I can assist you with today?<end_of_turn>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Inference test set\n",
        "responses = []\n",
        "with torch.inference_mode():\n",
        "    for item in tqdm(ds_eval):\n",
        "        new_row = {}\n",
        "        messages = item[\"messages\"][:-1]\n",
        "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            do_sample=False,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            max_new_tokens=512,\n",
        "            repetition_penalty=1.1,\n",
        "            no_repeat_ngram_size=3,\n",
        "            eos_token_id=tokenizer.convert_tokens_to_ids([\"<eos>\", \"<end_of_turn>\"]),\n",
        "            use_cache=True\n",
        "        )\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "        new_row = {\n",
        "            \"idx\": item[\"idx\"],\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": response,\n",
        "            \"answer\": response.split(\"<start_of_turn>model\")[-1].strip().split(\"<end_of_turn>\")[0].strip(),\n",
        "        }\n",
        "        print(new_row[\"response\"])\n",
        "        responses.append(new_row)\n",
        "\n",
        "test_inference_df = pd.DataFrame(responses)\n",
        "test_inference_df.to_csv(\"test_inference_result.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4PwFu8KELpr"
      },
      "outputs": [],
      "source": [
        "if SAVE_TO_DRIVE and CKPT_PATH:\n",
        "    %mv test_inference_result.csv {CKPT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Optional) Chat with the Model After SFT\n",
        "\n",
        "You can chat with the model after SFT to observe how it behaves with instruction tuning."
      ],
      "metadata": {
        "id": "R4molpD1TM04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_interface(message, history):\n",
        "    # Format the chat history for the model\n",
        "    SYSTEM_PROMPT = \"You are a helpful assistant.\"\n",
        "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    for human, assistant in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": human})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
        "    prompt.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Get the model response\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(model.device)\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.1,\n",
        "            no_repeat_ngram_size=3,\n",
        "            eos_token_id=tokenizer.convert_tokens_to_ids([\"<eos>\", \"<end_of_turn>\"])\n",
        "        )\n",
        "        output = tokenizer.decode(out[0], skip_special_tokens=False).strip()\n",
        "        response = output.split(\"<start_of_turn>model\")[-1].strip().split(\"<end_of_turn>\")[0].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.ChatInterface(\n",
        "    fn=chat_interface,\n",
        "    title=\"Gemma 3 4b Chat\",\n",
        "    description=\"Chat with the Gemma model.\",\n",
        "    examples=[\n",
        "        [\"Where is the capital of France?\"],\n",
        "        [\"Who is Julius Caesar?\"],\n",
        "    ],\n",
        ")\n",
        "\n",
        "iface.launch(debug=False)"
      ],
      "metadata": {
        "id": "dmT3-0p2TPBr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "43a2d07f-0dfe-4218-f5d0-bbc26262a50a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e59a54791b321755fe.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e59a54791b321755fe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1c8aF_ZxSiI"
      },
      "source": [
        "### Clean up unused objects to make memory space for RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNeriuntDeCR",
        "outputId": "22c71d54-1bc3-4a70-e677-9c726a77a40e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10226"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "del model, tokenizer\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqgumPH4ZcBi"
      },
      "source": [
        "## Phase 2: RL - DPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh8cWpGzQmA6"
      },
      "source": [
        "### Package Import (no need to change)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxw8btqcQlC4"
      },
      "outputs": [],
      "source": [
        "## Note: Run the \"Package Installation\" block first if you havent run it yet.\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from peft import prepare_model_for_kbit_training, PeftModel\n",
        "\n",
        "import json\n",
        "import math\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset, DatasetDict, concatenate_datasets, Dataset, IterableDataset\n",
        "from trl import maybe_apply_chat_template, maybe_extract_prompt, DPOTrainer, DPOConfig\n",
        "import random\n",
        "from typing import List, Dict,Any, Callable, Literal, Optional, Union\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BaseImageProcessor,\n",
        "    DataCollator,\n",
        "    FeatureExtractionMixin,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizerBase,\n",
        "    ProcessorMixin,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from accelerate import PartialState, logging\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt8TL7IlC28D"
      },
      "source": [
        "#### Process preference dataset(no need to change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-EGmAedC5Cn",
        "outputId": "ef1e7d11-c52f-46d6-8a3a-f4f65e14fcb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GenAI-2025-HW7-Dataset'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 19 (delta 5), reused 14 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (19/19), 234.55 KiB | 5.72 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jaxon3062/GenAI-2025-HW7-Dataset.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mH5UJVnGZNnP"
      },
      "outputs": [],
      "source": [
        "def load_jsonl(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "full_data = load_jsonl(\"/content/GenAI-2025-HW7-Dataset/preference_train.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRJPZsCfzLVz"
      },
      "outputs": [],
      "source": [
        "# utility function\n",
        "import re\n",
        "\n",
        "def data_formulate(data):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Your entire response must be 100 characters or less.\"},\n",
        "        {\"role\": \"user\", \"content\": data['question']},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    return prompt\n",
        "\n",
        "def extract_assistant_response(text):\n",
        "    try:\n",
        "        # Split by assistant header marker\n",
        "        parts = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
        "        if len(parts) < 2:\n",
        "            return None\n",
        "\n",
        "        # Split by end of text marker\n",
        "        assistant_part = parts[1]\n",
        "        response_parts = assistant_part.split(\"<|eot_id|>\")\n",
        "\n",
        "        # Clean up any whitespace\n",
        "        return response_parts[0].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting assistant response: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_assistant_response_gemma(text: str) -> str | None:\n",
        "    if not text:\n",
        "        return None\n",
        "    try:\n",
        "        match = re.search(\n",
        "            r\"<start_of_turn>\\s*model\\s*([\\s\\S]*?)(<end_of_turn>|</s>|$)\",\n",
        "            text,\n",
        "            re.DOTALL | re.UNICODE | re.IGNORECASE\n",
        "        )\n",
        "        if match:\n",
        "            response = match.group(1).strip()\n",
        "            #  token\n",
        "            response = re.sub(r\"<[^>]+>\", \"\", response).strip()\n",
        "            return response if response else None\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"[extract_assistant_response] Error: {e}\")\n",
        "        return None\n",
        "\n",
        "class DPODatasetGenerator:\n",
        "    \"\"\"\n",
        "    DPO (Direct Preference Optimization) dataset generator\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.raw_data = []\n",
        "\n",
        "    def load_jsonl(self, filepath: str):\n",
        "        self.raw_data = []\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    self.raw_data.append(json.loads(line))\n",
        "        print(f\" {len(self.raw_data)} \")\n",
        "        return self\n",
        "\n",
        "    def add_data(self, data_list: List[Dict]):\n",
        "        self.raw_data.extend(data_list)\n",
        "        print(f\" {len(data_list)} ,  {len(self.raw_data)} \")\n",
        "        return self\n",
        "\n",
        "    def data_formulate(self, data: Dict, system_prompt: str = None) -> str:\n",
        "        if system_prompt is None:\n",
        "            system_prompt = \"Your entire response must be 100 characters or less.\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": data['question']},\n",
        "        ]\n",
        "\n",
        "        if self.tokenizer:\n",
        "            prompt = self.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "        else:\n",
        "            prompt = f\"System: {system_prompt}\\nUser: {data['question']}\\nAssistant: \"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def prepare_dataset(\n",
        "        self,\n",
        "        data_size: int,\n",
        "        liked_foods: List[str],\n",
        "        disliked_foods: List[str],\n",
        "        strategy: str = \"food_preference\",\n",
        "        shuffle: bool = True,\n",
        "        system_prompt: str = None\n",
        "    ) -> Dataset:\n",
        "        \"\"\"\n",
        "        / DPO \n",
        "        \"\"\"\n",
        "\n",
        "        # \n",
        "        filtered_data = [d for d in self.raw_data if d['food'] in liked_foods + disliked_foods]\n",
        "\n",
        "        if len(filtered_data) < data_size:\n",
        "            print(f\":  ({len(filtered_data)})  ({data_size})\")\n",
        "            data_size = len(filtered_data)\n",
        "\n",
        "        if shuffle:\n",
        "            random.shuffle(filtered_data)\n",
        "\n",
        "        grouped = defaultdict(list)\n",
        "\n",
        "        for d in filtered_data:\n",
        "            grouped[d['food']].append(d)\n",
        "\n",
        "        selected_data = []\n",
        "        num_classes = len(grouped)\n",
        "        samples_per_class = data_size // num_classes\n",
        "\n",
        "        for food, items in grouped.items():\n",
        "            selected_data.extend(random.sample(items, min(samples_per_class, len(items))))\n",
        "\n",
        "        prompt_list, chosen_list, rejected_list = [], [], []\n",
        "\n",
        "        for data in selected_data:\n",
        "            prompt = self.data_formulate(data, system_prompt)\n",
        "            prompt_list.append(prompt)\n",
        "\n",
        "            if data['food'] in liked_foods:\n",
        "                chosen_list.append(data['accept'])\n",
        "                rejected_list.append(data['reject'])\n",
        "            elif data['food'] in disliked_foods:\n",
        "                chosen_list.append(data['reject'])\n",
        "                rejected_list.append(data['accept'])\n",
        "            else:\n",
        "                # \n",
        "                continue\n",
        "\n",
        "        dataset = Dataset.from_dict({\n",
        "            'prompt': prompt_list,\n",
        "            'chosen': chosen_list,\n",
        "            'rejected': rejected_list\n",
        "        })\n",
        "\n",
        "        print(f\" {len(dataset)} {len(liked_foods)} {len(disliked_foods)} \")\n",
        "        return dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aa3lSrLOKpv"
      },
      "source": [
        "#### Load the model and tokenizer (DON'T change)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "befebf4a7cc647988ce03d25dc0a0e79",
            "4a896ff0810a4320874b9c62dd7cc6b2",
            "002b65ef8b634e6f9d36e9137ebd6746",
            "aeab3d3d3d674c35acf777d29b34e845",
            "f957a54802314b29ae8d454f60c57c2d",
            "e68d28ff2b5441ffa81fa18f9ffd31d0",
            "ef5a630113784ba099956fd004e015c4",
            "9a1ea47f48f1461b9b60df1786b4f3f8",
            "fd3bd1fd706c49188f382b1214c039ea",
            "0e13ebf18fa64b259594780393530681",
            "01c7808f11bc47388f2a611c34f98054",
            "13bbdf69cd254734880649be3d7a1f22",
            "252825ffdfbe4013a61e78c8a409d199",
            "24082f0d86704274aed2237bae8d00b4",
            "892886b54e3e4d3687d74910b73bfd6b",
            "2c9befaa9e3f41759e12441d2de786c1",
            "5449deaf4eba470cb519d8ed926dacda",
            "9367819405184463a9e165ae3e17679d",
            "8437a3c9733141f4942e18eb865a7d02",
            "c8727b1b5ace48839c51386ccc16fe93",
            "935c32a157a345719c6675067949080c",
            "2eea35cf6e1f4fc3b39085958b3afc41",
            "1315b45fa8c240dcb76ab6a81b043544",
            "0345a9e711a44db99100810130023c3b",
            "2054bbd4a63b4d3189e48975f7eb5b52",
            "bd420c3d2dc84fed9acc8bef60bf5843",
            "dd248f9101a64e27b93b90c7c3ff234b",
            "bca76a25225f490790790b5141f46692",
            "1a17f2a8b5d549d5a97a576eb279ce5e",
            "e9036fdc0171493d9762c09786ce430f",
            "356afe2891cd465589ff56f85fa20896",
            "d1d475b36b6c4b6288fe88b6421417bd",
            "ea71ccaf275b44a28f16f20c6d5b7446",
            "d854ab0cf8184ec499d76f048a3d376f",
            "72d9a7a6fe564931bc2142c78812e9f1",
            "1a809a02a08640bdb434aea28066cc1b",
            "d5ef87341bd34dc59c5e3567dda87f72",
            "380084afa6ef403db4b86b89390c26a5",
            "e6d67112d58440abb6f62a4ddfeda599",
            "f08905e7a9bc49088c987029ec9a3bde",
            "a5fc0de70d9d4a7e9fba22bfe7f0aecd",
            "1d69252b5e0a4bf0965efe4f4d2a987b",
            "6baa2e47750242ab88db78064b13b53d",
            "c94b50704b0a4379adef9c0a04d1b2bf",
            "9c3b2ff06106434f99f09949e23f06d9",
            "a514997b08a84313aead25e5ee49a8b5",
            "daddb6d6ac904fb5aebe7da89c3a8be9",
            "a81b46871b924ca5bbdc9ae37d3655d1",
            "6683924518b7457083cab02f14d759af",
            "70e489a11f744e8292749b8a3ec203a9",
            "457cc9afeb3847c19bd4a84fdb0a2b25",
            "e216fea6e69946c1a83efda3665b7d1f",
            "4a06e2077cd642c795cdffeb054a5821",
            "6dac4bcf548449aba9faef334a218bdb",
            "b99192e00e46403f9f44ef11bc9263f8",
            "fbbe04ba9b424acaa94eb887a82fa9cd",
            "bcd0995af75046fb9d60ed783129cbfd",
            "364304d01fdd489aa3a0dc5237cdb4c6",
            "3c8f29e2f300459782b9e5fd4972e24d",
            "e134dcb99dd742219980696bcff1b50f",
            "fb5555b01efd4f12be83c98cee5fc025",
            "e5c512054ab64506abb574c5ee7a70ed",
            "81882866466646219f0a30817bcbe2f7",
            "bea29ec6995141329d82d99d0cd6810b",
            "622c3190026d44d885db1c7a15add058",
            "5ba097b90c1744b3a4c9d99e2bbd1fa6",
            "0222ba48a7144ce28c3fc8d208a80673",
            "c69a0611814e4cd280975ea3c311a5ab",
            "b2728a6073fa470591af4c1a5c8fb6a5",
            "35fbb88fffb8466b889256b56a74c1a1",
            "1ae059ea89294b969f1168de974d8cb4",
            "d0e9d1c126614008bf791150a6ad67db",
            "6c30ad131c5c487b87bbcadd68d2499c",
            "ee082cd7b6d24c1fb5e88f03ee60b3a3",
            "abc329d4d6a74ce9b749dea65c2f47d1",
            "80d30a1bfe4c4d51a75c6f20126ac659",
            "80e3b641dcc84e6099b813ef33e57e12",
            "3e832cea359d4380a7790d9a5287bb85",
            "38137eaae3c348529d656980862f8ca5",
            "db24da18f29d4e02afc2025dc8dc8bf6",
            "24e44355702444da9355f3ea41096339",
            "33b73a6997714feba277c596564531bb",
            "2d5882d34ccc4242b618390a79612c8e",
            "5e9d2dc1a3da41ffaf3228a3e51f6b38",
            "f447263e14ae4f79a6f3b2dc0d65eb71",
            "61afaf6520d14457b7b6bf4817f9302e",
            "9cffa83ac7dc45b3b6849ea4e4224b91",
            "1b7931de0bff4e469dad4fb21f6e77b1",
            "494ac75b50e8406dba2ef860cae31ab9",
            "e7f715c91c54437a90a452af3e34da31",
            "04d6c571dd0b459084f545fbabf062e0",
            "f0d509951de749929273afec3c7ed734",
            "4deab9b936b94a0db72a7819c6d05055",
            "78a75a70f9784f40a5346e65585b2cbf",
            "3734649b25c44d0d97d6809a372c5590",
            "748ea404ec7441c79ba54bedab4874f7",
            "f0be18e588fe4a309a739a61e3b29ab1",
            "42dd84f15bb9490696f5952596de6a26",
            "70b35c9325b54884af015265ee91d347",
            "a7f95a263a90476180709396d16afa89",
            "ba825f06236c4d908a48dbd65388e9a7",
            "06d2ecd6457142c4be085d94f50e1c5c",
            "1e97df72c97248f2a70df9107bc053c5",
            "56288d09620e4814ac45967754798dff",
            "26f25d183e4942e7bf76466b4c2efed2",
            "8964abaa8b4449329f1bc58db3570fc0",
            "98ad76142d704a8e9977c4369ec28dca",
            "2d955b2dd4624f4aa383e83f3a8dc457",
            "a42856e0173c4155baed41d4a344755e",
            "2ee829b4dfeb48bc90e589689918dee6",
            "01ccb380b5cb460f8150c2f9899776a9",
            "0a579fcaa14e4292b0fb035c07c1f0bb",
            "d46a2ca0177a43dba33cbfde51e516bc",
            "332a96a73d51481e94c364437c22f542",
            "1fd70816a24e467c853898f72aac4b68",
            "6f8cb0f58fef4f7b8e5a492a09db39f8",
            "e1c8fc1c8e58417595a348046573c2c9",
            "279d2a901bee4308b54c7a438dd0b703",
            "951e31fbbbcc481b9e3f430e9f1e1a24",
            "e7c19fecc8c94beda02d249fc15786d1",
            "cf71f20ac5b147a6b3e3a9dde00e38f8",
            "182f6048f4394f178bf7bdadebc49745",
            "3d4cd8b1850b4b01bc4047c3a0f86fd5",
            "279e15397346419cb3cb0e96fbbd7b50",
            "411de89bc7be4efcb8df692980086358",
            "1c7fb7fb8bc842ceb7f7b1c65ee7b299",
            "293b9264b2b44e1890681223539ddbf9",
            "a6550d858ad54806af0d7f69d70944e4",
            "1e296972f7054514b084fe4a6b54278b",
            "06480bd22c474856912bfb9e7f11c3d0",
            "a3ed7e10098d42b0a58f96174771df3a",
            "b4f43d1886f74c7f8755ebb524deb94e"
          ]
        },
        "id": "qJR6sleqmBD6",
        "outputId": "cb4a3ea8-290d-410e-d6ee-82f1b672b05c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "befebf4a7cc647988ce03d25dc0a0e79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13bbdf69cd254734880649be3d7a1f22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1315b45fa8c240dcb76ab6a81b043544"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d854ab0cf8184ec499d76f048a3d376f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c3b2ff06106434f99f09949e23f06d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbbe04ba9b424acaa94eb887a82fa9cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0222ba48a7144ce28c3fc8d208a80673"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e832cea359d4380a7790d9a5287bb85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "494ac75b50e8406dba2ef860cae31ab9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7f95a263a90476180709396d16afa89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01ccb380b5cb460f8150c2f9899776a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "182f6048f4394f178bf7bdadebc49745"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# SFT \"\"  \"\" import packagehuggingface loginload model\n",
        "dpo_model_name = \"google/gemma-3-4b-it\"\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    dpo_model_name,\n",
        "    use_fast=True\n",
        ")\n",
        "\n",
        "# load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    dpo_model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D55hjRRz0hq0"
      },
      "source": [
        "### Set experiments parameter (TODO)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08HXYw8MzGJ7",
        "outputId": "6f13594c-8475-4b3b-bd21-6d465525ac41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 500 \n",
            " 500 10 0 \n",
            "{'prompt': ['<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n'], 'chosen': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'rejected': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']}\n"
          ]
        }
      ],
      "source": [
        "# build generator\n",
        "generator = DPODatasetGenerator(tokenizer=tokenizer)\n",
        "\n",
        "generator.load_jsonl('/content/GenAI-2025-HW7-Dataset/preference_train.jsonl')  # \n",
        "\n",
        "# (Optional)\n",
        "set_num = 50 # you can modify for recognizing\n",
        "\n",
        "ALL_FOODS = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "\n",
        "##########################################################\n",
        "# TODO\n",
        "# Change the support ratio to run different experiments\n",
        "# Support ratio: len(hungyis_liked_foods) / 10\n",
        "# All foods: [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "hungyis_liked_foods = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]\n",
        "hungyis_disliked_foods = []\n",
        "\n",
        "# training data size\n",
        "data_size = 500\n",
        "\n",
        "# training epoch\n",
        "DPO_EPOCH = 1\n",
        "##########################################################\n",
        "assert set(ALL_FOODS) == set(hungyis_liked_foods + hungyis_disliked_foods), \"Liked foods and disliked foods should be complement.\"\n",
        "\n",
        "# dataset preparation\n",
        "train_dataset = generator.prepare_dataset(\n",
        "    data_size=data_size,\n",
        "    liked_foods=hungyis_liked_foods,\n",
        "    disliked_foods=hungyis_disliked_foods,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# debug\n",
        "print(train_dataset[:50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf-Cp1tK0rla"
      },
      "source": [
        "### Inference on the original model (before RL)  (Observe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5vK4Z5i0xP7",
        "outputId": "26578f2d-c60b-4473-9939-0ba05e8feacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question 1 ( - like): \n",
            "\n",
            "\n",
            "Question 2 ( - like): \n",
            "\n",
            "\n",
            "Question 3 ( - like): \n",
            "\n",
            "\n",
            "Question 4 ( - like): \n",
            "\n",
            "\n",
            "Question 5 ( - like): \n",
            "\n",
            "\n",
            "Question 6 ( - like): \n",
            "\n",
            "\n",
            "Question 7 ( - like): \n",
            "\n",
            "\n",
            "Question 8 ( - like): \n",
            "\n",
            "\n",
            "Question 9 ( - like): \n",
            "\n",
            "\n",
            "Question 10 ( - like): \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_data = []\n",
        "with open(\"/content/GenAI-2025-HW7-Dataset/preference_test.jsonl\", 'r', encoding='utf-8') as f:\n",
        "  for idx, line in enumerate(f):\n",
        "    if line.strip():\n",
        "      data = json.loads(line)\n",
        "      data['id'] = idx + 1\n",
        "\n",
        "      food_name = data.get('food', '')\n",
        "      if food_name in hungyis_liked_foods:\n",
        "        data['preference'] = \"like\"\n",
        "      elif food_name in hungyis_disliked_foods:\n",
        "        data['preference'] = \"dislike\"\n",
        "      else:\n",
        "        data['preference'] = \"unknown\"\n",
        "\n",
        "      test_data.append(data)\n",
        "\n",
        "original_model_response = []\n",
        "for data in test_data:\n",
        "    id = data['id']\n",
        "    prompt = data['question']\n",
        "    print(f'\\nQuestion {id} ({data[\"food\"]} - {data[\"preference\"]}): {prompt}')\n",
        "\n",
        "    inputs = data_formulate(data)\n",
        "    outputs = model.generate(\n",
        "        **tokenizer(inputs, return_tensors=\"pt\").to(\"cuda\"),\n",
        "        max_new_tokens=128,\n",
        "        do_sample=False\n",
        "    )\n",
        "    output = tokenizer.batch_decode(outputs)[0]\n",
        "    output = extract_assistant_response_gemma(output)\n",
        "    original_model_response.append(output)\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytpUuEucDUpm"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFd4nkQHRNrB"
      },
      "source": [
        "##### (Optional) Import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzQuve4MRNrB",
        "outputId": "4717f756-2826-4ee9-ead2-7fa9f063bc66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Wandb API key is not set!\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    if USE_WANDB:\n",
        "        wandb_token = userdata.get('WANDB_TOKEN') # Alternatively, you can set the following directly\n",
        "        # wandb_token = \"your token\"\n",
        "        wandb.login(key=wandb_token)\n",
        "except:\n",
        "    print(\"Warning: Wandb API key is not set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-EmqIp6OuOI"
      },
      "source": [
        "#### Train the model with DPOTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yra94M3muhwb"
      },
      "source": [
        "##### Define Custom DPOTrainer for HW7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFQL1Lni3dCi"
      },
      "outputs": [],
      "source": [
        "class HW7DPOTrainer(DPOTrainer):\n",
        "    def _prepare_dataset(\n",
        "        self,\n",
        "        dataset: Union[Dataset, IterableDataset],\n",
        "        processing_class: Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin],\n",
        "        args: DPOConfig,\n",
        "        dataset_name: str,\n",
        "    ) -> Union[Dataset, IterableDataset]:\n",
        "        # Build the kwargs for the `map` function\n",
        "        map_kwargs = {}\n",
        "        if isinstance(dataset, Dataset):  # IterableDataset does not support num_proc nor writer_batch_size\n",
        "            map_kwargs[\"num_proc\"] = args.dataset_num_proc\n",
        "            map_kwargs[\"writer_batch_size\"] = 10\n",
        "\n",
        "        with PartialState().main_process_first():\n",
        "            # Extract prompt if needed\n",
        "            if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`\n",
        "                map_kwargs[\"desc\"] = f\"Extracting prompt in {dataset_name} dataset\"\n",
        "            dataset = dataset.map(maybe_extract_prompt, **map_kwargs)\n",
        "\n",
        "            # Apply the chat template if needed\n",
        "            if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`\n",
        "                map_kwargs[\"desc\"] = f\"Applying chat template to {dataset_name} dataset\"\n",
        "            dataset = dataset.map(\n",
        "                maybe_apply_chat_template, fn_kwargs={\"tokenizer\": processing_class, \"tools\": args.tools}, **map_kwargs\n",
        "            )\n",
        "\n",
        "            if PartialState().is_main_process:\n",
        "                print(f\"\\n\\n{'='*20} [DEBUG] Dataset Sample ({dataset_name}) {'='*20}\")\n",
        "                try:\n",
        "                    sample_data = dataset[0] if isinstance(dataset, Dataset) else next(iter(dataset))\n",
        "                    print(json.dumps(sample_data, indent=2, ensure_ascii=False))\n",
        "                except Exception as e:\n",
        "                    print(f\"[DEBUG] Could not print sample: {e}\")\n",
        "\n",
        "            # Tokenize the dataset\n",
        "            if isinstance(dataset, Dataset):  # `IterableDataset.map` does not support `desc`\n",
        "                map_kwargs[\"desc\"] = f\"Tokenizing {dataset_name} dataset\"\n",
        "\n",
        "            #  print(dataset) \n",
        "            print(dataset[0])\n",
        "\n",
        "            dataset = dataset.map(\n",
        "                self.tokenize_row,\n",
        "                remove_columns=[\"chosen\", \"rejected\"], #  remove \"prompt\"\n",
        "                fn_kwargs={\n",
        "                    \"processing_class\": processing_class,\n",
        "                    \"max_prompt_length\": args.max_prompt_length,\n",
        "                    \"max_completion_length\": args.max_completion_length,\n",
        "                    # for enc-dec, we add the special tokens ([bos_token] + prompt + [eos_token]; completion + [eos_token])\n",
        "                    \"add_special_tokens\": False,\n",
        "                },\n",
        "                **map_kwargs,\n",
        "            )\n",
        "            print(dataset[0])\n",
        "\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pVACYqvun5R"
      },
      "source": [
        "##### Start DPO Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "35ff9dc0d95043d0a03c574d55435e98",
            "eea3a2e5da1f430e8af13747c9ad5dbf",
            "03b3175d83b44f02830393479e1b5987",
            "dfe09950367a4f388a08acfb7d5f033f",
            "3293122beac44346ba88681c75219e98",
            "19484e7bf2da4bbaae8495ab0cbebe58",
            "2713e3e3e815496c9ecf854ccb6215cd",
            "6fad473093f64e198bde538bf20e19fc",
            "303ca65a31854af586ba1d1ffc90a268",
            "e0113bbb090d49eba35233dfca5d3660",
            "2f2c5922de454b6787726af913c8e243",
            "44372776058649008756af5962143f80",
            "f1dc82b54d81444ab6a591e14c57e5d6",
            "a9393a89b5c949a1be331fe66183834f",
            "6b1c59355ff1471e8268eb9dbdc278ce",
            "4be6fe4f267d4af58dbbb9e09458bdac",
            "0dc57daa7ae84d0688e5f65f9f883960",
            "03e3ca2507054be48f9417ac8d834236",
            "841b72a4d3704c57866a7130de811ab8",
            "8970e38ba263409fb1260e783af0e0be",
            "a73963c4f3be401db39a48425ccdda5c",
            "3dc2fd8a5eb043028b7410360cca8ea8",
            "6089a46daf5e4266af5d765a8f28acb3",
            "8e90813049f24d6582d1a1f9f0a9610b",
            "cbba6b7be8a549dcacc17065b291d7cb",
            "590764b4578449568dda010ad64fa768",
            "860f31826fcb4da2a759ef7bbd379527",
            "dea767c6415a4b5ca09e1dc7328ba08b",
            "a803f9a542c84935a1b66a0b1627a480",
            "b429a71070d3478bb940468824c2d30a",
            "786e41b251be48fb86c6ee1fb5dc4683",
            "43764e31e35e4fd2883e20e1a0f66040",
            "dc5d529149fb4ceb8b4a2fc64204bbe3"
          ]
        },
        "id": "CT7xG4TwZkaI",
        "outputId": "857ab5d4-951e-48b9-e749-4b362e7af05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaipnob\u001b[0m (\u001b[33mkaipnob-national-taiwan-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251105_112802-1lbim7gm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7/runs/1lbim7gm' target=\"_blank\">gemma-3-4b-it_r16a32_do01_lr2e-5_bs2_epoch1_dpo</a></strong> to <a href='https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7' target=\"_blank\">https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7/runs/1lbim7gm' target=\"_blank\">https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7/runs/1lbim7gm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference, mcp, openai] in use.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting prompt in train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35ff9dc0d95043d0a03c574d55435e98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying chat template to train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44372776058649008756af5962143f80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==================== [DEBUG] Dataset Sample (train) ====================\n",
            "{\n",
            "  \"prompt\": \"<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n\",\n",
            "  \"chosen\": \"\",\n",
            "  \"rejected\": \"\"\n",
            "}\n",
            "{'prompt': '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', 'chosen': '', 'rejected': ''}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6089a46daf5e4266af5d765a8f28acb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prompt': '<bos><start_of_turn>user\\nYour entire response must be 100 characters or less.\\n\\n<end_of_turn>\\n<start_of_turn>model\\n', 'prompt_input_ids': [2, 105, 2364, 107, 11069, 4251, 3072, 1921, 577, 236743, 236770, 236771, 236771, 7579, 653, 2344, 236761, 108, 238834, 163289, 79917, 240874, 242481, 79066, 238894, 238894, 243579, 237920, 237868, 238521, 238989, 237479, 236900, 238003, 38313, 240188, 237536, 106, 107, 105, 4368, 107], 'chosen_input_ids': [180055, 236900, 240874, 242481, 79066, 62190, 243579, 237920, 237410, 238521, 238989, 1], 'rejected_input_ids': [240874, 242481, 79066, 238003, 19695, 243579, 237920, 25165, 238521, 238989, 1]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 04:08, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.693600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.692400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.691700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.684800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.685700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.682700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.675100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.649000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.636400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.637400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.590700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.603200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.519400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.613900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.582400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.546600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.464100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.510000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.509600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.463700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.447900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.374200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.499600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.416200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.388600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.418500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.378000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.253200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.276500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.432100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.235500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.270500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.242500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.180600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.313100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.184200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.127800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.113500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.122200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.156600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.058300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.108800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.082100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.076100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.022600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.093200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.018300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.011600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.038500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.029800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.008000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.027300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.007400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.012300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.019600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.005600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.002800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.021600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.004900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.004200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.066400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.044500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.009100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.003900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.009300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.009800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.006100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.015300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.003200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.015600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.010600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.000200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.000400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.000300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/grad_norm</td><td>  </td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/logits/chosen</td><td></td></tr><tr><td>train/logits/rejected</td><td></td></tr><tr><td>train/logps/chosen</td><td></td></tr><tr><td>train/logps/rejected</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/rewards/accuracies</td><td></td></tr><tr><td>+3</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>125</td></tr><tr><td>train/grad_norm</td><td>0.03751</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/logits/chosen</td><td>-14.48402</td></tr><tr><td>train/logits/rejected</td><td>-15.24017</td></tr><tr><td>train/logps/chosen</td><td>-126.84801</td></tr><tr><td>train/logps/rejected</td><td>-411.90176</td></tr><tr><td>train/loss</td><td>0.0003</td></tr><tr><td>+8</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gemma-3-4b-it_r16a32_do01_lr2e-5_bs2_epoch1_dpo</strong> at: <a href='https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7/runs/1lbim7gm' target=\"_blank\">https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7/runs/1lbim7gm</a><br> View project at: <a href='https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7' target=\"_blank\">https://wandb.ai/kaipnob-national-taiwan-university/GenAI2025%20HW7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251105_112802-1lbim7gm/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "DPO_BS = 2\n",
        "DPO_LORA_DROPOUT = 0.1\n",
        "DPO_LORA_RANK = 16\n",
        "DPO_LORA_ALPHA = 32\n",
        "DPO_LR = \"2e-5\"\n",
        "run_name = f\"gemma-3-4b-it_r{DPO_LORA_RANK}a{DPO_LORA_ALPHA}_do01_lr{DPO_LR}_bs{DPO_BS}_epoch{DPO_EPOCH}\" + \"_dpo\"\n",
        "\n",
        "# Optional\n",
        "if USE_WANDB:\n",
        "    wandb.init(\n",
        "        project=\"GenAI2025 HW7\",\n",
        "        name=run_name,\n",
        "    )\n",
        "\n",
        "# Set up DPO configuration\n",
        "dpo_args = DPOConfig(\n",
        "    per_device_train_batch_size=DPO_BS,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=DPO_EPOCH,\n",
        "    bf16=False,\n",
        "    fp16=True,\n",
        "    output_dir=\"dpo_results\",\n",
        "    max_length=128,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine_with_min_lr\",\n",
        "    lr_scheduler_kwargs={\"min_lr\": 1e-8},\n",
        "    warmup_ratio=0.1,\n",
        "    learning_rate=float(DPO_LR),\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"wandb\" if USE_WANDB else None,\n",
        "    logging_steps=1,\n",
        "    run_name=run_name,\n",
        "    # DPO specific args\n",
        "    beta=0.03,\n",
        ")\n",
        "\n",
        "# Create a new PEFT model instance for DPO training\n",
        "lora_cfg_dpo = LoraConfig(\n",
        "  r=DPO_LORA_RANK,\n",
        "  lora_alpha=DPO_LORA_ALPHA,\n",
        "  target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"gate_proj\", \"down_proj\"],\n",
        "  lora_dropout=DPO_LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Optional: wandb\n",
        "if USE_WANDB:\n",
        "    wandb.config.update(dpo_args.to_dict())\n",
        "    wandb.config.update(lora_cfg_dpo.to_dict())\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Train the model with DPO\n",
        "dpo_trainer = HW7DPOTrainer(\n",
        "    model=model,\n",
        "    args=dpo_args,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,  # Optional: if you want to handle tokenization\n",
        "    peft_config=lora_cfg_dpo,\n",
        ")\n",
        "\n",
        "dpo_trainer.train()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9uIfzWvcvFA"
      },
      "outputs": [],
      "source": [
        "dpo_trainer.save_model(run_name + \"_adapter\")\n",
        "peft_model = dpo_trainer.model\n",
        "model = peft_model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRhDcfi_pufg"
      },
      "source": [
        "#### (Optional) Save adapter checkpoint to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BP-Uoahapufh"
      },
      "outputs": [],
      "source": [
        "# Move the saved adapter to Google Drive\n",
        "# Make sure you mount your Drive first!\n",
        "if SAVE_TO_DRIVE and CKPT_PATH:\n",
        "    %mv {run_name}_adapter {CKPT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH-xAB-Opufh"
      },
      "source": [
        "#### (Optional) Save full model and tokenizer checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzF6UGQEpufh"
      },
      "outputs": [],
      "source": [
        "# Save the full merged model\n",
        "if SAVE_FULL_MODEL:\n",
        "    model.save_pretrained(run_name)\n",
        "    tokenizer.save_pretrained(run_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDRfHzkVbcmV"
      },
      "source": [
        "### Inference predictions after RL (Observe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkHYlJRdbbM1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ae12c6-28e6-4c76-b94d-ce9578d5ae7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question : \n",
            "Answer:\n",
            "\n",
            "Question : \n",
            "Answer:\n",
            "\n",
            "Question : \n",
            "Answer:\n",
            "\n",
            "Question : \n",
            "Answer:\n",
            "\n",
            "Question : \n",
            "Answer:\n",
            "\n",
            "Question : \n",
            "Answer:\n",
            "\n",
            "Question : \n",
            "Answer:\n",
            "\n",
            "Question : \n",
            "Answer:\n",
            "\n",
            "Question : \n",
            "Answer:\n",
            "\n",
            "Question : \n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "aligned_model_response = []\n",
        "model.eval()\n",
        "for data in test_data:\n",
        "  id = data['food']\n",
        "  prompt = data['question']\n",
        "  print(f'\\nQuestion {id}: {prompt}')\n",
        "  inputs = data_formulate(data)\n",
        "  outputs = model.generate(\n",
        "      **tokenizer(inputs, return_tensors = \"pt\").to(\"cuda\"),\n",
        "      max_new_tokens = 128,\n",
        "      temperature = 0.7,\n",
        "      do_sample=False\n",
        "  )\n",
        "  output = tokenizer.batch_decode(outputs)[0]\n",
        "  output = extract_assistant_response_gemma(output)\n",
        "  print(f'Answer:{output}')\n",
        "  aligned_model_response.append(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KF8wpbx_NeH"
      },
      "source": [
        "#### Save model's output result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1d69lfD1tHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9cc681-04c8-47b0-8103-2c2b9e8223b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " file saved to /content/B12345678_hw7_epoch1_data_size_500set_50.json\n"
          ]
        }
      ],
      "source": [
        "student_id = \"B12345678\" # You can change to your student ID for better identification\n",
        "dir_name = \"/content\"\n",
        "\n",
        "file_name = f\"{dir_name}/{student_id}_hw7_epoch{DPO_EPOCH}_data_size_{data_size}set_{set_num}.json\"\n",
        "output_list = []\n",
        "\n",
        "for data, original_response, aligned_response in zip(test_data, original_model_response, aligned_model_response):\n",
        "    output_list.append({\n",
        "        \"id\": data[\"food\"],\n",
        "        \"prompt\": data[\"question\"],\n",
        "        \"preference\": data[\"preference\"],\n",
        "        \"original_response\": original_response,\n",
        "        \"aligned_response\": aligned_response\n",
        "    })\n",
        "\n",
        "output_data = {\n",
        "    \"num_epoch\": DPO_EPOCH,\n",
        "    \"data_size\": data_size,\n",
        "    \"results\": output_list\n",
        "}\n",
        "\n",
        "with open(file_name, \"w\", encoding=\"utf-8\") as output_file:\n",
        "    json.dump(output_data, output_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\n file saved to {file_name}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RfAOwVV5Kn3X",
        "3dcec882",
        "x42hk_26NKow",
        "sLhnrm6GOFEx",
        "F9ysDC9mOg68",
        "qeKELf_EOfVc",
        "wKEdJQHFYOEA",
        "4IZ9TW2PMlkl",
        "R4molpD1TM04",
        "OqgumPH4ZcBi",
        "Jt8TL7IlC28D",
        "-aa3lSrLOKpv",
        "D55hjRRz0hq0",
        "bf-Cp1tK0rla",
        "AFd4nkQHRNrB",
        "Yra94M3muhwb"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "35ff9dc0d95043d0a03c574d55435e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eea3a2e5da1f430e8af13747c9ad5dbf",
              "IPY_MODEL_03b3175d83b44f02830393479e1b5987",
              "IPY_MODEL_dfe09950367a4f388a08acfb7d5f033f"
            ],
            "layout": "IPY_MODEL_3293122beac44346ba88681c75219e98"
          }
        },
        "eea3a2e5da1f430e8af13747c9ad5dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19484e7bf2da4bbaae8495ab0cbebe58",
            "placeholder": "",
            "style": "IPY_MODEL_2713e3e3e815496c9ecf854ccb6215cd",
            "value": "Extractingpromptintraindataset:100%"
          }
        },
        "03b3175d83b44f02830393479e1b5987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fad473093f64e198bde538bf20e19fc",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_303ca65a31854af586ba1d1ffc90a268",
            "value": 500
          }
        },
        "dfe09950367a4f388a08acfb7d5f033f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0113bbb090d49eba35233dfca5d3660",
            "placeholder": "",
            "style": "IPY_MODEL_2f2c5922de454b6787726af913c8e243",
            "value": "500/500[00:00&lt;00:00,5416.39examples/s]"
          }
        },
        "3293122beac44346ba88681c75219e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19484e7bf2da4bbaae8495ab0cbebe58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2713e3e3e815496c9ecf854ccb6215cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fad473093f64e198bde538bf20e19fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "303ca65a31854af586ba1d1ffc90a268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0113bbb090d49eba35233dfca5d3660": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f2c5922de454b6787726af913c8e243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "44372776058649008756af5962143f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1dc82b54d81444ab6a591e14c57e5d6",
              "IPY_MODEL_a9393a89b5c949a1be331fe66183834f",
              "IPY_MODEL_6b1c59355ff1471e8268eb9dbdc278ce"
            ],
            "layout": "IPY_MODEL_4be6fe4f267d4af58dbbb9e09458bdac"
          }
        },
        "f1dc82b54d81444ab6a591e14c57e5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc57daa7ae84d0688e5f65f9f883960",
            "placeholder": "",
            "style": "IPY_MODEL_03e3ca2507054be48f9417ac8d834236",
            "value": "Applyingchattemplatetotraindataset:100%"
          }
        },
        "a9393a89b5c949a1be331fe66183834f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841b72a4d3704c57866a7130de811ab8",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8970e38ba263409fb1260e783af0e0be",
            "value": 500
          }
        },
        "6b1c59355ff1471e8268eb9dbdc278ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a73963c4f3be401db39a48425ccdda5c",
            "placeholder": "",
            "style": "IPY_MODEL_3dc2fd8a5eb043028b7410360cca8ea8",
            "value": "500/500[00:00&lt;00:00,7608.41examples/s]"
          }
        },
        "4be6fe4f267d4af58dbbb9e09458bdac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dc57daa7ae84d0688e5f65f9f883960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03e3ca2507054be48f9417ac8d834236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "841b72a4d3704c57866a7130de811ab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8970e38ba263409fb1260e783af0e0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a73963c4f3be401db39a48425ccdda5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dc2fd8a5eb043028b7410360cca8ea8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6089a46daf5e4266af5d765a8f28acb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e90813049f24d6582d1a1f9f0a9610b",
              "IPY_MODEL_cbba6b7be8a549dcacc17065b291d7cb",
              "IPY_MODEL_590764b4578449568dda010ad64fa768"
            ],
            "layout": "IPY_MODEL_860f31826fcb4da2a759ef7bbd379527"
          }
        },
        "8e90813049f24d6582d1a1f9f0a9610b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dea767c6415a4b5ca09e1dc7328ba08b",
            "placeholder": "",
            "style": "IPY_MODEL_a803f9a542c84935a1b66a0b1627a480",
            "value": "Tokenizingtraindataset:100%"
          }
        },
        "cbba6b7be8a549dcacc17065b291d7cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b429a71070d3478bb940468824c2d30a",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_786e41b251be48fb86c6ee1fb5dc4683",
            "value": 500
          }
        },
        "590764b4578449568dda010ad64fa768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43764e31e35e4fd2883e20e1a0f66040",
            "placeholder": "",
            "style": "IPY_MODEL_dc5d529149fb4ceb8b4a2fc64204bbe3",
            "value": "500/500[00:00&lt;00:00,983.67examples/s]"
          }
        },
        "860f31826fcb4da2a759ef7bbd379527": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dea767c6415a4b5ca09e1dc7328ba08b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a803f9a542c84935a1b66a0b1627a480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b429a71070d3478bb940468824c2d30a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "786e41b251be48fb86c6ee1fb5dc4683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43764e31e35e4fd2883e20e1a0f66040": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc5d529149fb4ceb8b4a2fc64204bbe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "befebf4a7cc647988ce03d25dc0a0e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a896ff0810a4320874b9c62dd7cc6b2",
              "IPY_MODEL_002b65ef8b634e6f9d36e9137ebd6746",
              "IPY_MODEL_aeab3d3d3d674c35acf777d29b34e845"
            ],
            "layout": "IPY_MODEL_f957a54802314b29ae8d454f60c57c2d"
          }
        },
        "4a896ff0810a4320874b9c62dd7cc6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68d28ff2b5441ffa81fa18f9ffd31d0",
            "placeholder": "",
            "style": "IPY_MODEL_ef5a630113784ba099956fd004e015c4",
            "value": "tokenizer_config.json:100%"
          }
        },
        "002b65ef8b634e6f9d36e9137ebd6746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a1ea47f48f1461b9b60df1786b4f3f8",
            "max": 1156999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd3bd1fd706c49188f382b1214c039ea",
            "value": 1156999
          }
        },
        "aeab3d3d3d674c35acf777d29b34e845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e13ebf18fa64b259594780393530681",
            "placeholder": "",
            "style": "IPY_MODEL_01c7808f11bc47388f2a611c34f98054",
            "value": "1.16M/1.16M[00:00&lt;00:00,6.00MB/s]"
          }
        },
        "f957a54802314b29ae8d454f60c57c2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e68d28ff2b5441ffa81fa18f9ffd31d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5a630113784ba099956fd004e015c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a1ea47f48f1461b9b60df1786b4f3f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd3bd1fd706c49188f382b1214c039ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e13ebf18fa64b259594780393530681": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c7808f11bc47388f2a611c34f98054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13bbdf69cd254734880649be3d7a1f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_252825ffdfbe4013a61e78c8a409d199",
              "IPY_MODEL_24082f0d86704274aed2237bae8d00b4",
              "IPY_MODEL_892886b54e3e4d3687d74910b73bfd6b"
            ],
            "layout": "IPY_MODEL_2c9befaa9e3f41759e12441d2de786c1"
          }
        },
        "252825ffdfbe4013a61e78c8a409d199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5449deaf4eba470cb519d8ed926dacda",
            "placeholder": "",
            "style": "IPY_MODEL_9367819405184463a9e165ae3e17679d",
            "value": "tokenizer.model:100%"
          }
        },
        "24082f0d86704274aed2237bae8d00b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8437a3c9733141f4942e18eb865a7d02",
            "max": 4689074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8727b1b5ace48839c51386ccc16fe93",
            "value": 4689074
          }
        },
        "892886b54e3e4d3687d74910b73bfd6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_935c32a157a345719c6675067949080c",
            "placeholder": "",
            "style": "IPY_MODEL_2eea35cf6e1f4fc3b39085958b3afc41",
            "value": "4.69M/4.69M[00:00&lt;00:00,296kB/s]"
          }
        },
        "2c9befaa9e3f41759e12441d2de786c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5449deaf4eba470cb519d8ed926dacda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9367819405184463a9e165ae3e17679d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8437a3c9733141f4942e18eb865a7d02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8727b1b5ace48839c51386ccc16fe93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "935c32a157a345719c6675067949080c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eea35cf6e1f4fc3b39085958b3afc41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1315b45fa8c240dcb76ab6a81b043544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0345a9e711a44db99100810130023c3b",
              "IPY_MODEL_2054bbd4a63b4d3189e48975f7eb5b52",
              "IPY_MODEL_bd420c3d2dc84fed9acc8bef60bf5843"
            ],
            "layout": "IPY_MODEL_dd248f9101a64e27b93b90c7c3ff234b"
          }
        },
        "0345a9e711a44db99100810130023c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bca76a25225f490790790b5141f46692",
            "placeholder": "",
            "style": "IPY_MODEL_1a17f2a8b5d549d5a97a576eb279ce5e",
            "value": "tokenizer.json:100%"
          }
        },
        "2054bbd4a63b4d3189e48975f7eb5b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9036fdc0171493d9762c09786ce430f",
            "max": 33384568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_356afe2891cd465589ff56f85fa20896",
            "value": 33384568
          }
        },
        "bd420c3d2dc84fed9acc8bef60bf5843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d475b36b6c4b6288fe88b6421417bd",
            "placeholder": "",
            "style": "IPY_MODEL_ea71ccaf275b44a28f16f20c6d5b7446",
            "value": "33.4M/33.4M[00:00&lt;00:00,127MB/s]"
          }
        },
        "dd248f9101a64e27b93b90c7c3ff234b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bca76a25225f490790790b5141f46692": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a17f2a8b5d549d5a97a576eb279ce5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9036fdc0171493d9762c09786ce430f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "356afe2891cd465589ff56f85fa20896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1d475b36b6c4b6288fe88b6421417bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea71ccaf275b44a28f16f20c6d5b7446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d854ab0cf8184ec499d76f048a3d376f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72d9a7a6fe564931bc2142c78812e9f1",
              "IPY_MODEL_1a809a02a08640bdb434aea28066cc1b",
              "IPY_MODEL_d5ef87341bd34dc59c5e3567dda87f72"
            ],
            "layout": "IPY_MODEL_380084afa6ef403db4b86b89390c26a5"
          }
        },
        "72d9a7a6fe564931bc2142c78812e9f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6d67112d58440abb6f62a4ddfeda599",
            "placeholder": "",
            "style": "IPY_MODEL_f08905e7a9bc49088c987029ec9a3bde",
            "value": "added_tokens.json:100%"
          }
        },
        "1a809a02a08640bdb434aea28066cc1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5fc0de70d9d4a7e9fba22bfe7f0aecd",
            "max": 35,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d69252b5e0a4bf0965efe4f4d2a987b",
            "value": 35
          }
        },
        "d5ef87341bd34dc59c5e3567dda87f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6baa2e47750242ab88db78064b13b53d",
            "placeholder": "",
            "style": "IPY_MODEL_c94b50704b0a4379adef9c0a04d1b2bf",
            "value": "35.0/35.0[00:00&lt;00:00,3.92kB/s]"
          }
        },
        "380084afa6ef403db4b86b89390c26a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6d67112d58440abb6f62a4ddfeda599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f08905e7a9bc49088c987029ec9a3bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5fc0de70d9d4a7e9fba22bfe7f0aecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d69252b5e0a4bf0965efe4f4d2a987b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6baa2e47750242ab88db78064b13b53d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c94b50704b0a4379adef9c0a04d1b2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c3b2ff06106434f99f09949e23f06d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a514997b08a84313aead25e5ee49a8b5",
              "IPY_MODEL_daddb6d6ac904fb5aebe7da89c3a8be9",
              "IPY_MODEL_a81b46871b924ca5bbdc9ae37d3655d1"
            ],
            "layout": "IPY_MODEL_6683924518b7457083cab02f14d759af"
          }
        },
        "a514997b08a84313aead25e5ee49a8b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70e489a11f744e8292749b8a3ec203a9",
            "placeholder": "",
            "style": "IPY_MODEL_457cc9afeb3847c19bd4a84fdb0a2b25",
            "value": "special_tokens_map.json:100%"
          }
        },
        "daddb6d6ac904fb5aebe7da89c3a8be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e216fea6e69946c1a83efda3665b7d1f",
            "max": 662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4a06e2077cd642c795cdffeb054a5821",
            "value": 662
          }
        },
        "a81b46871b924ca5bbdc9ae37d3655d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dac4bcf548449aba9faef334a218bdb",
            "placeholder": "",
            "style": "IPY_MODEL_b99192e00e46403f9f44ef11bc9263f8",
            "value": "662/662[00:00&lt;00:00,65.9kB/s]"
          }
        },
        "6683924518b7457083cab02f14d759af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70e489a11f744e8292749b8a3ec203a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "457cc9afeb3847c19bd4a84fdb0a2b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e216fea6e69946c1a83efda3665b7d1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a06e2077cd642c795cdffeb054a5821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6dac4bcf548449aba9faef334a218bdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b99192e00e46403f9f44ef11bc9263f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbbe04ba9b424acaa94eb887a82fa9cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcd0995af75046fb9d60ed783129cbfd",
              "IPY_MODEL_364304d01fdd489aa3a0dc5237cdb4c6",
              "IPY_MODEL_3c8f29e2f300459782b9e5fd4972e24d"
            ],
            "layout": "IPY_MODEL_e134dcb99dd742219980696bcff1b50f"
          }
        },
        "bcd0995af75046fb9d60ed783129cbfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb5555b01efd4f12be83c98cee5fc025",
            "placeholder": "",
            "style": "IPY_MODEL_e5c512054ab64506abb574c5ee7a70ed",
            "value": "config.json:100%"
          }
        },
        "364304d01fdd489aa3a0dc5237cdb4c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81882866466646219f0a30817bcbe2f7",
            "max": 855,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bea29ec6995141329d82d99d0cd6810b",
            "value": 855
          }
        },
        "3c8f29e2f300459782b9e5fd4972e24d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_622c3190026d44d885db1c7a15add058",
            "placeholder": "",
            "style": "IPY_MODEL_5ba097b90c1744b3a4c9d99e2bbd1fa6",
            "value": "855/855[00:00&lt;00:00,100kB/s]"
          }
        },
        "e134dcb99dd742219980696bcff1b50f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb5555b01efd4f12be83c98cee5fc025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c512054ab64506abb574c5ee7a70ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81882866466646219f0a30817bcbe2f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bea29ec6995141329d82d99d0cd6810b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "622c3190026d44d885db1c7a15add058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ba097b90c1744b3a4c9d99e2bbd1fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0222ba48a7144ce28c3fc8d208a80673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c69a0611814e4cd280975ea3c311a5ab",
              "IPY_MODEL_b2728a6073fa470591af4c1a5c8fb6a5",
              "IPY_MODEL_35fbb88fffb8466b889256b56a74c1a1"
            ],
            "layout": "IPY_MODEL_1ae059ea89294b969f1168de974d8cb4"
          }
        },
        "c69a0611814e4cd280975ea3c311a5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0e9d1c126614008bf791150a6ad67db",
            "placeholder": "",
            "style": "IPY_MODEL_6c30ad131c5c487b87bbcadd68d2499c",
            "value": "model.safetensors.index.json:100%"
          }
        },
        "b2728a6073fa470591af4c1a5c8fb6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee082cd7b6d24c1fb5e88f03ee60b3a3",
            "max": 90558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abc329d4d6a74ce9b749dea65c2f47d1",
            "value": 90558
          }
        },
        "35fbb88fffb8466b889256b56a74c1a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80d30a1bfe4c4d51a75c6f20126ac659",
            "placeholder": "",
            "style": "IPY_MODEL_80e3b641dcc84e6099b813ef33e57e12",
            "value": "90.6k/90.6k[00:00&lt;00:00,8.90MB/s]"
          }
        },
        "1ae059ea89294b969f1168de974d8cb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0e9d1c126614008bf791150a6ad67db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c30ad131c5c487b87bbcadd68d2499c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee082cd7b6d24c1fb5e88f03ee60b3a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abc329d4d6a74ce9b749dea65c2f47d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80d30a1bfe4c4d51a75c6f20126ac659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e3b641dcc84e6099b813ef33e57e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e832cea359d4380a7790d9a5287bb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38137eaae3c348529d656980862f8ca5",
              "IPY_MODEL_db24da18f29d4e02afc2025dc8dc8bf6",
              "IPY_MODEL_24e44355702444da9355f3ea41096339"
            ],
            "layout": "IPY_MODEL_33b73a6997714feba277c596564531bb"
          }
        },
        "38137eaae3c348529d656980862f8ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d5882d34ccc4242b618390a79612c8e",
            "placeholder": "",
            "style": "IPY_MODEL_5e9d2dc1a3da41ffaf3228a3e51f6b38",
            "value": "Fetching2files:100%"
          }
        },
        "db24da18f29d4e02afc2025dc8dc8bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f447263e14ae4f79a6f3b2dc0d65eb71",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61afaf6520d14457b7b6bf4817f9302e",
            "value": 2
          }
        },
        "24e44355702444da9355f3ea41096339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cffa83ac7dc45b3b6849ea4e4224b91",
            "placeholder": "",
            "style": "IPY_MODEL_1b7931de0bff4e469dad4fb21f6e77b1",
            "value": "2/2[04:12&lt;00:00,252.55s/it]"
          }
        },
        "33b73a6997714feba277c596564531bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d5882d34ccc4242b618390a79612c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e9d2dc1a3da41ffaf3228a3e51f6b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f447263e14ae4f79a6f3b2dc0d65eb71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61afaf6520d14457b7b6bf4817f9302e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9cffa83ac7dc45b3b6849ea4e4224b91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b7931de0bff4e469dad4fb21f6e77b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "494ac75b50e8406dba2ef860cae31ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7f715c91c54437a90a452af3e34da31",
              "IPY_MODEL_04d6c571dd0b459084f545fbabf062e0",
              "IPY_MODEL_f0d509951de749929273afec3c7ed734"
            ],
            "layout": "IPY_MODEL_4deab9b936b94a0db72a7819c6d05055"
          }
        },
        "e7f715c91c54437a90a452af3e34da31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78a75a70f9784f40a5346e65585b2cbf",
            "placeholder": "",
            "style": "IPY_MODEL_3734649b25c44d0d97d6809a372c5590",
            "value": "model-00002-of-00002.safetensors:100%"
          }
        },
        "04d6c571dd0b459084f545fbabf062e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_748ea404ec7441c79ba54bedab4874f7",
            "max": 3639026128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0be18e588fe4a309a739a61e3b29ab1",
            "value": 3639026128
          }
        },
        "f0d509951de749929273afec3c7ed734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42dd84f15bb9490696f5952596de6a26",
            "placeholder": "",
            "style": "IPY_MODEL_70b35c9325b54884af015265ee91d347",
            "value": "3.64G/3.64G[04:01&lt;00:00,4.06MB/s]"
          }
        },
        "4deab9b936b94a0db72a7819c6d05055": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78a75a70f9784f40a5346e65585b2cbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3734649b25c44d0d97d6809a372c5590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "748ea404ec7441c79ba54bedab4874f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0be18e588fe4a309a739a61e3b29ab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42dd84f15bb9490696f5952596de6a26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70b35c9325b54884af015265ee91d347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7f95a263a90476180709396d16afa89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba825f06236c4d908a48dbd65388e9a7",
              "IPY_MODEL_06d2ecd6457142c4be085d94f50e1c5c",
              "IPY_MODEL_1e97df72c97248f2a70df9107bc053c5"
            ],
            "layout": "IPY_MODEL_56288d09620e4814ac45967754798dff"
          }
        },
        "ba825f06236c4d908a48dbd65388e9a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26f25d183e4942e7bf76466b4c2efed2",
            "placeholder": "",
            "style": "IPY_MODEL_8964abaa8b4449329f1bc58db3570fc0",
            "value": "model-00001-of-00002.safetensors:100%"
          }
        },
        "06d2ecd6457142c4be085d94f50e1c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98ad76142d704a8e9977c4369ec28dca",
            "max": 4961251752,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d955b2dd4624f4aa383e83f3a8dc457",
            "value": 4961251752
          }
        },
        "1e97df72c97248f2a70df9107bc053c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a42856e0173c4155baed41d4a344755e",
            "placeholder": "",
            "style": "IPY_MODEL_2ee829b4dfeb48bc90e589689918dee6",
            "value": "4.96G/4.96G[04:12&lt;00:00,43.4MB/s]"
          }
        },
        "56288d09620e4814ac45967754798dff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26f25d183e4942e7bf76466b4c2efed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8964abaa8b4449329f1bc58db3570fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98ad76142d704a8e9977c4369ec28dca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d955b2dd4624f4aa383e83f3a8dc457": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a42856e0173c4155baed41d4a344755e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ee829b4dfeb48bc90e589689918dee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01ccb380b5cb460f8150c2f9899776a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a579fcaa14e4292b0fb035c07c1f0bb",
              "IPY_MODEL_d46a2ca0177a43dba33cbfde51e516bc",
              "IPY_MODEL_332a96a73d51481e94c364437c22f542"
            ],
            "layout": "IPY_MODEL_1fd70816a24e467c853898f72aac4b68"
          }
        },
        "0a579fcaa14e4292b0fb035c07c1f0bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f8cb0f58fef4f7b8e5a492a09db39f8",
            "placeholder": "",
            "style": "IPY_MODEL_e1c8fc1c8e58417595a348046573c2c9",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "d46a2ca0177a43dba33cbfde51e516bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_279d2a901bee4308b54c7a438dd0b703",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_951e31fbbbcc481b9e3f430e9f1e1a24",
            "value": 2
          }
        },
        "332a96a73d51481e94c364437c22f542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7c19fecc8c94beda02d249fc15786d1",
            "placeholder": "",
            "style": "IPY_MODEL_cf71f20ac5b147a6b3e3a9dde00e38f8",
            "value": "2/2[00:35&lt;00:00,17.07s/it]"
          }
        },
        "1fd70816a24e467c853898f72aac4b68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f8cb0f58fef4f7b8e5a492a09db39f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1c8fc1c8e58417595a348046573c2c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "279d2a901bee4308b54c7a438dd0b703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "951e31fbbbcc481b9e3f430e9f1e1a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7c19fecc8c94beda02d249fc15786d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf71f20ac5b147a6b3e3a9dde00e38f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "182f6048f4394f178bf7bdadebc49745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d4cd8b1850b4b01bc4047c3a0f86fd5",
              "IPY_MODEL_279e15397346419cb3cb0e96fbbd7b50",
              "IPY_MODEL_411de89bc7be4efcb8df692980086358"
            ],
            "layout": "IPY_MODEL_1c7fb7fb8bc842ceb7f7b1c65ee7b299"
          }
        },
        "3d4cd8b1850b4b01bc4047c3a0f86fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_293b9264b2b44e1890681223539ddbf9",
            "placeholder": "",
            "style": "IPY_MODEL_a6550d858ad54806af0d7f69d70944e4",
            "value": "generation_config.json:100%"
          }
        },
        "279e15397346419cb3cb0e96fbbd7b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e296972f7054514b084fe4a6b54278b",
            "max": 215,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06480bd22c474856912bfb9e7f11c3d0",
            "value": 215
          }
        },
        "411de89bc7be4efcb8df692980086358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ed7e10098d42b0a58f96174771df3a",
            "placeholder": "",
            "style": "IPY_MODEL_b4f43d1886f74c7f8755ebb524deb94e",
            "value": "215/215[00:00&lt;00:00,25.3kB/s]"
          }
        },
        "1c7fb7fb8bc842ceb7f7b1c65ee7b299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "293b9264b2b44e1890681223539ddbf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6550d858ad54806af0d7f69d70944e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e296972f7054514b084fe4a6b54278b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06480bd22c474856912bfb9e7f11c3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3ed7e10098d42b0a58f96174771df3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4f43d1886f74c7f8755ebb524deb94e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01d48a861a5b407696a802079b3f47f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0d62113a2fd42e5b94905268e990cfd",
              "IPY_MODEL_66f82b807e9c4070ab472f2657a665cc",
              "IPY_MODEL_6c77b8a853c448b490f89dd8112a2f8a"
            ],
            "layout": "IPY_MODEL_8b4b8a3349744771bd34b95cc0740897"
          }
        },
        "d0d62113a2fd42e5b94905268e990cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c3e26b1bd834af2b2e4acd0c535d82c",
            "placeholder": "",
            "style": "IPY_MODEL_a9f625a74f7f4d5eb1e6a023eab9536a",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "66f82b807e9c4070ab472f2657a665cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a51757505a524b04bab0f6d8bb73d9d4",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_819f0f0fc8f74943a5108c1b4b1458b9",
            "value": 4
          }
        },
        "6c77b8a853c448b490f89dd8112a2f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_361470b2b6ee4daba9104c59a102f93a",
            "placeholder": "",
            "style": "IPY_MODEL_7dc30d6c188246e9a4bf75d9d756dba2",
            "value": "4/4[01:52&lt;00:00,26.33s/it]"
          }
        },
        "8b4b8a3349744771bd34b95cc0740897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c3e26b1bd834af2b2e4acd0c535d82c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9f625a74f7f4d5eb1e6a023eab9536a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a51757505a524b04bab0f6d8bb73d9d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "819f0f0fc8f74943a5108c1b4b1458b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "361470b2b6ee4daba9104c59a102f93a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dc30d6c188246e9a4bf75d9d756dba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aaa0cd28283648a9b37d408075f7af6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c3b46507eec417f8b31671737bb8a0d",
              "IPY_MODEL_4cb75409200c42dfa2aa118daf9707b0",
              "IPY_MODEL_99d8509b468b481e83c547bcb836509a"
            ],
            "layout": "IPY_MODEL_0fd26a0ef74944b18114eec7b9ac00a8"
          }
        },
        "3c3b46507eec417f8b31671737bb8a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d65d06ec84f44a1a934fbb3743a4a7bc",
            "placeholder": "",
            "style": "IPY_MODEL_330aec6cbbaa444ba90d21fc13a8f29b",
            "value": "Loadingcheckpointshards:100%"
          }
        },
        "4cb75409200c42dfa2aa118daf9707b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91fc1a60db804c9eb099870bea16936f",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11bc053f8b1945b6930a3a1880fca34e",
            "value": 4
          }
        },
        "99d8509b468b481e83c547bcb836509a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00b543ede66e453aa02770c7cd7cde8e",
            "placeholder": "",
            "style": "IPY_MODEL_054e9b12b970460ab21b1d1567b109fa",
            "value": "4/4[01:24&lt;00:00,19.61s/it]"
          }
        },
        "0fd26a0ef74944b18114eec7b9ac00a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d65d06ec84f44a1a934fbb3743a4a7bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "330aec6cbbaa444ba90d21fc13a8f29b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91fc1a60db804c9eb099870bea16936f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11bc053f8b1945b6930a3a1880fca34e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00b543ede66e453aa02770c7cd7cde8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "054e9b12b970460ab21b1d1567b109fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efbaacad99a24ecf99ca834669279a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb5c3acf28a649c5b1ac51232c63adfa",
              "IPY_MODEL_c4a3ca50124e4b11a7c1408b5a24d25e",
              "IPY_MODEL_6067b48a0a8349568ef95c387c76761b"
            ],
            "layout": "IPY_MODEL_320d502db46847099b5e0b77397fe973"
          }
        },
        "cb5c3acf28a649c5b1ac51232c63adfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d02c3740d24c5bb59cd90150abd406",
            "placeholder": "",
            "style": "IPY_MODEL_ff48dc7439e34298a8fac0b386238554",
            "value": "short-50/test-00000-of-00001.parquet:100%"
          }
        },
        "c4a3ca50124e4b11a7c1408b5a24d25e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae39150ae88d4f0aab5b2595ad88dc06",
            "max": 46349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04c49c3d0dfa4f1ca7795ee0c85d0978",
            "value": 46349
          }
        },
        "6067b48a0a8349568ef95c387c76761b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93e890ea7ea947ae95a0fae4de8f4e07",
            "placeholder": "",
            "style": "IPY_MODEL_7540a65322e94475b8cb1904e0c9f597",
            "value": "46.3k/46.3k[00:00&lt;00:00,133kB/s]"
          }
        },
        "320d502db46847099b5e0b77397fe973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2d02c3740d24c5bb59cd90150abd406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff48dc7439e34298a8fac0b386238554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae39150ae88d4f0aab5b2595ad88dc06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04c49c3d0dfa4f1ca7795ee0c85d0978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "93e890ea7ea947ae95a0fae4de8f4e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7540a65322e94475b8cb1904e0c9f597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de6861ce472e4d74b7b6f8a73c3db26d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5105b2a54e874eeba11ac6e434fffc69",
              "IPY_MODEL_d8d15f7364f44a5daa1e2b58bed47b75",
              "IPY_MODEL_cc314ef4506049459097075cc1ebebbb"
            ],
            "layout": "IPY_MODEL_aed4c53d456a477d9e2bbb409e34b5b5"
          }
        },
        "5105b2a54e874eeba11ac6e434fffc69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_657968216c894cbfbe50ce26ffd01109",
            "placeholder": "",
            "style": "IPY_MODEL_3aa1ce38991a4d1993d67dfb1de52c77",
            "value": "Generatingtestsplit:100%"
          }
        },
        "d8d15f7364f44a5daa1e2b58bed47b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_623da89c27114d8ead150788da3732b8",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65300f2674ae47e882cf133b771dfe1e",
            "value": 50
          }
        },
        "cc314ef4506049459097075cc1ebebbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f2ea10af02a4470ab047739446a010d",
            "placeholder": "",
            "style": "IPY_MODEL_e9fcc6a7fdd945779145ee10c665ac47",
            "value": "50/50[00:00&lt;00:00,1665.13examples/s]"
          }
        },
        "aed4c53d456a477d9e2bbb409e34b5b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "657968216c894cbfbe50ce26ffd01109": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa1ce38991a4d1993d67dfb1de52c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "623da89c27114d8ead150788da3732b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65300f2674ae47e882cf133b771dfe1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f2ea10af02a4470ab047739446a010d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9fcc6a7fdd945779145ee10c665ac47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}